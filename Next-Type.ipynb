{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next Type - A Mobile Typing Assistant\n",
    "[CS 505 - NLP] [Final Project]\n",
    "Completed by Muhammad Aseef Imran\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Compared to typing on a keyboard, typing on our phones considerably slower. Luckily most phones come built in with a feature to predict your next word to make up for this. However, after much experimentation, it seems most of these prediction algorithms seem to use simple N-grams with a window of around 3-4 words of left context. Many times, predicting the next word based on simple N-gram based probabilities work \"fine\" but often, this strategy produces poor results due to ignoring the context of the sentence. As NLP technology leaps forward exponentially we can do much better than simplistic N-grams. For this reason **Next Type** aims to bring the next generation of typing experiance to its users in order to raise productivity and typing speed for users leaving more time for the important things in life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"assets/bad-phone-example-1.jpg\" alt=\"N-gram example 1\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-2.jpg\" alt=\"N-gram example 2\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-3.jpg\" alt=\"N-gram example 3\" style=\"width: 33%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Presented above are three examples where unfortunately, the standard N-gram model employed on \"modern\" devices fails miserably.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Goals\n",
    "\n",
    "N-gram models perform poorly with shorter context window, and any large context windows require an immense amount of data to produce reasonable results. Moreover, at least in this case, our N-gram models do not consider the right context when suggesting words. However, despite having their weaknesses, the N-gram approach also has some nice benefits. Particularly, the nature of the N-gram model allows it to be easily updated with new data and adapt to the typing habits of its users with little processing power.\n",
    "\n",
    "Keeping these things in mind, we can when we set out on this project our (rather ambitious) goals for the model were:\n",
    "1. The model should consider the left (and ideally also the right context) before suggesting a \"natural\" word that fits the context.\n",
    "2. The model should be able to adapt to or learn from the user's word choices in various contexts.\n",
    "3. The model should be able to learn the user's word choice as—outlined above—quick enough to be useful.\n",
    "4. The model should be reasonably sized allowing it to be run and be updated on most modern-phone hardware with in reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definition a \"Successful\" Prediction\n",
    "\n",
    "We define a successful prediction as a prediction that meets the following criteria:\n",
    "```The predicted next word is one of the top 5 predictions```\n",
    "\n",
    "Also considered was to define a successful prediction if the predicted word as a similar meaning to the actual word. However, given the constraints this turned out to be a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Questions & Challenges\n",
    "\n",
    "As we set out on this prototype application, I outlined a few guiding key questions for myself:\n",
    "\n",
    "1. People evolve and change. So can I get a better accuracy by consider ALL known history? Or only \"recent\" history? Should the more recent history be weighted more\"? If so, should history be judged more by time or the volume typed? If I'm wrong about this, then more data = better. OR the time people change in is just longer.\n",
    "\n",
    "2. My problem is my data is ever evolving. So how do I prevent the model from overfitting? Overfitting will make newer data harder to generalize. (i.e. How do I prevent it from forgetting the stuff it knew in the base model?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Obtaining the Fine-Tuning Data\n",
    "\n",
    "It is important for my task that the fine-tunning data only come from a single person since my goal is to train the model to adapt to the typing behavior of a specific user. In order to achieve this we can use scraped messages from reddit for specific users and examine how the model adapts to their specific word-choice and typing habits. Particularly, we will be using data from a pre-scrapped dataset [Reddit comments/submissions 2005-06](https://academictorrents.com/details/89d24ff9d5fbc1efcdaf9d7689d72b7548f699fc). Further, we also want to make sure that sample data from any user we use:\n",
    "* Provides a reasonably large enough dataset to train on\n",
    "* Posts regularly (as opposed to posting a lot occasionally)\n",
    "\n",
    "With that defined, our focus will be between and including messages posted between [1/2011 - 6/2012]. Why this particular time range? No particular reason besides that data with in this time range was reasonably enough sized to be processed quickly yet still leave us with enough data to work with.\n",
    "\n",
    "We can then process the reddit dump creating a dictionary consisting of reddit users and the messages they sent. We further filter this data as follows:\n",
    "* \"Deleted\" users aren't included\n",
    "* First, we remove all authors with less than 546 (i.e. they must average more than 1 post a day) - although future filteration steps would've already ensured this requirement, we start of with this since this is a quick and dirty elimination step allowing for quicker processing in the next steps (which are a bit more complicated implementation wise)\n",
    "* Second, we filter to only authors that made at least 18 post per month in the time range without missing any month.\n",
    "* Third, we filter to only authors that made at least 2 post every week without missing a week.\n",
    "* Fourth, we filter down the remaining users to those that posted on at least 80% of the days.\n",
    "\n",
    "Code for this is implemented in the [data_processing_util.py](./data_processing_util.py) script file.\n",
    "\n",
    "Additional post-processing:\n",
    "* In our final step, we post process the post messages by running it through a sequence-to-sequence model already developed by someone correcting silly grammatical errors. Otherwise, we may end up having messages in our data set that contain non-existent vocab. In many cases, this post-processing fundamentally changed the sentence. However, I considered this a necessary evil because otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Establishing a Baseline\n",
    "\n",
    "As previously mentioned, we want to develop a model the beats the naive n-gram models (with a window of 3) used by most keyboard apps. Therefore, we will define our baseline as 3-gram model trained on the entire reddit corpus. A larger window size will not be feasible do to the massive amounts of memory required to hold such a model. This model uses simple probabilities based on the frequency of the n-gram in the corpus to make predictions.\n",
    "\n",
    "However, compared to n-gram models used by most keyboard apps, we will be making several simplification assumptions for our baseline:\n",
    "1. Our baseline model will not \"update\" n-gram model by using data from its user. On real keyboard apps, the model updates based on the users behavior (much like how we will update our models too later in this project). However, just for the purpose of having a working \"baseline\" and in the interest of time, this is a detail we neglect.\n",
    "2. Our model will also predict punctuations. Most keyboard apps only predict words however to get a fairer comparison to the models we will me employing (which do predict) punctuations, our version predicts punctuations (or tries to anyways).\n",
    "3. As previously implied, our n-gram model will be constructed solely of reddit. Specifically, the n-gram model is \"trained\" on all messages typed on reddit between [1/2011 - 6/2012].\n",
    "\n",
    "#### Baseline Training Details\n",
    "\n",
    "We will be training a 3-gram model and a 2-gram model. Our model will first attempt to use the 3-gram to make a prediction. If the 3-gram model does not have a machine n-gram, we will then fall back to 2-grams.\n",
    "\n",
    "The 3-gram model is trained as follows:\n",
    "1. For each month in the range [1/2011 - 6/2012], gather all posts and for each posts construct a 3-gram.\n",
    "    1. Add the 3-grams to a counter.\n",
    "    2. If by the time we finish processing the current month, if n-gram has not occurred at least 2 times delete it.\n",
    "2. Now for the entire n-gram dictionary, delete all n-gram that occurred less than 16 times in the entire corpus.\n",
    "\n",
    "Similarly, for the 2-gram model:\n",
    "1. For each month in the range [1/2011 - 6/2012], gather all posts and for each posts construct a 2-gram.\n",
    "    1. Add the 2-grams to a counter.\n",
    "    2. If by the time we finish processing the current month, if n-gram has not occurred at least 5 times delete it.\n",
    "2. Now for the entire n-gram dictionary, delete all n-gram that occurred less than 25 times in the entire corpus.\n",
    "\n",
    "Exact training script used can be found in the [create_raw_reddit_ngrams.py](./create_raw_reddit_ngrams.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation Stategy\n",
    "\n",
    "In light of our above outlined goals, we have two major evaluation goals:\n",
    "\n",
    "\n",
    "1. How well does the model predict the user's next token after having seen x tokens of examples from the user. In other words, not only how well the model predicts the user's next token but also how fast the model improves its prediction as a function of the data it has already seen?\n",
    "\n",
    "> We can evaluate \"how well\" the model predicts the user's next token by measuring the loss between what the user actually types vs what the model suggests. Then, we can further measure this loss as function of the number of tokens of examples the model has seen during its Fine-Tuning. For example, how does the loss change after the model has seen 1000 tokens of examples from the user?\n",
    "\n",
    "2. How much computation is needed is to both run the model and update the model on new data?\n",
    "\n",
    "> Measuring how long various parts of the model such prediction and training takes is trivial. (We can simply calculate the time between the target area of code). We may then analyze the run-time in context of the hardware the code is run on and comparing this information with current state of computational power of modern mobile devices. This information can be used to make an informed decision on the sequence lengths to input to the model to ensure our model can suggest new words to users in real-time on standard mobile hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Plan and Exploring Potential Solutions\n",
    "\n",
    "Once again, our goal is to accuractely and effectively predict the user's next token in real-time while adapting to the user's behavior and writing style over time.\n",
    "\n",
    "In order to reasonably meet these goals, we will fine-tune one or more combination of existing models such as T5, Bert, and GPT2, and/or their \"Distilled\" counterparts. We will use the Hugging Face transformers library simply due to its vast popularity and easy of use. I intend to use the SCC for rapid prototyping and experimentation as I already have significant experiance using the SCC at this stage.\n",
    "\n",
    "Finally I should note that in my initial research, I have identified potential pitfalls with each of these models and their strengths and weaknesses for my task. However, further experimentation will be needed to make a final decision on which model (or model combinations) to use. Detailed experimentation with each model will be required to evaluate its pros/cons.\n",
    "\n",
    "#### DistilBert [267MB]\n",
    "Having been trained on a mask-fill task, Bert naturally lends itself to the kind of project I am trying to do. Being a relatively small model, and still quite versatile for the task, Bert may be a great choice. However, one downside to Bert is that Bert seems to perform poorly when attempting to Mask-Fill multiple words in the middle of a sentence. (See the bottom of this notebook for a demonstration).\n",
    "\n",
    "#### DistilGPT2 [352MB]\n",
    "GPT2 was essentially trained on predicting the next word. Indeed, this is the task we want to achieve ourselves. However, in some cases, we may need to predict the middle word (if someone is editing the middle of a sentence they wrote). This is not a task GPT2 was designed for although this may still be possible due to the surprising generality of the model. Further research and experimentation will be needed.\n",
    "\n",
    "#### T5-Small [242 MB]\n",
    "T5 is an extremely general purpose model than can adapt to many NLP tasks. Unlike bert Being a substantially larger model than both GPT2 and Bert, T5 is slower to retrain. Yet at the same time, T5 seems to do a much better job mask-filling between sentences. Yet, in a realistic scenario how often does one write in the middle of the sentence? Is the increased computing cost really worth it? These are the questions I hope to answer with the first stages of my research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Limitations\n",
    "* Compared to the N-gram approach, this new model cannot easily learn new words?\n",
    "* You may talk different with friends vs family vs boss. This training and results was done specifically for reddit. It may be the case that the model will not generalize as well to a broader domain in an actual key board app. (Still probability at least better than the ngram stuff right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparison and Exploratory Between Various Models\n",
    "\n",
    "We begin with an exploratory between different promising transformer models that have been historically very successful with a variety of tasks. Namely, we will do a comparisons between Bert, GPT2, and T5 on various tasks.\n",
    "\n",
    "1. We compare between the following 5 sentences to see how well the models do. These results are just to get a basic idea of each model's capabilities. We will \"eyeball this result\". Note that the \"|\" token represents the current \"cursor\" location.\n",
    "\n",
    "    a. `After forcefully breaking into the bank, they|`\n",
    "\n",
    "    b. `Every |, my family and I visit Hawaii.`\n",
    "\n",
    "    c. `In my family, there is my |`\n",
    "\n",
    "    d. `My favorite | is apple.`\n",
    "\n",
    "    e. `It has been 2 months since I graduated. However, unfortunately I still haven't found a |. At this rate I won't be able to pay rent!`\n",
    "\n",
    "2. After that, we will test each model to see how well the model does in predicting this \"next\" word token. This task is the most important task for our proposed application to do well.\n",
    "\n",
    "3. Finally, we will choose the most promising model to develop a fine-tuning method for this continuous learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:47:43.964048600Z",
     "start_time": "2023-12-23T04:47:37.010185600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs505ws/aseef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports here\n",
    "from typing import Union\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List, Set, Any, Union\n",
    "import statistics\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, PreTrainedTokenizerBase, PreTrainedModel, GPT2Config\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, DistilBertConfig, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from datasets import Dataset\n",
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:47:43.964048600Z",
     "start_time": "2023-12-23T04:47:43.943483700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"After forcefully breaking into the bank, they |\", # predict the next word using context\n",
    "    \"In my family, there is my |\", # predict the next word using context\n",
    "    \"Every |, my family and I visit Hawaii.\",  # simple mask fill with one missing word\n",
    "    \"My favorite | is apple.\", # simple mask fill with one missing word\n",
    "    \"It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\", # need multiple words here before sentence makes sense\n",
    "    \"Following the American Civil War, | assassinated.\",  # need multiple words here before sentence makes sense\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:47:43.964048600Z",
     "start_time": "2023-12-23T04:47:43.945466100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"/projectnb/cs505ws/projects/NextType/data\"\n",
    "\n",
    "def does_var_exists_gz(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'{data_dir}/{var_name}.pkl.gz')\n",
    "\n",
    "def dump_var_gz(var_name: str, obj) -> None:\n",
    "    os.makedirs(f\"{data_dir}\", exist_ok=True)\n",
    "    with gzip.open(F'{data_dir}/{var_name}.pkl.gz', 'wb', compresslevel=1) as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_var_gz(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists_gz(var_name):\n",
    "        return None\n",
    "\n",
    "    file_path = F'{data_dir}/{var_name}.pkl.gz'  # Updated file extension\n",
    "    with gzip.open(file_path, 'rb', compresslevel=1) as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:47:50.856895500Z",
     "start_time": "2023-12-23T04:47:43.949482900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load n_gram models\n",
    "reddit_2_grams_pmf: Dict[Tuple[str, str], Dict[str, float]] = load_var_gz(\"reddit_2_grams_pmf\")\n",
    "reddit_3_grams_pmf: Dict[Tuple[str, str], Dict[str, float]] = load_var_gz(\"reddit_3_grams_pmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.576637500Z",
     "start_time": "2023-12-23T04:47:50.856895500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load author to lines data\n",
    "author_to_posts_dict: Dict[str, Tuple[int, str]] = load_var_gz(\"author_to_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.577654900Z",
     "start_time": "2023-12-23T04:48:18.576637500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ThePain',\n",
       " 'GoatseMcShitbungle',\n",
       " 'Mike81890',\n",
       " 'aerynmoo',\n",
       " 'zoomshoes',\n",
       " 'General_Mayhem',\n",
       " 'kingmanic',\n",
       " 'triliana',\n",
       " 'smilingarmpits',\n",
       " 'elitexero',\n",
       " 'xyqxyq',\n",
       " 'yoda133113',\n",
       " 'skootles',\n",
       " 'swordgeek',\n",
       " 'aardvarkious',\n",
       " 'darkshaddow42',\n",
       " 'schlitz100',\n",
       " 'NukeThePope',\n",
       " 'ryeguy',\n",
       " 'ten_thousand_puppies',\n",
       " 'ObeseSnake',\n",
       " 'prostidude',\n",
       " 'gsfgf',\n",
       " 'arlanTLDR',\n",
       " 'amus']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_user_sample = random.sample(list(author_to_posts_dict.keys()), 25)\n",
    "random_user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.577654900Z",
     "start_time": "2023-12-23T04:48:18.577654900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample up to 50 posts from each user\n",
    "random_post_samples = []\n",
    "for rand_user in random_user_sample:\n",
    "    all_posts = author_to_posts_dict[rand_user]\n",
    "    sampled_posts = random.sample(list(all_posts), 50)\n",
    "    random_post_samples += sampled_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.656239900Z",
     "start_time": "2023-12-23T04:48:18.577654900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device={device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.657171300Z",
     "start_time": "2023-12-23T04:48:18.604662400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import html\n",
    "def normalize_text(post_text: str):\n",
    "    # get rid of new lines\n",
    "    post_text = re.sub(\"\\n\", \" \", post_text)\n",
    "    # remove html characters\n",
    "    post_text = html.unescape(post_text)\n",
    "    # Remove bold and italic formatting\n",
    "    post_text = post_text.replace('*', \"\")\n",
    "    # Remove headers\n",
    "    post_text = re.sub(r'^#{1,6}\\s', '', post_text)\n",
    "    # Remove hyperlinks\n",
    "    post_text = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'\\1', post_text)\n",
    "    # Remove inline code\n",
    "    post_text = re.sub(r'`([^`]+)`', r'\\1', post_text)\n",
    "    # Remove block code\n",
    "    post_text = re.sub(r'```[^`]*```', '', post_text)\n",
    "    # Remove lists (unordered and ordered)\n",
    "    post_text = re.sub(r'^\\s*([\\*\\-\\+]\\s|(\\d+\\.)\\s)', '', post_text)\n",
    "    post_text = re.sub(\"(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3\", \"\", post_text)\n",
    "    # remove double spaces\n",
    "    post_text = re.sub(\" {2,}\", \" \", post_text)\n",
    "    # replace urls\n",
    "    post_text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"{URL}\", post_text)\n",
    "    # replace references to a specific subreddit but just the token \"{SUB_REDDIT}\"\n",
    "    post_text = re.sub(r\"(\\W)(r/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB_REDDIT}\\3\", post_text)\n",
    "    post_text = re.sub(r\"(\\W)(/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB_REDDIT}\\3\", post_text)\n",
    "    post_text = post_text.strip()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:35:56.123109700Z",
     "start_time": "2023-12-21T17:35:56.062574400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ngram_prediction(current_sent) -> Union[None, str]:\n",
    "    def tokenize_post(pre_tokenized_post_data):\n",
    "        return [word for sent in nltk.sent_tokenize(pre_tokenized_post_data) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    current_sent = normalize_text(current_sent)\n",
    "    current_sent = current_sent.lower()\n",
    "    ngram_input = tokenize_post(current_sent)\n",
    "\n",
    "    if tuple(ngram_input[-2:]) in reddit_3_grams_pmf:\n",
    "        prediction = reddit_3_grams_pmf[tuple(ngram_input[-2:])]\n",
    "    elif tuple(ngram_input[-1:]) in reddit_2_grams_pmf:\n",
    "        prediction = reddit_2_grams_pmf[tuple(ngram_input[-1:])]\n",
    "    else:\n",
    "        prediction = None\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Baseline n-grams On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:32:44.615245400Z",
     "start_time": "2023-12-21T17:32:44.605229500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: {'are': 0.3346073933449349, \"'re\": 0.30994814463253, 'do': 0.12743342298159574, 'have': 0.12373499270509238, 'will': 0.104276046335847}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: {'favorite': 0.5853830605651704, 'first': 0.1888119327439376, 'favourite': 0.0983645108370054, 'new': 0.06445652599929452, 'opinion': 0.06298396985459208}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: {'time': 0.3896827529049508, 'day': 0.23419787074369317, 'single': 0.18737812391879125, 'other': 0.1147109820572633, 'year': 0.0740302703753015}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: {'.': 0.33868374032162, 'part': 0.25645473496128646, 'is': 0.17503722453841572, ',': 0.12518612269207863, 'thing': 0.10463817748659916}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: {'way': 0.4054198728671797, 'new': 0.1742165718746515, 'few': 0.1631091780974685, 'good': 0.14767480762796922, 'lot': 0.10957956953273112}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: {'and': 0.39923997633459246, 'but': 0.24832749283211214, 'the': 0.16813816957174715, 'i': 0.09372866700040959, 'it': 0.09056569426113867}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    # out ngram model can only consider left context\n",
    "    input_text = sentence[:sentence.index(\"|\")].strip()\n",
    "    suggestions = get_ngram_prediction(input_text)\n",
    "\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Baseline n-grams on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:36:51.974502800Z",
     "start_time": "2023-12-21T17:36:22.130391200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:29<00:00, 41.95it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_correct_guesses = 0\n",
    "baseline_total_guesses = 0\n",
    "baseline_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "\n",
    "        predictions = get_ngram_prediction(current_prompt)\n",
    "        if predictions is not None and actual_next_word in predictions:\n",
    "            baseline_correct_guesses += 1\n",
    "        baseline_total_guesses += 1\n",
    "\n",
    "        baseline_inference_times += [time.time() - start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:37:05.216969400Z",
     "start_time": "2023-12-21T17:37:05.183749800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38662746585735963\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = baseline_correct_guesses / baseline_total_guesses\n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.022976395781948235\n",
      "Inference Std: 0.00535418975672332\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(baseline_inference_times))\n",
    "print('Inference Std:', statistics.stdev(baseline_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T06:41:21.269795Z",
     "start_time": "2023-12-23T06:41:19.119092100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the t5 model\n",
    "T5_path = 't5-small'\n",
    "t5_config = T5Config.from_pretrained(T5_path)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_path, legacy=False)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_path, config=t5_config).to(device)\n",
    "t5_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T06:41:22.428319700Z",
     "start_time": "2023-12-23T06:41:21.256815700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('<pad> <extra_id_0> broke', 0.22146426141262054), ('<pad> <extra_id_0> break', 0.21711385250091553), ('<pad> <extra_id_0> are', 0.21013765037059784), ('<pad> <extra_id_0> were', 0.17779791355133057), ('<pad> <extra_id_0> will', 0.1734863519668579)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('<pad> <extra_id_0> family', 0.27247941493988037), ('<pad> <extra_id_0> own', 0.25976160168647766), ('<pad> <extra_id_0> daughter', 0.15796756744384766), ('<pad> <extra_id_0> mother', 0.1555204540491104), ('<pad> <extra_id_0> home', 0.1542709767818451)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('<pad> <extra_id_0> year', 0.3100126087665558), ('<pad> <extra_id_0> day', 0.2699778378009796), ('<pad> <extra_id_0> month', 0.1532614529132843), ('<pad> <extra_id_0> week', 0.14059576392173767), ('<pad> <extra_id_0> time', 0.12615235149860382)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('<pad> <extra_id_0> apple', 0.469388872385025), ('<pad> <extra_id_0> fruit', 0.16375479102134705), ('<pad> <extra_id_0> thing', 0.15048746764659882), ('<pad> <extra_id_0> pie', 0.11327831447124481), ('<pad> <extra_id_0> recipe', 0.10309050232172012)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('<pad> <extra_id_0> rent', 0.24719730019569397), ('<pad> <extra_id_0> rate', 0.22261090576648712), ('<pad> <extra_id_0> fixed', 0.21800415217876434), ('<pad> <extra_id_0> rental', 0.16750206053256989), ('<pad> <extra_id_0> flat', 0.1446855515241623)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('<pad> <extra_id_0> the', 0.314527302980423), ('<pad> <extra_id_0> ', 0.2124253213405609), ('<pad> <extra_id_0> American', 0.2062595784664154), ('<pad> <extra_id_0> many', 0.13480444252490997), ('<pad> <extra_id_0> an', 0.13198336958885193)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"<extra_id_0>\")\n",
    "    input_ids = t5_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    num_samples = 5\n",
    "    with torch.no_grad():\n",
    "        output = t5_model.generate(\n",
    "            input_ids,\n",
    "            num_beams=num_samples,\n",
    "            min_new_tokens=2,\n",
    "            max_new_tokens=2,\n",
    "            num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(output.sequences, probabilities):\n",
    "        decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "        suggestions += [(decoded_output, prob.item())]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T06:41:32.527174900Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                                     | 23/1250 [00:59<52:41,  2.58s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      " 19%|██████████████████████▏                                                                                                | 233/1250 [03:49<16:41,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "t5_correct_guesses = 0\n",
    "t5_total_guesses = 0\n",
    "t5_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" <extra_id_0>\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids = t5_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # t5 can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            num_samples = 7\n",
    "            output = t5_model.generate(\n",
    "                input_ids,\n",
    "                num_beams=num_samples,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=2,\n",
    "                num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for sample_output, prob in zip(output.sequences, probabilities):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            t5_correct_guesses += 1\n",
    "        t5_total_guesses += 1\n",
    "        t5_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:59:26.006531800Z",
     "start_time": "2023-12-21T17:59:26.001529100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37467754172989376\n"
     ]
    }
   ],
   "source": [
    "t5_accuracy = t5_correct_guesses / t5_total_guesses\n",
    "print(t5_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:59:26.198139Z",
     "start_time": "2023-12-21T17:59:26.007530800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.02229035642783812\n",
      "Inference Std: 0.0007565512458901443\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(t5_inference_times))\n",
    "print('Inference Std:', statistics.stdev(t5_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "#### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:04:34.651566200Z",
     "start_time": "2023-12-11T18:04:33.756145900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distbert_path = 'distilbert-base-cased'\n",
    "distbert_model = DistilBertForMaskedLM.from_pretrained(distbert_path).to(device=device)\n",
    "distbert_config = DistilBertConfig.from_pretrained(distbert_path)\n",
    "distbert_tokenizer = DistilBertTokenizer.from_pretrained(distbert_path, config=distbert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:16.485638800Z",
     "start_time": "2023-12-11T16:15:16.418844600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"[MASK]\")\n",
    "    input_ids = distbert_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    with torch.no_grad():\n",
    "        output = distbert_model(input_ids)\n",
    "        predictions = output.logits\n",
    "\n",
    "    # Get the top-k predicted tokens and their probabilities\n",
    "    top_k = 5  # Adjust as needed\n",
    "    probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "        suggestions += [(sample_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:14.923844Z",
     "start_time": "2023-12-11T16:55:59.910583900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:14<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "distbert_correct_guesses = 0\n",
    "distbert_total_guesses = 0\n",
    "distbert_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" [MASK]\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids: torch.Tensor = distbert_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # bert can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "        # Get the position of the masked token\n",
    "        mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            output = distbert_model(input_ids)\n",
    "            predictions = output.logits\n",
    "\n",
    "        # Get the top-k predicted tokens and their probabilities\n",
    "        top_k = 8  # Adjust as needed\n",
    "        probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "        # Convert indices back to tokens\n",
    "        predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for decoded_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            if prob < 0.04:\n",
    "                # avoid bizarre suggestions by simply filtering out low prob\n",
    "                # terms. We don't HAVE TO suggest exactly 5 words\n",
    "                break\n",
    "            # bert also suggests \"sub-words\". Ehh... we'll just ignore those.\n",
    "            # otherwise stuff will get too complicated\n",
    "            if decoded_output.startswith(\"##\"):\n",
    "                continue\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            distbert_correct_guesses += 1\n",
    "        # since the way the bert tokenizer works, it can suggest \"sub-words\" - example: characteristically = characteristic + ##ally,\n",
    "        # so we will give bert half a point for suggest the same start of the word\n",
    "        for s in suggestions:\n",
    "            if actual_next_word.startswith(s):\n",
    "                distbert_correct_guesses += 0.5\n",
    "                break\n",
    "        distbert_total_guesses += 1\n",
    "        distbert_inference_times += [time.time() - start_time]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:20.001862900Z",
     "start_time": "2023-12-11T17:00:19.985839900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18298021889747967\n"
     ]
    }
   ],
   "source": [
    "bert_accuracy = distbert_correct_guesses / distbert_total_guesses\n",
    "print(bert_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:22.286787800Z",
     "start_time": "2023-12-11T17:00:22.234767400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.005107757104114663\n",
      "Inference Std: 0.001992808943326104\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distbert_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distbert_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Distil-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:17:59.963504100Z",
     "start_time": "2023-12-23T02:17:58.367356500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the GPT-2 model variant\n",
    "distgpt2_model_name = \"distilgpt2\"\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "distgpt2_model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(distgpt2_model_name)\n",
    "distgpt2_tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(distgpt2_model_name)\n",
    "# Move the model to the GPU (if available)\n",
    "distgpt2_model = distgpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:09:22.420843400Z",
     "start_time": "2023-12-21T19:09:21.863161400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [(' were', 0.6859785914421082), (' found', 0.10938050597906113), (' had', 0.08820205926895142), (' took', 0.059164125472307205), (' began', 0.05727475509047508)]\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [(' family', 0.27839019894599915), (' brother', 0.2245674729347229), (' mother', 0.18426108360290527), (' sister', 0.1591966301202774), (' wife', 0.1535845696926117)]\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [(' The', 0.40960025787353516), ('.', 0.16524524986743927), (' A', 0.15191836655139923), ('\\n', 0.13858996331691742), ('The', 0.13464611768722534)]\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [(' favorite', 0.44314318895339966), (' part', 0.18813173472881317), (' thing', 0.16900213062763214), (' of', 0.11418430507183075), ('.', 0.08553868532180786)]\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [(' way', 0.38864630460739136), (' job', 0.33211749792099), (' place', 0.1317850798368454), (' new', 0.09153558313846588), (' good', 0.055915530771017075)]\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [(' the', 0.7203230857849121), (' a', 0.11123217642307281), (' it', 0.07635104656219482), (' and', 0.05197402089834213), (' in', 0.04011968895792961)]\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    # delete everything including and after |\n",
    "    # GPT2 is physically unable to consider right context\n",
    "    input_text = sentence[:sentence.index(\"|\")].strip()\n",
    "    # Tokenize the prompt\n",
    "    input_ids = distgpt2_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    # Generate probabilities for the next words\n",
    "    with torch.no_grad():\n",
    "        outputs = distgpt2_model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "    # after all, we don't want to suggest too many words!\n",
    "    top_k = 5\n",
    "    # Get the probability distribution for the next word\n",
    "    next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "    top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "    # Normalize probabilities\n",
    "    top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "    # Convert the probabilities to a list\n",
    "    top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "    top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "        decoded_output = distgpt2_tokenizer.decode(sample_output)\n",
    "        suggestions += [(decoded_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.059494800Z",
     "start_time": "2023-12-21T19:09:27.834935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████████████████████████████████▌                                                                                           | 566/1250 [01:41<02:03,  5.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:22<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "        if input_ids.shape[1] == 0:\n",
    "            continue\n",
    "        # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 1024:\n",
    "            input_ids = input_ids[:, -1024:]\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = distgpt2_model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "        top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "        # filter to only words with a probability greater than p%\n",
    "        removal_marked = []\n",
    "        for j in range(len(top_k_probs_list)):\n",
    "            if top_k_probs_list[j] < top_p:\n",
    "                removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "        for to_remove in removal_marked:\n",
    "            top_k_probs_list.remove(to_remove[0])\n",
    "            top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "        guesses = set()\n",
    "        for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "            token = distgpt2_tokenizer.decode([token_id])\n",
    "            guesses.add(token)\n",
    "\n",
    "        # we consider a prediction correct if the actual word was\n",
    "        # one of the (up to) 5 words suggested\n",
    "        if ' ' + actual_next_word in guesses:\n",
    "            distilgpt2_correct_predictions += 1\n",
    "        distilgpt2_total_predictions += 1\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.064494100Z",
     "start_time": "2023-12-21T19:13:50.062513200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36095087922396757\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print(gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.153521100Z",
     "start_time": "2023-12-21T19:13:50.064494100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.0052210369432941625\n",
      "Inference Std: 0.002328083751368582\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distilgpt2_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distilgpt2_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary of the results from Model Comparison\n",
    "\n",
    "With a 38.6% accuracy, on top of having the fastest inference speeds, none of the models beat the baseline n-gram model. I was surprised by this result. All 3 transformer models had much larger context windows to work of from and understand language much better than n-grams, yet none beat n-grams on the random sample used.\n",
    "\n",
    "In 2nd place was T5-small with an accuracy of 37.5%.\n",
    "In 3rd place was DistilGPT2 with an accuracy of 36.1%.\n",
    "And in 4th place was DistilBert with an accuracy of 18.2%.\n",
    "\n",
    "The fact T5 beat DistilGPT2 was another shock. Despite being trained on the \"predict the next word\" task and being a larger model in raw size than T5-small, DistilGPT2 still lost. In fact T5-small is the smallest model here!\n",
    "\n",
    "Let's now look at the runtimes of all 3 models on both GPU and CPU\n",
    "\n",
    "* T5 was the slowest model\n",
    "GPU\n",
    "* 24:39 - T5-Small\n",
    "* 5:10 - DistilBert\n",
    "* 5:32 - DistilGPT2\n",
    "CPU\n",
    "* 3:19:43: T5-Small\n",
    "* 1:41:39 - DistilBert\n",
    "* 2:29:33 - DistilGPT2\n",
    "\n",
    "Based on these results, I have decided to **use GPT2 for further fine-tuning** to see if we can beat the baseline for the following reasons:\n",
    "* GPT2 is much faster than T5 on a GPU and considerably faster on a CPU. On an actual mobile device, using GPT2 allows me to reach a user-base with lower end CPUs as well. In other words, T5 is simply to slow to test and run!\n",
    "* Due to T5's dexterity, initial research suggests fine-tuning T5 will be a much more complicated task than fine-tuning GPT2 for a \"predict the next word task\". After all, \"predict the next word\" is not the primary thing T5 was trained for. As a result, not as many resources exist either on achieving this.\n",
    "* DistilGPT2 is the largest model here, and as such the extra parameters may allow it to adapt to our domain better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Experimenting with Fine-Tuning GPT2\n",
    "In this section, we Fine-tune GPT2 using data from a randomly select reddit user and then examining how GPT2 performs on an evaluation set from that user. This section is simply an experiment to see how well GPT2 might be able to adapt to the writing styles from a specific user.\n",
    "\n",
    "This section differs from a real-word use case because in the real world, you will not have all the data from the user in once place to train on. Rather, in the real-word, we would continuously be updating an existing model as new data comes in. Continuous learning is instead something we explore in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:18.657171300Z",
     "start_time": "2023-12-23T04:48:18.608159200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The_Jackal'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select a user to train with\n",
    "random_user = random.choice(list(author_to_posts_dict.keys()))\n",
    "random_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:19.829150500Z",
     "start_time": "2023-12-23T04:48:18.611186500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this was the only small-ish grammar correction model I found.\n",
    "# had I more time, I would create my own model. But I don't so I will focus\n",
    "# on my primary task.\n",
    "# the downsides of this models is that besides correcting spellings, it often alters the\n",
    "# structure of the sentence which could fundamentally undermine our purpose.\n",
    "# so question: does benefits of correcting grammar using this outweighs the harms?\n",
    "# after all, if the model doesnt recgonize a word, it'll just ignore it and wont learn from it!\n",
    "grammar_corrector = pipeline(\n",
    "               'text2text-generation',\n",
    "               'pszemraj/grammar-synthesis-small',\n",
    "                 device=device\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:19.832149500Z",
     "start_time": "2023-12-23T04:48:19.829150500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_posts_corrected = author_to_posts_dict[random_user]\n",
    "for i in tqdm(range(len(user_posts_corrected))):\n",
    "    creation, message = user_posts_corrected[i]\n",
    "    message = normalize_text(message)\n",
    "    sentences = sent_tokenize(message)\n",
    "    for j in range(len(sentences)):\n",
    "        sent = sentences[j]\n",
    "        updated_message = grammar_corrector(sent)[0]['generated_text']\n",
    "        sentences[j] = updated_message\n",
    "    updated_message = ' '.join(sentences)\n",
    "    user_posts_corrected[i] = (creation, updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:19.835151700Z",
     "start_time": "2023-12-23T04:48:19.831149500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_var_gz(f\"{random_user}-corrected-posts\", user_posts_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:19.842151Z",
     "start_time": "2023-12-23T04:48:19.838149Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#user_posts_corrected = load_var_gz('The_Jackal-corrected-posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:19.865151200Z",
     "start_time": "2023-12-23T04:48:19.849152500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3787"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_dataset_list = user_posts_corrected\n",
    "#entire_dataset_list = author_to_posts_dict[random_user]\n",
    "# filter out posts with just a single word\n",
    "entire_dataset_list = [data for data in entire_dataset_list if len(data[1].split(\" \")) > 1]\n",
    "\n",
    "entire_dataset_list = [{\"post\": x[1], \"time\": x[0]} for x in entire_dataset_list]\n",
    "len(entire_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:20.992227700Z",
     "start_time": "2023-12-23T04:48:19.854150100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 89805\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "for data_entry in entire_dataset_list:\n",
    "    post = data_entry[\"post\"]\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    total_words += len(tokenized_words)\n",
    "print('Total words:', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:20.992227700Z",
     "start_time": "2023-12-23T04:48:20.974230700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the list based on the \"time\" key\n",
    "entire_dataset_list = sorted(entire_dataset_list, key=lambda x: x[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:20.993225800Z",
     "start_time": "2023-12-23T04:48:20.976227Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entire_dataset = Dataset.from_list(entire_dataset_list)\n",
    "split_sets = entire_dataset.train_test_split(train_size=0.85, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:21.020266200Z",
     "start_time": "2023-12-23T04:48:20.993225800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['post', 'time'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['post', 'time'],\n",
       "        num_rows: 569\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:23.484031500Z",
     "start_time": "2023-12-23T02:10:23.378176200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b12f63961224378a1e5aac339b143fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc37935cd1540ec843b1e44f87d23e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_sets.save_to_disk(f\"{data_dir}/dataset-{random_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:23.439228900Z",
     "start_time": "2023-12-23T04:48:20.996233300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 3218\n",
      "Validation dataset size: 569\n",
      "Example from training dataset: {'post': 'Or a right-wing multi-millionaire?', 'time': '1315765171'}\n",
      "Example from validation dataset: {'post': \"They didn't? . .\", 'time': '1310158819'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model_original = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "distgpt2_tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(model_name)\n",
    "distgpt2_tokenizer.pad_token = distgpt2_tokenizer.eos_token\n",
    "\n",
    "trained_dataset = load_from_disk(f\"{data_dir}/dataset-{random_user}\")['train']\n",
    "validation_dataset = load_from_disk(f\"{data_dir}/dataset-{random_user}\")['test']\n",
    "\n",
    "print(\"Training dataset size:\", len(trained_dataset))\n",
    "print(\"Validation dataset size:\", len(validation_dataset))\n",
    "print(\"Example from training dataset:\", trained_dataset[0])\n",
    "print(\"Example from validation dataset:\", validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:23.439228900Z",
     "start_time": "2023-12-23T04:48:23.437225500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(batch):\n",
    "    return distgpt2_tokenizer([x.strip('\\n\\r') for x in batch['post']], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:48:25.304206600Z",
     "start_time": "2023-12-23T04:48:23.440226600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ded43a80e8349df974bd6173c3b243f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_dataset_processed = trained_dataset.map(encode, batched=True, batch_size=len(trained_dataset))\n",
    "trained_dataset_processed.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "validation_dataset_processed = validation_dataset.map(encode, batched=True, batch_size=len(validation_dataset))\n",
    "validation_dataset_processed.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:32.266405800Z",
     "start_time": "2023-12-23T02:10:32.204405500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['post', 'time', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3218\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:24:34.532585400Z",
     "start_time": "2023-12-23T03:19:23.489064Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1074 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1074, training_loss=3.063382762770413, metrics={'train_runtime': 309.1785, 'train_samples_per_second': 20.816, 'train_steps_per_second': 3.474, 'total_flos': 1326971051311104.0, 'train_loss': 3.063382762770413, 'epoch': 2.0})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=distgpt2_tokenizer,\n",
    "    mlm=False  # we're not using masked language modeling here\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{data_dir}/gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,  # adjust as needed\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=12,\n",
    "    logging_steps=1,\n",
    "    save_steps=1_000,  # adjust as needed\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=distgpt2_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=trained_dataset_processed,\n",
    "    #eval_dataset=validation_dataset_processed,  #TODO\n",
    ")\n",
    "\n",
    "# Fine-tune the GPT-2 model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:53:05.530205Z",
     "start_time": "2023-12-23T04:53:05.481181800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(test_model, data: List[str], progress_bar=True):\n",
    "    distilgpt2_correct_predictions = 0\n",
    "    distilgpt2_total_predictions = 0\n",
    "    distilgpt2_inference_times = []\n",
    "\n",
    "    test_model.eval()\n",
    "\n",
    "    for post in tqdm(data, smoothing=0, disable=not progress_bar):\n",
    "        sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "        tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "        tokenized_words = [word for s in tokenized_words for word in s]\n",
    "        for i in range(1, len(tokenized_words) - 1):\n",
    "            start_time = time.time()\n",
    "            current_prompt = ' '.join(tokenized_words[:i])\n",
    "            current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "            current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "            actual_next_word = tokenized_words[i]\n",
    "            # Tokenize the prompt\n",
    "            input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "            if input_ids.shape[1] == 0:\n",
    "                continue\n",
    "            # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "            # we trim it before passing into the model\n",
    "            if input_ids[0].shape[0] > 1024:\n",
    "                input_ids = input_ids[:, -1024:]\n",
    "            # Generate probabilities for the next words\n",
    "            with torch.no_grad():\n",
    "                outputs = test_model(input_ids)\n",
    "                logits = outputs.logits\n",
    "            # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "            # after all, we don't want to suggest too many words!\n",
    "            top_k = 5\n",
    "            top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "            # Get the probability distribution for the next word\n",
    "            next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "            top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "            # Normalize probabilities\n",
    "            top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "            # Convert the probabilities to a list\n",
    "            top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "            top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "            # filter to only words with a probability greater than p%\n",
    "            removal_marked = []\n",
    "            for j in range(len(top_k_probs_list)):\n",
    "                if top_k_probs_list[j] < top_p:\n",
    "                    removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "            for to_remove in removal_marked:\n",
    "                top_k_probs_list.remove(to_remove[0])\n",
    "                top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "            guesses = set()\n",
    "            for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "                token = distgpt2_tokenizer.decode([token_id])\n",
    "                guesses.add(token)\n",
    "\n",
    "            # we consider a prediction correct if the actual word was\n",
    "            # one of the (up to) 5 words suggested\n",
    "            if ' ' + actual_next_word in guesses:\n",
    "                distilgpt2_correct_predictions += 1\n",
    "            distilgpt2_total_predictions += 1\n",
    "            distilgpt2_inference_times += [time.time() - start_time]\n",
    "\n",
    "    return (distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:31:52.317402100Z",
     "start_time": "2023-12-23T03:30:47.787432300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 569/569 [01:04<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(distgpt2_model, validation_dataset_processed['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:31:52.317402100Z",
     "start_time": "2023-12-23T03:31:52.307403600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla accuracy: 0.320960024695169\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print('vanilla accuracy:', gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:32:56.593069200Z",
     "start_time": "2023-12-23T03:31:52.319483800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 569/569 [01:04<00:00,  8.85it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(trainer.model, validation_dataset_processed['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:36:00.831782900Z",
     "start_time": "2023-12-23T03:36:00.820787100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned accuracy: 0.3979009106343572\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print('finetuned accuracy:', gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### GPT2 Fine-tuning Experiment Results\n",
    "I conducted some experiments to see how well the model could adapt to the posts of a randomly selected user. On the raw posts the answer is not well. I found that Reddit contains a lot of domain specific vocabulary. Typos in posts also complicate things.\n",
    "\n",
    "To try to circumnavigate this issues, I used a T5-based grammar correction model. Due to the aforementioned issues, if this model saw unfamiliar words, it would make a guess and replace those works with something it knew. Moreover, the model also sometimes changed the sentence structures if it recognized unfamiliar grammar.\n",
    "\n",
    "So even though in the above tests, we see the model achieve a 39.7% accuracy after fine-tuning (finally beating the baseline), before this prototype can become a real app, we would need to solve the issues of domain specific vocabulary.\n",
    "\n",
    "##### Challenges with Domain Specific\n",
    "\n",
    "I conducted significant research on adapting models to domain specific vocabulary. Unfortunately, generally speaking, there is no good way to do it without retraining the entire model. Approaches involving unfreezing weights of the embedding layers and simply updating existing vocabulary exist however, according to some sources, this only works in the amount of new vocabulary is small. And even then, we simply may not have enough data to update the embeddings for those models.\n",
    "\n",
    "One solution I see would be to collect massive amounts of data on what people most often type on their phones, retrain the entire GPT2 model (or something else) from scratch on that data and then allow small fine-tuning to be done on user-devices. However, I neither have the skill-set or the resources to pull something like that off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Continuous Learning and The Problem of Catastrophic Forgetting\n",
    "So far, in the examples we have tested, we have trained our model on the entire dataset at ones (as opposed to training the model on the data sequentially). That is, in the real world, in the type of application we want to develop, our model would be exposed to snippets of data over a long period of time. However, this introduces the problem of \"catastrophic forgetting\" where the model, while optimizing the weights on the new data forgets about the old tasks it learned to do.\n",
    "\n",
    "To overcome this problem, we turn to DeepMind research paper titled [\"Overcoming catastrophic forgetting in neural networks\"](https://arxiv.org/abs/1612.00796). This paper arrives upon 3 key insights to help develop an algorithm for the aforementioned problem:\n",
    "1. Many optimal configurations of the set of weights and biases θ exist.\n",
    "2. It is likely that one those optimal configurations is close to the previous learned tasks.\n",
    "3. It is therefore possible to constraint parameters to stay in a region of low error for previous learned tasks during training.\n",
    "\n",
    "Inspired by biological mechanisms found in real life, the paper refers to this algorithm as elastic weight consolidation (EWC). EWC is essentially a new loss function. This is exactly what we are looking for because:\n",
    "1. Ofcourse now we can learn new tasks without forgetting the others.\n",
    "2. EWC allows us to define the importance of each tasks relative to each other. In theory, this means we could classify more recent examples as more important! This gives us a lot of power.\n",
    "3. EWC can be trained for an arbitrary number of new tasks.\n",
    "\n",
    "##### How I will be using EWC\n",
    "Despite discouraging results in previous experiments we conducted in this notebook, another advantage of this approach is the model in many cases improves performances compared to standard fine-tuning approaches. Therefore, its worth experimenting!\n",
    "\n",
    "I plan to split the 1.5 years of data into smaller chunks. These will be the \"tasks\" EWC will be training on. Every \"task\" represents new data coming in that the model needs to be fine-tuned for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### EWC - Elastic Weight Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:52:44.002255200Z",
     "start_time": "2023-12-23T04:52:43.983186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words_required = 1600\n",
    "num_epochs = 1\n",
    "ewc_lambda = 0.99  # how important previously learned stuff is\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:51:59.400079800Z",
     "start_time": "2023-12-23T04:51:10.084665400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:47<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 56 different task sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into different \"tasks\" where each task is composed of at least 1200 words\n",
    "# every 1200 words we update the model using ewc\n",
    "# at the end of each task set, we see how well the model is able to predict the words in the next task set\n",
    "# we consider a single post the smallest divisible unit\n",
    "task_sets = []\n",
    "current_set_len = 0\n",
    "current_task_set = []\n",
    "for data_entry in entire_dataset_list:\n",
    "    post = data_entry[\"post\"]\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    current_set_len += len(tokenized_words)\n",
    "    current_task_set += [data_entry]\n",
    "    if current_set_len >= num_words_required:\n",
    "        task_sets += [current_task_set]\n",
    "        current_task_set = []\n",
    "        current_set_len = 0\n",
    "task_sets += [current_task_set]  # last task set may have less than 1000 words\n",
    "\n",
    "task_datasets = []\n",
    "disable_progress_bar()\n",
    "for task_set in tqdm(task_sets):\n",
    "    # no need to split this data into training and eval\n",
    "    # because we next task is the evaluation\n",
    "    task_set = Dataset.from_list(task_set)\n",
    "    task_set_processed = task_set.map(encode, batched=True, batch_size=len(task_set))\n",
    "    task_set_processed.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "    task_datasets += [task_set_processed]\n",
    "enable_progress_bar()\n",
    "\n",
    "print(f'Created {len(task_datasets)} different task sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:52:49.223204400Z",
     "start_time": "2023-12-23T04:52:47.964015900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device=device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:52:50.641565300Z",
     "start_time": "2023-12-23T04:52:50.633047400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def on_task_update(model, device, task_id, optimizer, training_data_loader):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # accumulating gradients\n",
    "    for batch in training_data_loader:\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "    fisher_dict[task_id] = {}\n",
    "    optpar_dict[task_id] = {}\n",
    "\n",
    "    # gradients accumulated can be used to calculate fisher\n",
    "    for name, param in model.named_parameters():\n",
    "        # this fills up GPU memory real fast - must calc this on CPU :(\n",
    "        optpar_dict[task_id][name] = param.data.clone().cpu()\n",
    "        fisher_dict[task_id][name] = param.grad.data.clone().pow(2).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:52:51.318289100Z",
     "start_time": "2023-12-23T04:52:51.288289200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, device, task_id, optimizer, training_data_loader):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_data_loader))\n",
    "\n",
    "    for batch in training_data_loader:\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        ### magic here! :-)\n",
    "        # only worry about up to 12 tasks otherwise it takes too long to train on\n",
    "        # more tasks since we had to keep track of the losses for ALL tasks!\n",
    "        for task in range(0 if task_id - 12 < 0 else task_id - 12, task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fisher_dict[task][name]\n",
    "                optpar = optpar_dict[task][name]\n",
    "                param = param.cpu()\n",
    "                loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(training_data_loader)\n",
    "    print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T04:53:19.166652200Z",
     "start_time": "2023-12-23T04:53:12.032201100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 82/82 [00:07<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.34561891515994436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the first sample dataset with out any training\n",
    "target_data_set = task_datasets[0]['post']\n",
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(model, target_data_set)\n",
    "first_task_set_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print(\"Accuracy: \", distilgpt2_correct_predictions / distilgpt2_total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T06:01:20.912555100Z",
     "start_time": "2023-12-23T04:53:20.202997900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task:  0\n",
      "Average Loss: 2.1817838928916236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                                        | 1/56 [00:08<08:02,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15950069348127602\n",
      "Training on task:  1\n",
      "Average Loss: 1.6112454398111864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▍                                                                                                                      | 2/56 [00:21<09:53, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2241498959056211\n",
      "Training on task:  2\n",
      "Average Loss: 0.8571711778640747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▌                                                                                                                    | 3/56 [00:42<13:45, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.23624161073825503\n",
      "Training on task:  3\n",
      "Average Loss: 0.8925763517618179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████▊                                                                                                                  | 4/56 [01:09<17:31, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.24833997343957503\n",
      "Training on task:  4\n",
      "Average Loss: 0.8251875319651195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████▉                                                                                                                | 5/56 [01:39<20:01, 23.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35305851063829785\n",
      "Training on task:  5\n",
      "Average Loss: 1.1980485692620277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████████▏                                                                                                             | 6/56 [02:12<22:20, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3116036505867014\n",
      "Training on task:  6\n",
      "Average Loss: 0.7906135022640228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████▍                                                                                                           | 7/56 [02:49<24:37, 30.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33957033957033955\n",
      "Training on task:  7\n",
      "Average Loss: 0.7197588194500316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████████████▌                                                                                                         | 8/56 [03:32<27:31, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3324022346368715\n",
      "Training on task:  8\n",
      "Average Loss: 0.9724736457521265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████▊                                                                                                       | 9/56 [04:25<31:24, 40.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.332892998678996\n",
      "Training on task:  9\n",
      "Average Loss: 0.734594430242266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████▊                                                                                                    | 10/56 [05:10<31:48, 41.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3355525965379494\n",
      "Training on task:  10\n",
      "Average Loss: 0.4784785636833736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▉                                                                                                  | 11/56 [06:02<33:40, 44.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3210455764075067\n",
      "Training on task:  11\n",
      "Average Loss: 0.8651777356863022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████████████████▏                                                                                               | 12/56 [07:06<37:02, 50.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3492723492723493\n",
      "Training on task:  12\n",
      "Average Loss: 0.9186270296573639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████▎                                                                                             | 13/56 [08:29<43:15, 60.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.40273972602739727\n",
      "Training on task:  13\n",
      "Average Loss: 0.5085960262351565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████▌                                                                                           | 14/56 [09:56<48:04, 68.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35444743935309975\n",
      "Training on task:  14\n",
      "Average Loss: 1.0238864885436163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████████▋                                                                                         | 15/56 [11:22<50:22, 73.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.32070175438596493\n",
      "Training on task:  15\n",
      "Average Loss: 1.04454272488753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████▊                                                                                       | 16/56 [13:14<56:47, 85.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.363758389261745\n",
      "Training on task:  16\n",
      "Average Loss: 0.7084984555840492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████████████                                                                                     | 17/56 [14:46<56:48, 87.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3233695652173913\n",
      "Training on task:  17\n",
      "Average Loss: 0.8418941274285316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████████████████▏                                                                                  | 18/56 [16:03<53:16, 84.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3382352941176471\n",
      "Training on task:  18\n",
      "Average Loss: 0.5226737130433321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████▍                                                                                | 19/56 [17:20<50:38, 82.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3689655172413793\n",
      "Training on task:  19\n",
      "Average Loss: 0.6441682040691376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████████████████▌                                                                              | 20/56 [18:55<51:29, 85.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3597025016903313\n",
      "Training on task:  20\n",
      "Average Loss: 1.1114078760147095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████████████████████▊                                                                            | 21/56 [20:13<48:44, 83.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37774816788807464\n",
      "Training on task:  21\n",
      "Average Loss: 1.0194129794836044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████████████████▉                                                                          | 22/56 [21:33<46:43, 82.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33062330623306235\n",
      "Training on task:  22\n",
      "Average Loss: 0.7488929405808449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████                                                                        | 23/56 [22:49<44:19, 80.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.41168289290681503\n",
      "Training on task:  23\n",
      "Average Loss: 0.9089638617905703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████████████████████████████████████████▎                                                                     | 24/56 [24:32<46:30, 87.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3902777777777778\n",
      "Training on task:  24\n",
      "Average Loss: 0.6947686794129285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████████████████████████▍                                                                   | 25/56 [26:14<47:28, 91.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3239917976760082\n",
      "Training on task:  25\n",
      "Average Loss: 1.0622787078221638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████████████████████████▋                                                                 | 26/56 [27:46<45:57, 91.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.19302775984506132\n",
      "Training on task:  26\n",
      "Average Loss: 0.5032412101115499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████▊                                                               | 27/56 [29:11<43:22, 89.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.11223203026481715\n",
      "Training on task:  27\n",
      "Average Loss: 1.3098162412643433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████                                                             | 28/56 [29:33<32:19, 69.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3107067879636109\n",
      "Training on task:  28\n",
      "Average Loss: 1.085995301604271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████▏                                                          | 29/56 [31:20<36:20, 80.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.31272210376687987\n",
      "Training on task:  29\n",
      "Average Loss: 0.26925005806753266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████████████████████████████████████████▎                                                        | 30/56 [33:19<39:57, 92.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.31073825503355706\n",
      "Training on task:  30\n",
      "Average Loss: 0.8282691687345505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████████████████████████████████▌                                                      | 31/56 [34:55<38:51, 93.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3678526048284625\n",
      "Training on task:  31\n",
      "Average Loss: 0.9162320664950779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████████████████████████████▋                                                    | 32/56 [36:06<34:35, 86.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33155792276964047\n",
      "Training on task:  32\n",
      "Average Loss: 0.4020188365663801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████████████████████████████▉                                                  | 33/56 [37:14<31:05, 81.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35637583892617447\n",
      "Training on task:  33\n",
      "Average Loss: 0.3106099548084395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████████████████████████████████████████████████████                                                | 34/56 [38:27<28:50, 78.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35135135135135137\n",
      "Training on task:  34\n",
      "Average Loss: 0.8462059668132237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████████████████████▎                                             | 35/56 [39:36<26:28, 75.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3617511520737327\n",
      "Training on task:  35\n",
      "Average Loss: 0.6059134410960334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████████████████████████████████▍                                           | 36/56 [40:49<25:02, 75.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3478802992518703\n",
      "Training on task:  36\n",
      "Average Loss: 1.0959935933351517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████▌                                         | 37/56 [41:36<21:06, 66.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.31538948701709946\n",
      "Training on task:  37\n",
      "Average Loss: 1.4657848278681438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████▊                                       | 38/56 [42:14<17:24, 58.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3807667316439246\n",
      "Training on task:  38\n",
      "Average Loss: 1.0022767441613334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████████▉                                     | 39/56 [43:22<17:15, 60.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3715441672285907\n",
      "Training on task:  39\n",
      "Average Loss: 0.6394879668951035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████████████████████████████████▏                                  | 40/56 [44:39<17:33, 65.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3695364238410596\n",
      "Training on task:  40\n",
      "Average Loss: 0.741868756711483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████████████████▎                                | 41/56 [45:57<17:22, 69.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3065883887801696\n",
      "Training on task:  41\n",
      "Average Loss: 0.5866477519273758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████▌                              | 42/56 [46:49<14:58, 64.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.39325842696629215\n",
      "Training on task:  42\n",
      "Average Loss: 0.9118811885515848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████▋                            | 43/56 [47:57<14:07, 65.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3516260162601626\n",
      "Training on task:  43\n",
      "Average Loss: 0.6418066438701417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████▊                          | 44/56 [49:24<14:22, 71.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.31977946243969674\n",
      "Training on task:  44\n",
      "Average Loss: 0.5761830264871771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████                        | 45/56 [51:07<14:51, 81.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3640776699029126\n",
      "Training on task:  45\n",
      "Average Loss: 0.49954594536261127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 46/56 [52:51<14:40, 88.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.38540949759119064\n",
      "Training on task:  46\n",
      "Average Loss: 0.5716968074440956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                   | 47/56 [54:26<13:29, 89.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3823738450604122\n",
      "Training on task:  47\n",
      "Average Loss: 0.7014696101347605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                 | 48/56 [56:39<13:44, 103.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3724137931034483\n",
      "Training on task:  48\n",
      "Average Loss: 0.9921123325824738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 49/56 [58:11<11:38, 99.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3427004797806717\n",
      "Training on task:  49\n",
      "Average Loss: 0.831055811047554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 50/56 [59:46<09:49, 98.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3721590909090909\n",
      "Training on task:  50\n",
      "Average Loss: 0.6955816012162429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 51/56 [1:01:42<08:38, 103.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3476775956284153\n",
      "Training on task:  51\n",
      "Average Loss: 0.8510771935636346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 52/56 [1:03:23<06:51, 102.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35704323570432356\n",
      "Training on task:  52\n",
      "Average Loss: 0.9939664254585902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 53/56 [1:05:13<05:14, 104.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.30192435301924353\n",
      "Training on task:  53\n",
      "Average Loss: 0.40428907317774637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 54/56 [1:06:22<03:08, 94.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33423180592991913\n",
      "Training on task:  54\n",
      "Average Loss: 0.8802333399653435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 55/56 [1:07:38<01:28, 88.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.34563106796116505\n",
      "Training on task:  55\n",
      "Average Loss: 1.5960047841072083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 55/56 [1:08:00<01:14, 74.19s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     train_ewc(model, device, \u001b[38;5;28mid\u001b[39m, optimizer, training_data_loader)\n\u001b[1;32m     16\u001b[0m     on_task_update(model, device, \u001b[38;5;28mid\u001b[39m, optimizer, training_data_loader)\n\u001b[0;32m---> 18\u001b[0m target_data_set \u001b[38;5;241m=\u001b[39m \u001b[43mtask_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times \u001b[38;5;241m=\u001b[39m test_model_accuracy(model, target_data_set, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, distilgpt2_correct_predictions \u001b[38;5;241m/\u001b[39m distilgpt2_total_predictions)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ewc_accs = [first_task_set_accuracy]\n",
    "\n",
    "for id, task_dataset in tqdm(enumerate(task_datasets), total=len(task_datasets)):\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        dataset=task_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    print(\"Training on task: \", id)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_ewc(model, device, id, optimizer, training_data_loader)\n",
    "        on_task_update(model, device, id, optimizer, training_data_loader)\n",
    "\n",
    "    if len(task_datasets) >= id+1:\n",
    "        continue  # end of loop\n",
    "\n",
    "    target_data_set = task_datasets[id+1]['post']\n",
    "    distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(model, target_data_set, False)\n",
    "    print(\"Accuracy: \", distilgpt2_correct_predictions / distilgpt2_total_predictions)\n",
    "    ewc_accs += [distilgpt2_correct_predictions / distilgpt2_total_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T06:22:17.488795700Z",
     "start_time": "2023-12-23T06:22:17.358714200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1487a5c313c0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ewc_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Conclusion and Limits of EWC\n",
    "\n",
    "After this project, I have a lot more respect for the naive but effective ngram models on my phone. Without significant investments into designing effective solutions specifically those that can run on mobile devices I don't see n-grams going away anytime soon.\n",
    "\n",
    "Sure we can beat ngrams with enough GPU power, plenty of ram, and SSD storage. But most smartphones in existent today don't have that capability.\n",
    "\n",
    "\n",
    "#### Limits of EWC\n",
    "I found that despite having its advantages, Elastic Weight Consolidation (EWC) has its limits. Specifically:\n",
    "* EWC needs to keep track of each task for which it wants to prevent \"catastrophic forgetting\"\n",
    "* Consequently, it takes a lot of memory to store the \"gradients\" that EWC needs to track to figure out which parameters are important.\n",
    "* Since the amount of memory needed to run EWC is large, it may not be practical to store the gradients on the GPU (as was the case here). This requires moving the gradients tracked by EWC to and from the CPU all the time significantly increasing training time.\n",
    "* The more tasks EWC has to account for, the slower it gets. I dealt with this problem by limiting the tasks EWC is required to track to the 12 recent ones. However, this just made EWC likely to \"catastrophic forgetting\" for some of the weights resulting in unreliable performance.\n",
    "\n",
    "#### Results\n",
    "As seen by the final plot as produced above, EWC's performance in this setting was not super reliable. Yes, in some cases, it was able to score as high as 40%. With over 1600 words in each target dataset, I don't think this was due to random chance. EWC did learn the right patterns. However, due to technical limitations I imposed on the models as explained above, EWC was only allowed to keep track of up to 12 most recent task sets to maintain reasonable training times. On a smartphone, even just 12 task sets would be too many due to the stringent ram limitations on such devices. Therefore, my theory on what happened is that the \"forgetting\" of old tasks allowed the wrong weights to be affected causing unreliable performance. I also tried several other variations of hyperparameters with no-better results.\n",
    "\n",
    " At least until smartphones become significantly more powerful (and come built in with GPUs) or breakthroughs for CPU training happens. In its current state, the only way something like \"NextType\" could work is if all typed data is sent to a powerful machine for calculations, and suggestions are streamed back onto the smartphones. Naturally with privacy concerns this is impractical. All in all, at 146MB, the ngrams baseline model was the king, and it seems looking at the challenges that it will stay that way for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Further exploration:\n",
    "\n",
    "If more time was available, further extentions of this projects may include:\n",
    "* Allowing new vocabulary to be learned.\n",
    "* Trying this experiment with T5 or another model.\n",
    "* Using custom grammar correction models for better data pre-processing.\n",
    "* More messing around with hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resources and Publications\n",
    "\n",
    "Special thanks to the following resources and research papers for making this project possible:\n",
    "\n",
    "1. Tunstall, L., Werra, L., Wolf, T., &amp; Géron, A. (2022). Natural language processing with transformers: Building language applications with hugging face. O’Reilly.\n",
    "\n",
    "> Repeatedly recommended by both Professor Snyder and other students in CS505, this book helped familiarize me with the hugging face eco-system.\n",
    "\n",
    "2. \"Transformers\" Hugging Face, https://huggingface.co/docs/transformers/. Accessed 1 Dec. 2023.\n",
    "\n",
    "> Various documentation pages on using the Hugging face library were invaluable help!\n",
    "\n",
    "3. Kirkpatrick, James, et al. “Overcoming Catastrophic Forgetting in Neural Networks.” arXiv.Org, 25 Jan. 2017, arxiv.org/abs/1612.00796.\n",
    "\n",
    "> The original ground-breaking paper that introduced the Elastic Weight Consolidation (EWC) algorithm used in this project.\n",
    "\n",
    "4. https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb\n",
    "\n",
    "> Example notebook by the ContinualAI open source community whose code was adapted to help me my own EWC implementation.\n",
    "\n",
    "5. https://212digital.medium.com/fine-tuning-the-gpt-2-large-language-model-unlocking-its-full-potential-66e3a082ab9c\n",
    "\n",
    "> Another resource on Fine-Tuning, this one specifically for GPT2, the model I used in this project.\n",
    "\n",
    "6. RaiderBDev, et al. “Reddit Comments/Submissions 2005-06 to 2023-09.” https://academictorrents.com/details/89d24ff9d5fbc1efcdaf9d7689d72b7548f699fc\n",
    "\n",
    "> Neatly organized scrapped data from reddit. This is where the raw data used in this project was extracted from.\n",
    "\n",
    "7. https://github.com/huggingface/tokenizers/issues/1160\n",
    "\n",
    "> A GitHub conversation on the hugging face GitHub discussing vocab updating strategies.\n",
    "\n",
    "8. https://huggingface.co/pszemraj/grammar-synthesis-small\n",
    "\n",
    "> The grammar correction pipeline used in the project. It uses T5-small on the backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
