{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next Type - A Mobile Typing Assistant\n",
    "[CS 505 - NLP] [Final Project]\n",
    "Completed by Muhammad Aseef Imran\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Compared to typing on a keyboard, typing on our phones considerably slower. Luckily most phones come built in with a feature to predict your next word to make up for this. However, after much experimentation, it seems most of these prediction algorithms seem to use simple N-grams with a window of around 3-4 words of left context. Many times, predicting the next word based on simple N-gram based probabilities work \"fine\" but often, this strategy produces poor results due to ignoring the context of the sentence. As NLP technology leaps forward exponentially we can do much better than simplistic N-grams. For this reason **Next Type** aims to bring the next generation of typing experiance to its users in order to raise productivity and typing speed for users leaving more time for the important things in life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"assets/bad-phone-example-1.jpg\" alt=\"N-gram example 1\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-2.jpg\" alt=\"N-gram example 2\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-3.jpg\" alt=\"N-gram example 3\" style=\"width: 33%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Presented above are three examples where unfortunately, the standard N-gram model employed on \"modern\" devices fails miserably.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Goals\n",
    "\n",
    "N-gram models perform very poorly with shorter context window, and any large context windows require an immense amount of data to produce reasonable results. Moreover, at least in this case, our N-gram models do not consider the right context when suggesting words. However, despite having their weaknesses, the N-gram approach also has some nice benefits. Particularly, the nature of the N-gram model allows it to be easily updated with new data and adapt to the typing habits of its users with little processing power.\n",
    "\n",
    "Keeping these things in mind, we can summarize our goals for the model we have set out to develop as:\n",
    "1. The model should consider both the left and right context before suggesting a \"natural\" word that fits the context.\n",
    "2. The model should be able to adapt to or learn from the user's word choices in various contexts.\n",
    "3. The model should be able to learn the user's word choice as—outlined above—quick enough to be useful.\n",
    "4. The model should be reasonably sized allowing it to be run and be updated on most modern-phone hardware with in reasonable time.\n",
    "\n",
    "Additional things that may be considered (depending on time) in our model may be that:\n",
    "\n",
    "5. The model should validate the input data for grammatical correctness to avoid learning incorrect patterns.\n",
    "6. The model should (optionally) avoid suggesting profane language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Plan and Exploring Potential Solutions\n",
    "\n",
    "Once again, our goal is to accuractely and effectively predict the user's next token in real-time while adapting to the user's behavior and writing style over time.\n",
    "\n",
    "In order to reasonably meet these goals, we will fine-tune one or more combination of existing models such as T5, Bert, and GPT2, and/or their \"Distilled\" counterparts. We will use the Hugging Face transformers library simply due to its vast popularity and easy of use. I intend to use the SCC for rapid prototyping and experimentation as I already have significant experiance using the SCC at this stage.\n",
    "\n",
    "Finally I should note that in my initial research, I have identified potential pitfalls with each of these models and their strengths and weaknesses for my task. However, further experimentation will be needed to make a final decision on which model (or model combinations) to use. Detailed experimentation with each model will be required to evaluate its pros/cons.\n",
    "\n",
    "#### Bert\n",
    "Having been trained on a mask-fill task, Bert naturally lends itself to the kind of project I am trying to do. Being a relatively small model, and still quite versatile for the task, Bert may be a great choice. However, one downside to Bert is that Bert seems to perform poorly when attempting to Mask-Fill multiple words in the middle of a sentence. (See the bottom of this notebook for a demonstration).\n",
    "\n",
    "#### GPT2\n",
    "GPT2 was essentially trained on predicting the next word. Indeed, this is the task we want to achieve ourselves. However, in some cases, we may need to predict the middle word (if someone is editing the middle of a sentence they wrote). This is not a task GPT2 was designed for although this may still be possible due to the surprising generality of the model. Further research and experimentation will be needed.\n",
    "\n",
    "#### T5\n",
    "T5 is an extremely general purpose model than can adapt to many NLP tasks. Unlike bert Being a substantially larger model than both GPT2 and Bert, T5 is slower to retrain. Yet at the same time, T5 seems to do a much better job mask-filling between sentences. Yet, in a realistic scenario how often does one write in the middle of the sentence? Is the increased computing cost really worth it? These are the questions I hope to answer with the first stages of my research.\n",
    "\n",
    "#### The \"Distilled Version\"\n",
    "Models like Distilled-Bert and Distilled-GPT2 are indeed smaller. However, one *major* pitfall to this may be that the model may struggle to generalize to new tasks and during retraining. Retraining forms an essential component to this project and depending on the severity of this effected, the \"Distilled\" models may prove ineffective. Further experimentation is needed to make any conclusions, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Questions & Challenges\n",
    "\n",
    "Let us better define the problem and explain exactly what we are seeking to achieve and possible challanges.\n",
    "\n",
    "1. People evolve and change. So can I get a better accuracy by consider ALL known history? Or only \"recent\" history? Should the more recent history be weighted more\"? If so, should history be judged more by time or the volume typed? If I'm wrong about this, then more data = better. OR the time people change in is just longer.\n",
    "2. My problem is my data is ever evolving. So how do I prevent the model from overfitting? Overfitting will make newer data harder to generalize. (i.e. How do I prevent it from forgetting the stuff it knew in the base model?). Also suppose if I don't want my model to consider the \"old\" history, how can I make the model \"forgot\" the old stuff (some kind of vanishing gradient is my best choice)?\n",
    "\n",
    "Shortcomings of this research: data may be biased. Only considering reddit users. Particularly those who type somewhat frequently on reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Obtaining the Fine-Tuning Data\n",
    "\n",
    "It is important for my task that the fine-tunning data only come from a single person since my goal is to train the model to adapt to the typing behavior of a specific user. In order to achieve this we can use scraped messages from reddit for specific users and examine how the model adapts to their specific word-choice and typing habits. Particularly, we will be using data from a pre-scrapped dataset [Reddit comments/submissions 2005-06](https://academictorrents.com/details/89d24ff9d5fbc1efcdaf9d7689d72b7548f699fc). Further, we also want to make sure that sample data from any user we use:\n",
    "* Provides a reasonably large enough dataset to train on\n",
    "* Posts regularly (as opposed to posting a lot occasionally)\n",
    "\n",
    "With that defined, our focus will be between and including messages posted between [1/2011 - 6/2012]. Why this particular time range? No particular reason besides that data with in this time range was reasonably enough sized to be processed quickly yet still leave us with enough data to work with.\n",
    "\n",
    "We can then process the reddit dump creating a dictionary consisting of reddit users and the messages they sent. We further filter this data as follows:\n",
    "* \"Deleted\" users aren't included\n",
    "* First, we remove all authors with less than 546 (i.e. they must average more than 1 post a day) - although future filteration steps would've already ensured this requirement, we start of with this since this is a quick and dirty elimination step allowing for quicker processing in the next steps (which are a bit more complicated implementation wise)\n",
    "* Second, we filter to only authors that made at least 18 post per month in the time range without missing any month.\n",
    "* Third, we filter to only authors that made at least 2 post every week without missing a week.\n",
    "* Fourth, we filter down the remaining users to those that posted on at least 80% of the days.\n",
    "* In our final step, we post process the post messages by running it through a sequence-to-sequence model already developed by someone correcting silly grammatical errors. Otherwise, we may end up having messages in our data set that contain non-existent vocab.\n",
    "TODO: grammer corrrection may be needed! https://huggingface.co/pszemraj/grammar-synthesis-small/tree/main also trained on t5\n",
    "or https://huggingface.co/flexudy/t5-small-wav2vec2-grammar-fixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation Stategy\n",
    "\n",
    "In light of our above outlined goals, we have two major evaluation goals:\n",
    "\n",
    "\n",
    "1. How well does the model predict the user's next token after having seen x tokens of examples from the user. In other words, not only how well the model predicts the user's next token but also how fast the model improves its prediction as a function of the data it has already seen?\n",
    "\n",
    "> We can evaluate \"how well\" the model predicts the user's next token by measuring the loss between what the user actually types vs what the model suggests. Then, we can further measure this loss as function of the number of tokens of examples the model has seen during its Fine-Tuning. For example, how does the loss change after the model has seen 1000 tokens of examples from the user?\n",
    "\n",
    "2. How much computation is needed is to both run the model and update the model on new data?\n",
    "\n",
    "> Measuring how long various parts of the model such prediction and training takes is trivial. (We can simply calculate the time between the target area of code). We may then analyze the run-time in context of the hardware the code is run on and comparing this information with current state of computational power of modern mobile devices. This information can be used to make an informed decision on the sequence lengths to input to the model to ensure our model can suggest new words to users in real-time on standard mobile hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Limitations\n",
    "* Compared to the N-gram approach, this new model cannot easily learn new words?\n",
    "* You may talk different with friends vs family vs boss. This training and results was done specifically for reddit. It may be the case that the model will not generalize as well to a broader domain in an actual key board app. (Still probability at least better than the ngram stuff right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resources and Publications\n",
    "\n",
    "For this project, I am considering/planning to use the following resources for research:\n",
    "\n",
    "1. Tunstall, L., Werra, L., Wolf, T., &amp; Géron, A. (2022). Natural language processing with transformers: Building language applications with hugging face. O’Reilly.\n",
    "\n",
    "> This book has been repeatedly recommended by both Professor Snyder and other students in CS505. Upon a coarse inspection, I expect to particular find the sections on \"Fine Tuning\" various models helpful (since this is very much a Fine-Tuning project). The book also contains extensive details about various transformer architectures which will inevitably prove useful.\n",
    "\n",
    "2. \"Fine-Tune a Pretrained Model.\" Hugging Face, https://huggingface.co/docs/transformers/training. Accessed 1 Dec. 2023.\n",
    "\n",
    "> This blog post contains examples Fine-Tuning bert using three different methods along with model evaluation. Although unlike source (#1), it goes into less details on the theory, the sample code is more rich, and easier to work with.\n",
    "\n",
    "3. Raffel, Colin, et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Arxiv, 23 Oct. 2019, Accessed 1 Dec. 2023.\n",
    "\n",
    "> This is the original paper that introduced T5 to the world. As covered in lecture, similar to Bert, T5 was (partially) trained on Mask-Fill task where random spans of texts were removed which naturally make it a great candidate for what we want to achieve here. However, this Mask-Fill process was only part of the process. T5 is a very versitile (and large) model, and has huge applications. There is no better place to learn what it is, how it works, and how to use it than the original paper!\n",
    "\n",
    "4. https://212digital.medium.com/fine-tuning-the-gpt-2-large-language-model-unlocking-its-full-potential-66e3a082ab9c\n",
    "\n",
    "> This great blog post contains all the coded needed to get started on fine-tuning GPT2 and the warnings about the various pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparison and Exploratory Between Various Models\n",
    "\n",
    "We will begin our model comparisons by testing Bert, GPT2, and T5 on the following 5 sentences to see how well the models do. These results are just to get a basic idea of each model's capabilities. We will \"eyeball this result\". Note that the \"|\" token represents the \"cursor\" location.\n",
    "\n",
    "1. 'After forcefully breaking into the bank, they|'\n",
    "2. 'Every |, my family and I visit Hawaii.'\n",
    "3. 'In my family, there is my |'\n",
    "4. 'My favorite | is apple.'\n",
    "5. 'It has been 2 months since I graduated. However, unfortunately I still haven't found a |. At this rate I won't be able to pay rent!'\n",
    "\n",
    "After that, we will test each model to see how well the model does in predicting this \"next\" word token. This task is the most important task for our proposed application to do well. Next, we will see how well each model does at predicting a middle token. Finally, we will choose the most promising model to develop a fine-tuning method for this continuous learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup: Imports and loading Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T18:59:47.094934600Z",
     "start_time": "2023-12-10T18:59:47.088841900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs505ws/aseef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports here\n",
    "from typing import Union\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List, Set, Any, Union\n",
    "import statistics\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, PreTrainedTokenizerBase, PreTrainedModel, GPT2Config\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, DistilBertConfig, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:01:44.104709200Z",
     "start_time": "2023-12-10T20:01:44.100708Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"After forcefully breaking into the bank, they |\", # predict the next word using context\n",
    "    \"In my family, there is my |\", # predict the next word using context\n",
    "    \"Every |, my family and I visit Hawaii.\",  # simple mask fill with one missing word\n",
    "    \"My favorite | is apple.\", # simple mask fill with one missing word\n",
    "    \"It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\", # need multiple words here before sentence makes sense\n",
    "    \"Following the American Civil War, | assassinated.\"  # need multiple words here before sentence makes sense\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T18:56:01.743453900Z",
     "start_time": "2023-12-10T18:56:01.742453900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"/projectnb/cs505ws/projects/NextType/data\"\n",
    "\n",
    "def does_var_exists_gz(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'{data_dir}/{var_name}.pkl.gz')\n",
    "\n",
    "def dump_var_gz(var_name: str, obj) -> None:\n",
    "    os.makedirs(f\"{data_dir}\", exist_ok=True)\n",
    "    with gzip.open(F'{data_dir}/{var_name}.pkl.gz', 'wb', compresslevel=1) as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_var_gz(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists_gz(var_name):\n",
    "        return None\n",
    "\n",
    "    file_path = F'{data_dir}/{var_name}.pkl.gz'  # Updated file extension\n",
    "    with gzip.open(file_path, 'rb', compresslevel=1) as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T18:56:01.819524900Z",
     "start_time": "2023-12-10T18:56:01.743453900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device={device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T18:56:33.002318300Z",
     "start_time": "2023-12-10T18:56:01.744475500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_to_posts_dict: Dict[str, Tuple[int, str]] = load_var_gz(\"author_to_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:53:42.433108600Z",
     "start_time": "2023-12-10T20:53:42.429105900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apriloneil',\n",
       " 'adenbley',\n",
       " 'rednukleus',\n",
       " 'KMFDM781',\n",
       " 'Echospree',\n",
       " 'OrangePlus',\n",
       " 'AyeMatey',\n",
       " 'Cheesejaguar',\n",
       " 'GreenStrong',\n",
       " 'KellyTheET',\n",
       " 'EasyReader',\n",
       " 'yoda17',\n",
       " 'scientologist2',\n",
       " 'Westhawk',\n",
       " 'erode',\n",
       " 'elastic-craptastic',\n",
       " 'alettuce',\n",
       " 'elapid',\n",
       " 'Cyrius',\n",
       " 'Doomdoomkittydoom',\n",
       " 'sinndogg',\n",
       " 'midas22',\n",
       " 'LiteweightPhenomenal',\n",
       " 'killswithspoon',\n",
       " 'CACuzcatlan']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_user_sample = random.sample(list(author_to_posts_dict.keys()), 25)\n",
    "random_user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:53:43.520099800Z",
     "start_time": "2023-12-10T20:53:43.500335Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample up to 60 posts from each user\n",
    "random_post_samples = []\n",
    "for rand_user in random_user_sample:\n",
    "    all_posts = author_to_posts_dict[rand_user]\n",
    "    sampled_posts = random.sample(list(all_posts), 60)\n",
    "    random_post_samples += sampled_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:18:59.306458300Z",
     "start_time": "2023-12-10T04:18:59.290280Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random_key = random.choice(list(author_to_posts_dict.keys()))\n",
    "# print(f'Testing on user u/{random_key}')\n",
    "random_key = 'The_Jackal'\n",
    "user_posts = author_to_posts_dict[random_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T02:31:01.059559900Z",
     "start_time": "2023-12-10T02:30:35.908380400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this was the only small-ish grammar correction model I found.\n",
    "# had I more time, I would create my own model. But I don't so I will focus\n",
    "# on my primary task.\n",
    "# the downsides of this models is that besides correcting spellings, it often alters the\n",
    "# structure of the sentence which could fundamentally undermine our purpose.\n",
    "# so question: does benefits of correcting grammar using this outweighs the harms?\n",
    "# after all, if the model doesnt recgonize a word, it'll just ignore it and wont learn from it!\n",
    "grammar_corrector = pipeline(\n",
    "               'text2text-generation',\n",
    "               'pszemraj/grammar-synthesis-small',\n",
    "                 device=device\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:19:15.374726500Z",
     "start_time": "2023-12-10T04:19:15.359333100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import html\n",
    "def normalize_text(post_text: str):\n",
    "    # get rid of new lines\n",
    "    post_text = re.sub(\"\\n\", \" \", post_text)\n",
    "    # remove html characters\n",
    "    post_text = html.unescape(post_text)\n",
    "    # Remove bold and italic formatting\n",
    "    post_text = re.sub(r'(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3', r'\\2\\4', post_text)\n",
    "    # Remove headers\n",
    "    post_text = re.sub(r'^#{1,6}\\s', '', post_text)\n",
    "    # Remove hyperlinks\n",
    "    post_text = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'\\1', post_text)\n",
    "    # Remove inline code\n",
    "    post_text = re.sub(r'`([^`]+)`', r'\\1', post_text)\n",
    "    # Remove block code\n",
    "    post_text = re.sub(r'```(?:[^`]+|`(?!``))*```', '', post_text)\n",
    "    # Remove lists (unordered and ordered)\n",
    "    post_text = re.sub(r'^\\s*([\\*\\-\\+]\\s|(\\d+\\.)\\s)', '', post_text)\n",
    "    post_text = re.sub(\"(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3\", \"\", post_text)\n",
    "    # remove double spaces\n",
    "    post_text = re.sub(\" {2,}\", \" \", post_text)\n",
    "    # replace urls\n",
    "    post_text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"{URL}\", post_text)\n",
    "    # replace references to a specific subreddit but just the token \"[subreddit]\"\n",
    "    post_text = re.sub(r\"(\\W)(r/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB REDDIT}\\3\", post_text)\n",
    "    post_text = re.sub(r\"(\\W)(/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB REDDIT}\\3\", post_text)\n",
    "    post_text = post_text.strip()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:19:04.388304500Z",
     "start_time": "2023-12-10T04:19:04.342428900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/3794 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(user_posts_corrected))):\n\u001b[1;32m      3\u001b[0m     creation, message \u001b[38;5;241m=\u001b[39m user_posts_corrected[i]\n\u001b[0;32m----> 4\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m(message)\n\u001b[1;32m      5\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(message)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_text' is not defined"
     ]
    }
   ],
   "source": [
    "user_posts_corrected = list(user_posts)\n",
    "for i in tqdm(range(len(user_posts_corrected))):\n",
    "    creation, message = user_posts_corrected[i]\n",
    "    message = normalize_text(message)\n",
    "    sentences = sent_tokenize(message)\n",
    "    for j in range(len(sentences)):\n",
    "        sent = sentences[j]\n",
    "        updated_message = grammar_corrector(sent)[0]['generated_text']\n",
    "        sentences[j] = updated_message\n",
    "    updated_message = ' '.join(sentences)\n",
    "    user_posts_corrected[i] = (creation, updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:54:09.504695300Z",
     "start_time": "2023-12-10T03:54:09.475765500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_var_gz(f\"{random_key}-corrected-posts\", user_posts_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:18:22.269864700Z",
     "start_time": "2023-12-10T04:18:22.239835600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_posts_corrected = load_var_gz('The_Jackal-corrected-posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:19:21.488013400Z",
     "start_time": "2023-12-10T04:19:21.407406700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "index = 0\n",
    "for old_post, new_post in zip(user_posts, user_posts_corrected):\n",
    "    normalized_post = normalize_text(old_post[1])\n",
    "    if old_post[1].strip() != new_post[1].strip():\n",
    "        print(\"-=+=--=+=--=+=--=+=--=+=--=+=-\")\n",
    "        print(f'Old Post {index}:', normalized_post)\n",
    "        print(\"~~+~~~~+~~~~+~~~~+~~~~+~~~~+~~\")\n",
    "        print(f'New Post {index}:', new_post[1])\n",
    "        print(\"-=+=--=+=--=+=--=+=--=+=--=+=-\")\n",
    "        counter += 1\n",
    "    index += 1\n",
    "    if counter > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T19:01:59.460616400Z",
     "start_time": "2023-12-10T19:01:57.364249100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load in the t5 model\n",
    "T5_path = 't5-small'\n",
    "t5_config = T5Config.from_pretrained(T5_path)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_path, legacy=False)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_path, config=t5_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:01:48.912757700Z",
     "start_time": "2023-12-10T20:01:48.677375300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('broke', 0.221464142203331), ('break', 0.21711383759975433), ('are', 0.210137739777565), ('were', 0.17779788374900818), ('will', 0.1734863519668579)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('year', 0.310012549161911), ('day', 0.26997795701026917), ('month', 0.15326142311096191), ('week', 0.14059579372406006), ('time', 0.12615227699279785)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('family', 0.27247941493988037), ('own', 0.25976160168647766), ('daughter', 0.15796758234500885), ('mother', 0.1555204540491104), ('home', 0.1542709469795227)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('apple', 0.46938860416412354), ('fruit', 0.16375479102134705), ('thing', 0.15048767626285553), ('pie', 0.11327830702066422), ('recipe', 0.10309061408042908)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('rent', 0.2471972554922104), ('rate', 0.22261089086532593), ('fixed', 0.21800415217876434), ('rental', 0.16750217974185944), ('flat', 0.1446855515241623)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('the', 0.31452736258506775), ('', 0.21242530643939972), ('American', 0.20625953376293182), ('many', 0.13480441272258759), ('an', 0.1319834440946579)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"<extra_id_0>\")\n",
    "    input_ids = t5_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    num_samples = 5\n",
    "    with torch.no_grad():\n",
    "        output = t5_model.generate(\n",
    "            input_ids,\n",
    "            num_beams=num_samples,\n",
    "            min_new_tokens=2,\n",
    "            max_new_tokens=2,\n",
    "            num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(output.sequences, probabilities):\n",
    "        decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        suggestions += [(decoded_output, prob.item())]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-10T21:05:27.051428800Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████████████▊                                                                                                                                            | 241/1500 [05:04<14:11,  1.48it/s]"
     ]
    }
   ],
   "source": [
    "correct_guesses = 0\n",
    "total_guesses = 0\n",
    "\n",
    "for creation, post in tqdm(random_post_samples):\n",
    "    sent_tokenized = sent_tokenize(post)\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" <extra_id_0>\"\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids = t5_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            num_samples = 7\n",
    "            output = t5_model.generate(\n",
    "                input_ids,\n",
    "                num_beams=num_samples,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=2,\n",
    "                num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for sample_output, prob in zip(output.sequences, probabilities):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            if prob.item() < 0.05:\n",
    "                # avoid bizarre suggestions by simply filtering out low prob\n",
    "                # terms. We don't HAVE TO suggest exactly 5 words\n",
    "                break\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            correct_guesses += 1\n",
    "        total_guesses += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T21:05:25.789739500Z",
     "start_time": "2023-12-10T21:05:25.763946800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3578509753757595\n"
     ]
    }
   ],
   "source": [
    "print(correct_guesses / total_guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Middle Token\"\n",
    "OK, predicting the next token is fun and all. But what if a user wants to add something to the middle of their sentence? Wouldn't it be nice to be able to both use left and right context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "#### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:59:07.645110400Z",
     "start_time": "2023-12-10T20:59:06.543214500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distbert_path = 'distilbert-base-cased'\n",
    "distbert_model = DistilBertForMaskedLM.from_pretrained(distbert_path).to(device=device)\n",
    "distbert_config = DistilBertConfig.from_pretrained(distbert_path)\n",
    "distbert_tokenizer = DistilBertTokenizer.from_pretrained(distbert_path, config=distbert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T21:03:07.533051Z",
     "start_time": "2023-12-10T21:03:07.368557200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.754231929779053), ('.', 7.455650806427002), ('escape', 6.470277786254883), (':', 6.367412567138672), ('find', 6.345893383026123)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442168712615967), ('day', 5.875978469848633), ('morning', 5.826065540313721), ('##night', 5.271542549133301), ('night', 5.131310939788818)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815901756286621), ('destiny', 6.189595699310303), ('.', 6.164964199066162), ('love', 6.162735462188721), ('family', 6.145948886871338)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.05435562133789), ('apple', 10.834715843200684), ('tree', 10.448901176452637), ('grape', 8.978236198425293), ('vegetable', 8.545184135437012)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.645590782165527), ('reasonable', 8.736655235290527), ('fixed', 7.847304344177246), ('satisfactory', 7.836097717285156), ('flat', 7.64866828918457)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271700382232666), ('he', 4.914365291595459), ('soldiers', 3.721961259841919), ('rebels', 3.621227502822876), ('others', 3.600985050201416)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"[MASK]\")\n",
    "    input_ids = distbert_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    num_samples = 5\n",
    "    with torch.no_grad():\n",
    "        output = distbert_model(input_ids)\n",
    "        predictions = output.logits\n",
    "\n",
    "    # Get the top-k predicted tokens and their probabilities\n",
    "    top_k = 5  # Adjust as needed\n",
    "    probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "        suggestions += [(sample_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T01:14:47.945971400Z",
     "start_time": "2023-12-10T01:14:46.894382900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: life, Probability: 6.206722259521484\n",
      "Token: place, Probability: 6.069875240325928\n",
      "Token: money, Probability: 5.827669620513916\n",
      "Token: ., Probability: 5.503716468811035\n",
      "Token: heart, Probability: 5.2692131996154785\n"
     ]
    }
   ],
   "source": [
    "# Text with a masked token\n",
    "text = \"The greedy robber took my [MASK]\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = distbert_tokenizer.tokenize(distbert_tokenizer.decode(distbert_tokenizer.encode(text)))\n",
    "inputs = distbert_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the position of the masked token\n",
    "mask_token_index = torch.where(inputs == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "    outputs = distbert_model(inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the top-k predicted tokens and their probabilities\n",
    "top_k = 5  # Adjust as needed\n",
    "probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "# Convert indices back to tokens\n",
    "predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "# Print the results\n",
    "for token, prob in zip(predicted_tokens, probs.tolist()):\n",
    "    print(f\"Token: {token}, Probability: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aseef\\PycharmProjects\\505-Final-Project\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Your test string with a [MASK] token\n",
      "Predicted Masked Token: binary\n"
     ]
    }
   ],
   "source": [
    "# Example: Load your data as a string\n",
    "your_data = \"Your long string data here\"\n",
    "\n",
    "# Tokenize the data and automatically mask 15% of tokens\n",
    "tokens = distbert_tokenizer.tokenize(distbert_tokenizer.decode(distbert_tokenizer.encode(your_data)))\n",
    "num_mask_tokens = int(0.15 * len(tokens))\n",
    "mask_indices = random.sample(range(len(tokens)), num_mask_tokens)\n",
    "\n",
    "for index in mask_indices:\n",
    "    tokens[index] = '[MASK]'\n",
    "\n",
    "# Convert masked tokens back to input IDs\n",
    "input_ids = distbert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Prepare data for training\n",
    "labels = [-100 if token != '[MASK]' else token_id for token, token_id in zip(tokens, input_ids)]\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "attention_mask = torch.ones_like(input_ids)  # Assuming all tokens are attended to\n",
    "\n",
    "labels = torch.tensor(labels).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 3  # Replace with your desired number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    distbert_model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Optional: Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Test the updated model on mask-fill tasks\n",
    "distbert_model.eval()\n",
    "\n",
    "# Example: Test a masked input\n",
    "test_text = \"Your test string with a [MASK] token\"\n",
    "tokenized_test = distbert_tokenizer.tokenize(distbert_tokenizer.decode(distbert_tokenizer.encode(test_text)))\n",
    "masked_index = tokenized_test.index('[MASK]')\n",
    "\n",
    "input_ids_test = distbert_tokenizer.convert_tokens_to_ids(tokenized_test)\n",
    "input_ids_test = torch.tensor(input_ids_test).unsqueeze(0)  # Add batch dimension\n",
    "attention_mask_test = torch.ones_like(input_ids_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_test = distbert_model(input_ids=input_ids_test, attention_mask=attention_mask_test)\n",
    "    predictions = outputs_test.logits.argmax(dim=-1)\n",
    "\n",
    "predicted_token = distbert_tokenizer.convert_ids_to_tokens(predictions[0][masked_index].item())\n",
    "print(\"Original Text:\", test_text)\n",
    "print(\"Predicted Masked Token:\", predicted_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Middle Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Distil-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T01:23:48.697920400Z",
     "start_time": "2023-12-10T01:23:47.079501100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the GPT-2 model variant\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Move the model to the GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T01:16:16.366892500Z",
     "start_time": "2023-12-10T01:16:16.231460400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  life, Probability: 0.5644989013671875\n",
      "Token:  money, Probability: 0.32303494215011597\n",
      "Token:  wallet, Probability: 0.04720593988895416\n",
      "Token:  wife, Probability: 0.03292037919163704\n",
      "Token:  hand, Probability: 0.03233984857797623\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The greedy robber took my\"\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "# Generate probabilities for the next words\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "# we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "# after all, we don't want to suggest too many words!\n",
    "top_k = 5\n",
    "top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "# Get the probability distribution for the next word\n",
    "next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "# Normalize probabilities\n",
    "top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "# Convert the probabilities to a list\n",
    "top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "for token, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "    print(f\"Token: {tokenizer.decode(token)}, Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T01:28:20.113695400Z",
     "start_time": "2023-12-10T01:26:44.009883900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2144/2144 [01:36<00:00, 22.31it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "total_correct_predictions = 0\n",
    "total_wrong_predictions = 0\n",
    "\n",
    "m = 0\n",
    "for creation, target_sent in tqdm(user_posts):\n",
    "    split_sent = target_sent.split(' ')\n",
    "    if len(split_sent) == 0:\n",
    "        continue\n",
    "    correct_predictions = 0\n",
    "    wrong_predictions = 0\n",
    "    for i in range(len(split_sent) - 1):\n",
    "        prompt = ' '.join(split_sent[:i+1])\n",
    "        actual_next_word = split_sent[i+1]\n",
    "        # Tokenize the prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        if input_ids.shape[1] == 0:\n",
    "            continue\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "        top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "        # filter to only words with a probability greater than 1%\n",
    "        removal_marked = []\n",
    "        for j in range(len(top_k_probs_list)):\n",
    "            if top_k_probs_list[j] < top_p:\n",
    "                removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "        for to_remove in removal_marked:\n",
    "            top_k_probs_list.remove(to_remove[0])\n",
    "            top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "        guesses = set()\n",
    "        for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "            token = tokenizer.decode([token_id])\n",
    "            guesses.add(token)\n",
    "\n",
    "        # we consider a prediction correct if the actual word was\n",
    "        # one of the (up to) 5 words suggested\n",
    "        if ' ' + actual_next_word in guesses:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            wrong_predictions += 1\n",
    "\n",
    "    total_correct_predictions += correct_predictions\n",
    "    total_wrong_predictions += wrong_predictions\n",
    "    # statistics show, dividing by zero is a bad idea\n",
    "    if correct_predictions + wrong_predictions == 0:\n",
    "        continue\n",
    "    accuracies += [(correct_predictions / (correct_predictions + wrong_predictions))]\n",
    "\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T01:29:45.941641300Z",
     "start_time": "2023-12-10T01:29:45.904695200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 had a total word accuracy of: 0.24670193531105605\n",
      "In each post, on average, 0.16597790275819954 of the words were predicted correctly.\n"
     ]
    }
   ],
   "source": [
    "print('GPT2 had a total word accuracy of:', (total_correct_predictions / (total_correct_predictions + total_wrong_predictions)))\n",
    "print(f'In each post, on average, {statistics.mean(accuracies)} of the words were predicted correctly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:55:40.247191700Z",
     "start_time": "2023-12-09T23:55:40.241859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 37.7% total accuracy before (with 0.001)\n",
    "# 37.55 with 0.02\n",
    "# 37.0 with 0.04\n",
    "\n",
    "# 26.3% accuracy (corrected) vs 24.6% (uncorrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### The Problem of Catastrophic Forgetting\n",
    "So far, in the examples we have tested, we have trained our model on the entire dataset at ones (as opposed to training the model on the data sequentially). That is, in the real world, in the type of application we want to develop, our model would be exposed to snippets of data over a long period of time. However, this introduces the problem of \"catastrophic forgetting\" where the model, while optimizing the weights on the new data forgets about the old tasks it learned to do.\n",
    "\n",
    "To overcome this problem, we turn to DeepMind research paper titled [\"Overcoming catastrophic forgetting in neural networks\"](https://arxiv.org/abs/1612.00796). This paper arrives upon 3 key insights to help develop an algorithm for the aforementioned problem:\n",
    "1. Many optimal configurations of the set of weights and biases θ exist.\n",
    "2. It is likely that one those optimal configurations is close to the previous learned tasks.\n",
    "3. It is therefore possible to constraint parameters to stay in a region of low error for previous learned tasks during training.\n",
    "\n",
    "Inspired by biological mechanisms found in real life, the paper refers to this algorithm as elastic weight consolidation (EWC). EWC is essentially a new loss function. This is exactly what we are looking for because:\n",
    "1. Ofcourse now we can learn new tasks with out forgetting the others.\n",
    "2. EWC allows us to define the importance of each tasks relative to each other. In theory, this means we could classify more recent examples as more important! This gives us a lot of power.\n",
    "3. EWC can be trained for an arbitrary number of new tasks.\n",
    "\n",
    "https://github.com/moskomule/ewc.pytorch/blob/master/demo.ipynb\n",
    "https://github.com/kuc2477/pytorch-ewc\n",
    "https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
