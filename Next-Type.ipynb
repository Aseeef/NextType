{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next Type - A Mobile Typing Assistant\n",
    "[CS 505 - NLP] [Final Project]\n",
    "Completed by Muhammad Aseef Imran\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Compared to typing on a keyboard, typing on our phones considerably slower. Luckily most phones come built in with a feature to predict your next word to make up for this. However, after much experimentation, it seems most of these prediction algorithms seem to use simple N-grams with a window of around 3-4 words of left context. Many times, predicting the next word based on simple N-gram based probabilities work \"fine\" but often, this strategy produces poor results due to ignoring the context of the sentence. As NLP technology leaps forward exponentially we can do much better than simplistic N-grams. For this reason **Next Type** aims to bring the next generation of typing experiance to its users in order to raise productivity and typing speed for users leaving more time for the important things in life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"assets/bad-phone-example-1.jpg\" alt=\"N-gram example 1\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-2.jpg\" alt=\"N-gram example 2\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-3.jpg\" alt=\"N-gram example 3\" style=\"width: 33%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Presented above are three examples where unfortunately, the standard N-gram model employed on \"modern\" devices fails miserably.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Goals\n",
    "\n",
    "N-gram models perform very poorly with shorter context window, and any large context windows require an immense amount of data to produce reasonable results. Moreover, at least in this case, our N-gram models do not consider the right context when suggesting words. However, despite having their weaknesses, the N-gram approach also has some nice benefits. Particularly, the nature of the N-gram model allows it to be easily updated with new data and adapt to the typing habits of its users with little processing power.\n",
    "\n",
    "Keeping these things in mind, we can summarize our goals for the model we have set out to develop as:\n",
    "1. The model should consider both the left and right context before suggesting a \"natural\" word that fits the context.\n",
    "2. The model should be able to adapt to or learn from the user's word choices in various contexts.\n",
    "3. The model should be able to learn the user's word choice as—outlined above—quick enough to be useful.\n",
    "4. The model should be reasonably sized allowing it to be run and be updated on most modern-phone hardware with in reasonable time.\n",
    "\n",
    "Additional things that may be considered (depending on time) in our model may be that:\n",
    "\n",
    "5. The model should validate the input data for grammatical correctness to avoid learning incorrect patterns.\n",
    "6. The model should (optionally) avoid suggesting profane language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Questions & Challenges\n",
    "\n",
    "Let us better define the problem and explain exactly what we are seeking to achieve and possible challanges.\n",
    "\n",
    "1. People evolve and change. So can I get a better accuracy by consider ALL known history? Or only \"recent\" history? Should the more recent history be weighted more\"? If so, should history be judged more by time or the volume typed? If I'm wrong about this, then more data = better. OR the time people change in is just longer.\n",
    "2. My problem is my data is ever evolving. So how do I prevent the model from overfitting? Overfitting will make newer data harder to generalize. (i.e. How do I prevent it from forgetting the stuff it knew in the base model?). Also suppose if I don't want my model to consider the \"old\" history, how can I make the model \"forgot\" the old stuff (some kind of vanishing gradient is my best choice)?\n",
    "\n",
    "Shortcomings of this research: data may be biased. Only considering reddit users. Particularly those who type somewhat frequently on reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Obtaining the Fine-Tuning Data\n",
    "\n",
    "It is important for my task that the fine-tunning data only come from a single person since my goal is to train the model to adapt to the typing behavior of a specific user. In order to achieve this we can use scraped messages from reddit for specific users and examine how the model adapts to their specific word-choice and typing habits. Particularly, we will be using data from a pre-scrapped dataset [Reddit comments/submissions 2005-06](https://academictorrents.com/details/89d24ff9d5fbc1efcdaf9d7689d72b7548f699fc). Further, we also want to make sure that sample data from any user we use:\n",
    "* Provides a reasonably large enough dataset to train on\n",
    "* Posts regularly (as opposed to posting a lot occasionally)\n",
    "\n",
    "With that defined, our focus will be between and including messages posted between [1/2011 - 6/2012]. Why this particular time range? No particular reason besides that data with in this time range was reasonably enough sized to be processed quickly yet still leave us with enough data to work with.\n",
    "\n",
    "We can then process the reddit dump creating a dictionary consisting of reddit users and the messages they sent. We further filter this data as follows:\n",
    "* \"Deleted\" users aren't included\n",
    "* First, we remove all authors with less than 546 (i.e. they must average more than 1 post a day) - although future filteration steps would've already ensured this requirement, we start of with this since this is a quick and dirty elimination step allowing for quicker processing in the next steps (which are a bit more complicated implementation wise)\n",
    "* Second, we filter to only authors that made at least 18 post per month in the time range without missing any month.\n",
    "* Third, we filter to only authors that made at least 2 post every week without missing a week.\n",
    "* Fourth, we filter down the remaining users to those that posted on at least 80% of the days.\n",
    "* In our final step, we post process the post messages by running it through a sequence-to-sequence model already developed by someone correcting silly grammatical errors. Otherwise, we may end up having messages in our data set that contain non-existent vocab.\n",
    "TODO: grammer corrrection may be needed! https://huggingface.co/pszemraj/grammar-synthesis-small/tree/main also trained on t5\n",
    "or https://huggingface.co/flexudy/t5-small-wav2vec2-grammar-fixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Establishing a Baseline\n",
    "\n",
    "As previously mentioned, we want to develop a model the beats the naive n-gram models (with a window of 3) used by most keyboard apps. Therefore, we will define our baseline as 3-gram model trained on the entire reddit corpus. This model uses simple probabilities based on the frequency of the n-gram in the corpus to make predictions.\n",
    "\n",
    "Although modern keyboard apps adapt and update their n-gram model by using data from its user, as a 'baseline' model, we neglect this detail in the interest of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definition a \"Successful\" Prediction\n",
    "\n",
    "We define a successful prediction as a prediction that meets the following criteria:\n",
    "```The predicted next word is one of the top 3 predictions or a \"close enough\" synonym of one of the top 3 words.```\n",
    "\n",
    "#### Justification\n",
    "The goal of our \"predict the next word\" is to allow our users to type faster by suggesting their next word. Generally, most mobile devices have enough screen space to suggest at least 3 words for the \"next word\". Moreover, users may not care enough to type to exact word they were thinking of if the suggested word has a \"close enough\" meaning to the word they intended to write.\n",
    "\n",
    "#### Defining a \"close enough\" synonym\n",
    "We will base our definition of a \"close enough\" synonym on the assumption that \"most words have a synonym\". Then, we define a synonym to be close enough as follows:\n",
    "1. For each word in the word embedding space, find the word closest to that word using some distance metric.\n",
    "2. Calculate the mean and standard deviation of the distances between the closest words.\n",
    "3. Now we define a word to be \"close enough\" as being with in (the mean minus 1 standard deviation) unit distance from the predicted word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation Stategy\n",
    "\n",
    "In light of our above outlined goals, we have two major evaluation goals:\n",
    "\n",
    "\n",
    "1. How well does the model predict the user's next token after having seen x tokens of examples from the user. In other words, not only how well the model predicts the user's next token but also how fast the model improves its prediction as a function of the data it has already seen?\n",
    "\n",
    "> We can evaluate \"how well\" the model predicts the user's next token by measuring the loss between what the user actually types vs what the model suggests. Then, we can further measure this loss as function of the number of tokens of examples the model has seen during its Fine-Tuning. For example, how does the loss change after the model has seen 1000 tokens of examples from the user?\n",
    "\n",
    "2. How much computation is needed is to both run the model and update the model on new data?\n",
    "\n",
    "> Measuring how long various parts of the model such prediction and training takes is trivial. (We can simply calculate the time between the target area of code). We may then analyze the run-time in context of the hardware the code is run on and comparing this information with current state of computational power of modern mobile devices. This information can be used to make an informed decision on the sequence lengths to input to the model to ensure our model can suggest new words to users in real-time on standard mobile hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Plan and Exploring Potential Solutions\n",
    "\n",
    "Once again, our goal is to accuractely and effectively predict the user's next token in real-time while adapting to the user's behavior and writing style over time.\n",
    "\n",
    "In order to reasonably meet these goals, we will fine-tune one or more combination of existing models such as T5, Bert, and GPT2, and/or their \"Distilled\" counterparts. We will use the Hugging Face transformers library simply due to its vast popularity and easy of use. I intend to use the SCC for rapid prototyping and experimentation as I already have significant experiance using the SCC at this stage.\n",
    "\n",
    "Finally I should note that in my initial research, I have identified potential pitfalls with each of these models and their strengths and weaknesses for my task. However, further experimentation will be needed to make a final decision on which model (or model combinations) to use. Detailed experimentation with each model will be required to evaluate its pros/cons.\n",
    "\n",
    "#### Bert\n",
    "Having been trained on a mask-fill task, Bert naturally lends itself to the kind of project I am trying to do. Being a relatively small model, and still quite versatile for the task, Bert may be a great choice. However, one downside to Bert is that Bert seems to perform poorly when attempting to Mask-Fill multiple words in the middle of a sentence. (See the bottom of this notebook for a demonstration).\n",
    "\n",
    "#### GPT2\n",
    "GPT2 was essentially trained on predicting the next word. Indeed, this is the task we want to achieve ourselves. However, in some cases, we may need to predict the middle word (if someone is editing the middle of a sentence they wrote). This is not a task GPT2 was designed for although this may still be possible due to the surprising generality of the model. Further research and experimentation will be needed.\n",
    "\n",
    "#### T5\n",
    "T5 is an extremely general purpose model than can adapt to many NLP tasks. Unlike bert Being a substantially larger model than both GPT2 and Bert, T5 is slower to retrain. Yet at the same time, T5 seems to do a much better job mask-filling between sentences. Yet, in a realistic scenario how often does one write in the middle of the sentence? Is the increased computing cost really worth it? These are the questions I hope to answer with the first stages of my research.\n",
    "\n",
    "#### The \"Distilled Version\"\n",
    "Models like Distilled-Bert and Distilled-GPT2 are indeed smaller. However, one *major* pitfall to this may be that the model may struggle to generalize to new tasks and during retraining. Retraining forms an essential component to this project and depending on the severity of this effected, the \"Distilled\" models may prove ineffective. Further experimentation is needed to make any conclusions, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Limitations\n",
    "* Compared to the N-gram approach, this new model cannot easily learn new words?\n",
    "* You may talk different with friends vs family vs boss. This training and results was done specifically for reddit. It may be the case that the model will not generalize as well to a broader domain in an actual key board app. (Still probability at least better than the ngram stuff right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resources and Publications\n",
    "\n",
    "For this project, I am considering/planning to use the following resources for research:\n",
    "\n",
    "1. Tunstall, L., Werra, L., Wolf, T., &amp; Géron, A. (2022). Natural language processing with transformers: Building language applications with hugging face. O’Reilly.\n",
    "\n",
    "> This book has been repeatedly recommended by both Professor Snyder and other students in CS505. Upon a coarse inspection, I expect to particular find the sections on \"Fine Tuning\" various models helpful (since this is very much a Fine-Tuning project). The book also contains extensive details about various transformer architectures which will inevitably prove useful.\n",
    "\n",
    "2. \"Fine-Tune a Pretrained Model.\" Hugging Face, https://huggingface.co/docs/transformers/training. Accessed 1 Dec. 2023.\n",
    "\n",
    "> This blog post contains examples Fine-Tuning bert using three different methods along with model evaluation. Although unlike source (#1), it goes into less details on the theory, the sample code is more rich, and easier to work with.\n",
    "\n",
    "3. Raffel, Colin, et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Arxiv, 23 Oct. 2019, Accessed 1 Dec. 2023.\n",
    "\n",
    "> This is the original paper that introduced T5 to the world. As covered in lecture, similar to Bert, T5 was (partially) trained on Mask-Fill task where random spans of texts were removed which naturally make it a great candidate for what we want to achieve here. However, this Mask-Fill process was only part of the process. T5 is a very versitile (and large) model, and has huge applications. There is no better place to learn what it is, how it works, and how to use it than the original paper!\n",
    "\n",
    "4. https://212digital.medium.com/fine-tuning-the-gpt-2-large-language-model-unlocking-its-full-potential-66e3a082ab9c\n",
    "\n",
    "> This great blog post contains all the coded needed to get started on fine-tuning GPT2 and the warnings about the various pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparison and Exploratory Between Various Models\n",
    "\n",
    "We begin with an exploratory between different promising transformer models that have been historically very successful with a variety of tasks. Namely, we will do a comparisons between Bert, GPT2, and T5 on various tasks.\n",
    "\n",
    "1. We compare between the following 5 sentences to see how well the models do. These results are just to get a basic idea of each model's capabilities. We will \"eyeball this result\". Note that the \"|\" token represents the current \"cursor\" location.\n",
    "\n",
    "    a. `After forcefully breaking into the bank, they|`\n",
    "\n",
    "    b. `Every |, my family and I visit Hawaii.`\n",
    "\n",
    "    c. `In my family, there is my |`\n",
    "\n",
    "    d. `My favorite | is apple.`\n",
    "\n",
    "    e. `It has been 2 months since I graduated. However, unfortunately I still haven't found a |. At this rate I won't be able to pay rent!`\n",
    "\n",
    "2. After that, we will test each model to see how well the model does in predicting this \"next\" word token. This task is the most important task for our proposed application to do well.\n",
    "\n",
    "3. Next, we will see how well each model does at predicting a randomly removed \"middle\" token.\n",
    "\n",
    "4. Then, we do a similar challenge by now comparing how each model does at predicting when multiple \"middle\" tokens have been deleted.\n",
    "\n",
    "5. Finally, we will choose the most promising model to develop a fine-tuning method for this continuous learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:02.975608900Z",
     "start_time": "2023-12-11T18:06:59.069540200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs505ws/aseef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports here\n",
    "from typing import Union\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List, Set, Any, Union\n",
    "import statistics\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, PreTrainedTokenizerBase, PreTrainedModel, GPT2Config\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, DistilBertConfig, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:02.976608200Z",
     "start_time": "2023-12-11T18:07:02.972609700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"After forcefully breaking into the bank, they |\", # predict the next word using context\n",
    "    \"In my family, there is my |\", # predict the next word using context\n",
    "    \"Every |, my family and I visit Hawaii.\",  # simple mask fill with one missing word\n",
    "    \"My favorite | is apple.\", # simple mask fill with one missing word\n",
    "    \"It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\", # need multiple words here before sentence makes sense\n",
    "    \"Following the American Civil War, | assassinated.\",  # need multiple words here before sentence makes sense\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:02.976608200Z",
     "start_time": "2023-12-11T18:07:02.973608500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"/projectnb/cs505ws/projects/NextType/data\"\n",
    "\n",
    "def does_var_exists_gz(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'{data_dir}/{var_name}.pkl.gz')\n",
    "\n",
    "def dump_var_gz(var_name: str, obj) -> None:\n",
    "    os.makedirs(f\"{data_dir}\", exist_ok=True)\n",
    "    with gzip.open(F'{data_dir}/{var_name}.pkl.gz', 'wb', compresslevel=1) as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_var_gz(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists_gz(var_name):\n",
    "        return None\n",
    "\n",
    "    file_path = F'{data_dir}/{var_name}.pkl.gz'  # Updated file extension\n",
    "    with gzip.open(file_path, 'rb', compresslevel=1) as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:02.979611Z",
     "start_time": "2023-12-11T18:07:02.977610500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device={device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:29.891196600Z",
     "start_time": "2023-12-11T18:07:02.980612Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_to_posts_dict: Dict[str, Tuple[int, str]] = load_var_gz(\"author_to_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:29.891196600Z",
     "start_time": "2023-12-11T18:07:29.888194700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analogkid01',\n",
       " 'rednightmare',\n",
       " 'alle0441',\n",
       " 'ChingShih',\n",
       " 'zulubanshee',\n",
       " 'whoami9',\n",
       " 'lakerswiz',\n",
       " 'jared555',\n",
       " 'Flammy',\n",
       " 'Peritract',\n",
       " 'from_the_sidelines',\n",
       " 'Wyrmshadow',\n",
       " 'wartornhero',\n",
       " 'gndn',\n",
       " 'PalermoJohn',\n",
       " 'powercow',\n",
       " 'mariesoleil',\n",
       " 'browwiw',\n",
       " 'peewinkle',\n",
       " 'pillage',\n",
       " 'cylinderhead',\n",
       " 'chicofaraby',\n",
       " 'Wiebelhaus',\n",
       " 'Naly_D',\n",
       " 'Maxion']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_user_sample = random.sample(list(author_to_posts_dict.keys()), 25)\n",
    "random_user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:29.894196300Z",
     "start_time": "2023-12-11T18:07:29.891196600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample up to 50 posts from each user\n",
    "random_post_samples = []\n",
    "for rand_user in random_user_sample:\n",
    "    all_posts = author_to_posts_dict[rand_user]\n",
    "    sampled_posts = random.sample(list(all_posts), 50)\n",
    "    random_post_samples += sampled_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:31.061897800Z",
     "start_time": "2023-12-11T18:07:29.895194900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this was the only small-ish grammar correction model I found.\n",
    "# had I more time, I would create my own model. But I don't so I will focus\n",
    "# on my primary task.\n",
    "# the downsides of this models is that besides correcting spellings, it often alters the\n",
    "# structure of the sentence which could fundamentally undermine our purpose.\n",
    "# so question: does benefits of correcting grammar using this outweighs the harms?\n",
    "# after all, if the model doesnt recgonize a word, it'll just ignore it and wont learn from it!\n",
    "grammar_corrector = pipeline(\n",
    "               'text2text-generation',\n",
    "               'pszemraj/grammar-synthesis-small',\n",
    "                 device=device\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:31.065900900Z",
     "start_time": "2023-12-11T18:07:31.062900200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import html\n",
    "def normalize_text(post_text: str):\n",
    "    # get rid of new lines\n",
    "    post_text = re.sub(\"\\n\", \" \", post_text)\n",
    "    # remove html characters\n",
    "    post_text = html.unescape(post_text)\n",
    "    # Remove bold and italic formatting\n",
    "    post_text = re.sub(r'(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3', r'\\2\\4', post_text)\n",
    "    # Remove headers\n",
    "    post_text = re.sub(r'^#{1,6}\\s', '', post_text)\n",
    "    # Remove hyperlinks\n",
    "    post_text = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'\\1', post_text)\n",
    "    # Remove inline code\n",
    "    post_text = re.sub(r'`([^`]+)`', r'\\1', post_text)\n",
    "    # Remove block code\n",
    "    post_text = re.sub(r'```(?:[^`]+|`(?!``))*```', '', post_text)\n",
    "    # Remove lists (unordered and ordered)\n",
    "    post_text = re.sub(r'^\\s*([\\*\\-\\+]\\s|(\\d+\\.)\\s)', '', post_text)\n",
    "    post_text = re.sub(\"(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3\", \"\", post_text)\n",
    "    # remove double spaces\n",
    "    post_text = re.sub(\" {2,}\", \" \", post_text)\n",
    "    # replace urls\n",
    "    post_text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"{URL}\", post_text)\n",
    "    # replace references to a specific subreddit but just the token \"[subreddit]\"\n",
    "    post_text = re.sub(r\"(\\W)(r/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB REDDIT}\\3\", post_text)\n",
    "    post_text = re.sub(r\"(\\W)(/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB REDDIT}\\3\", post_text)\n",
    "    post_text = post_text.strip()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/3794 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(user_posts_corrected))):\n\u001b[1;32m      3\u001b[0m     creation, message \u001b[38;5;241m=\u001b[39m user_posts_corrected[i]\n\u001b[0;32m----> 4\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m(message)\n\u001b[1;32m      5\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(message)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_text' is not defined"
     ]
    }
   ],
   "source": [
    "user_posts_corrected = list(user_posts)\n",
    "for i in tqdm(range(len(user_posts_corrected))):\n",
    "    creation, message = user_posts_corrected[i]\n",
    "    message = normalize_text(message)\n",
    "    sentences = sent_tokenize(message)\n",
    "    for j in range(len(sentences)):\n",
    "        sent = sentences[j]\n",
    "        updated_message = grammar_corrector(sent)[0]['generated_text']\n",
    "        sentences[j] = updated_message\n",
    "    updated_message = ' '.join(sentences)\n",
    "    user_posts_corrected[i] = (creation, updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_var_gz(f\"{random_key}-corrected-posts\", user_posts_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_posts_corrected = load_var_gz('The_Jackal-corrected-posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "index = 0\n",
    "for old_post, new_post in zip(user_posts, user_posts_corrected):\n",
    "    normalized_post = normalize_text(old_post[1])\n",
    "    if old_post[1].strip() != new_post[1].strip():\n",
    "        print(\"-=+=--=+=--=+=--=+=--=+=--=+=-\")\n",
    "        print(f'Old Post {index}:', normalized_post)\n",
    "        print(\"~~+~~~~+~~~~+~~~~+~~~~+~~~~+~~\")\n",
    "        print(f'New Post {index}:', new_post[1])\n",
    "        print(\"-=+=--=+=--=+=--=+=--=+=--=+=-\")\n",
    "        counter += 1\n",
    "    index += 1\n",
    "    if counter > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:04:33.755115600Z",
     "start_time": "2023-12-11T18:04:32.183788100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load in the t5 model\n",
    "T5_path = 't5-small'\n",
    "t5_config = T5Config.from_pretrained(T5_path)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_path, legacy=False)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_path, config=t5_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T15:51:11.626697Z",
     "start_time": "2023-12-11T15:51:10.886491700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('broke', 0.22146426141262054), ('break', 0.21711385250091553), ('are', 0.21013765037059784), ('were', 0.17779791355133057), ('will', 0.1734863519668579)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('family', 0.27247941493988037), ('own', 0.25976160168647766), ('daughter', 0.15796756744384766), ('mother', 0.1555204540491104), ('home', 0.1542709767818451)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('year', 0.3100126087665558), ('day', 0.2699778378009796), ('month', 0.1532614529132843), ('week', 0.14059576392173767), ('time', 0.12615235149860382)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('apple', 0.469388872385025), ('fruit', 0.16375479102134705), ('thing', 0.15048746764659882), ('pie', 0.11327831447124481), ('recipe', 0.10309050232172012)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('rent', 0.24719730019569397), ('rate', 0.22261090576648712), ('fixed', 0.21800415217876434), ('rental', 0.16750206053256989), ('flat', 0.1446855515241623)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('the', 0.314527302980423), ('', 0.2124253213405609), ('American', 0.2062595784664154), ('many', 0.13480444252490997), ('an', 0.13198336958885193)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"<extra_id_0>\")\n",
    "    input_ids = t5_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    num_samples = 5\n",
    "    with torch.no_grad():\n",
    "        output = t5_model.generate(\n",
    "            input_ids,\n",
    "            num_beams=num_samples,\n",
    "            min_new_tokens=2,\n",
    "            max_new_tokens=2,\n",
    "            num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(output.sequences, probabilities):\n",
    "        decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        suggestions += [(decoded_output, prob.item())]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:15.373653100Z",
     "start_time": "2023-12-11T15:56:10.303377300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [19:05<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "t5_correct_guesses = 0\n",
    "t5_total_guesses = 0\n",
    "t5_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" <extra_id_0>\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids = t5_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # t5 can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            num_samples = 7\n",
    "            output = t5_model.generate(\n",
    "                input_ids,\n",
    "                num_beams=num_samples,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=2,\n",
    "                num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for sample_output, prob in zip(output.sequences, probabilities):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            if prob.item() < 0.05:\n",
    "                # avoid bizarre suggestions by simply filtering out low prob\n",
    "                # terms. We don't HAVE TO suggest exactly 5 words\n",
    "                break\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            t5_correct_guesses += 1\n",
    "        t5_total_guesses += 1\n",
    "        t5_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:15.373653100Z",
     "start_time": "2023-12-11T16:15:15.314458200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3592931017170399\n"
     ]
    }
   ],
   "source": [
    "t5_accuracy = t5_correct_guesses / t5_total_guesses\n",
    "print(t5_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:15.419996100Z",
     "start_time": "2023-12-11T16:15:15.317210Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.022976395781948235\n",
      "Inference Std: 0.00535418975672332\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(t5_inference_times))\n",
    "print('Inference Std:', statistics.stdev(t5_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Middle Token\"\n",
    "OK, predicting the next token is fun and all. But what if a user wants to add something to the middle of their sentence? Wouldn't it be nice to be able to both use left and right context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:15.422994300Z",
     "start_time": "2023-12-11T16:15:15.419996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "#### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:04:34.651566200Z",
     "start_time": "2023-12-11T18:04:33.756145900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distbert_path = 'distilbert-base-cased'\n",
    "distbert_model = DistilBertForMaskedLM.from_pretrained(distbert_path).to(device=device)\n",
    "distbert_config = DistilBertConfig.from_pretrained(distbert_path)\n",
    "distbert_tokenizer = DistilBertTokenizer.from_pretrained(distbert_path, config=distbert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:16.485638800Z",
     "start_time": "2023-12-11T16:15:16.418844600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"[MASK]\")\n",
    "    input_ids = distbert_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    with torch.no_grad():\n",
    "        output = distbert_model(input_ids)\n",
    "        predictions = output.logits\n",
    "\n",
    "    # Get the top-k predicted tokens and their probabilities\n",
    "    top_k = 5  # Adjust as needed\n",
    "    probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "        suggestions += [(sample_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:14.923844Z",
     "start_time": "2023-12-11T16:55:59.910583900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:14<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "distbert_correct_guesses = 0\n",
    "distbert_total_guesses = 0\n",
    "distbert_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" [MASK]\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids: torch.Tensor = distbert_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # bert can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "        # Get the position of the masked token\n",
    "        mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            output = distbert_model(input_ids)\n",
    "            predictions = output.logits\n",
    "\n",
    "        # Get the top-k predicted tokens and their probabilities\n",
    "        top_k = 8  # Adjust as needed\n",
    "        probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "        # Convert indices back to tokens\n",
    "        predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for decoded_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            if prob < 0.04:\n",
    "                # avoid bizarre suggestions by simply filtering out low prob\n",
    "                # terms. We don't HAVE TO suggest exactly 5 words\n",
    "                break\n",
    "            # bert also suggests \"sub-words\". Ehh... we'll just ignore those.\n",
    "            # otherwise stuff will get too complicated\n",
    "            if decoded_output.startswith(\"##\"):\n",
    "                continue\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            distbert_correct_guesses += 1\n",
    "        # since the way the bert tokenizer works, it can suggest \"sub-words\" - example: characteristically = characteristic + ##ally,\n",
    "        # so we will give bert half a point for suggest the same start of the word\n",
    "        for s in suggestions:\n",
    "            if actual_next_word.startswith(s):\n",
    "                distbert_correct_guesses += 0.5\n",
    "                break\n",
    "        distbert_total_guesses += 1\n",
    "        distbert_inference_times += [time.time() - start_time]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:20.001862900Z",
     "start_time": "2023-12-11T17:00:19.985839900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18298021889747967\n"
     ]
    }
   ],
   "source": [
    "bert_accuracy = distbert_correct_guesses / distbert_total_guesses\n",
    "print(bert_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:22.286787800Z",
     "start_time": "2023-12-11T17:00:22.234767400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.005107757104114663\n",
      "Inference Std: 0.001992808943326104\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distbert_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distbert_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Middle Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Distil-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:32.322299100Z",
     "start_time": "2023-12-11T18:07:31.063901300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the GPT-2 model variant\n",
    "distgpt2_model_name = \"distilgpt2\"\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "distgpt2_model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(distgpt2_model_name)\n",
    "distgpt2_tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(distgpt2_model_name)\n",
    "# Move the model to the GPU (if available)\n",
    "distgpt2_model = distgpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:07:32.838487Z",
     "start_time": "2023-12-11T18:07:32.317287800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [(' were', 0.6859785914421082), (' found', 0.10938050597906113), (' had', 0.08820205926895142), (' took', 0.059164125472307205), (' began', 0.05727475509047508)]\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [(' family', 0.27839019894599915), (' brother', 0.2245674729347229), (' mother', 0.18426108360290527), (' sister', 0.1591966301202774), (' wife', 0.1535845696926117)]\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [(' The', 0.40960025787353516), ('.', 0.16524524986743927), (' A', 0.15191836655139923), ('\\n', 0.13858996331691742), ('The', 0.13464611768722534)]\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [(' favorite', 0.44314318895339966), (' part', 0.18813173472881317), (' thing', 0.16900213062763214), (' of', 0.11418430507183075), ('.', 0.08553868532180786)]\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [(' way', 0.38864630460739136), (' job', 0.33211749792099), (' place', 0.1317850798368454), (' new', 0.09153558313846588), (' good', 0.055915530771017075)]\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [(' the', 0.7203230857849121), (' a', 0.11123217642307281), (' it', 0.07635104656219482), (' and', 0.05197402089834213), (' in', 0.04011968895792961)]\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    # delete everything including and after |\n",
    "    # GPT2 is physically unable to consider right context\n",
    "    input_text = sentence[:sentence.index(\"|\")].strip()\n",
    "    # Tokenize the prompt\n",
    "    input_ids = distgpt2_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    # Generate probabilities for the next words\n",
    "    with torch.no_grad():\n",
    "        outputs = distgpt2_model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "    # after all, we don't want to suggest too many words!\n",
    "    top_k = 5\n",
    "    # Get the probability distribution for the next word\n",
    "    next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "    top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "    # Normalize probabilities\n",
    "    top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "    # Convert the probabilities to a list\n",
    "    top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "    top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "        decoded_output = distgpt2_tokenizer.decode(sample_output)\n",
    "        suggestions += [(decoded_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:04:40.665834100Z",
     "start_time": "2023-12-11T18:04:40.252521300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/1250 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [80,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "  0%|                                                                                                                                 | 0/1250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Generate probabilities for the next words\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdistgpt2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# we are only interested in either the top 5 words or words with a probability of > 1%\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# after all, we don't want to suggest too many words!\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "#distgpt2_tokenizer.pad_token = distgpt2_tokenizer.eos_token\n",
    "distgpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "\n",
    "    current_batch_prompt = []\n",
    "    current_batch_answer = []\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]).strip()\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        current_batch_prompt += [current_prompt]\n",
    "        current_batch_answer += [actual_next_word]\n",
    "\n",
    "        assert len(current_batch_prompt) == len(current_batch_answer)\n",
    "        if len(current_batch_prompt) < 8:\n",
    "            continue\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.batch_encode_plus(current_batch_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024, truncation_strategy=\"longest_first\").input_ids.to(device)\n",
    "\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = distgpt2_model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()\n",
    "        top_k_indices_list: List = top_k_indices.tolist()\n",
    "\n",
    "        # for each element in the batch\n",
    "        for current_prompt, top_k_probs, top_k_indices, actual_next_word in zip(current_batch_prompt, top_k_probs_list, top_k_indices_list, current_batch_answer):\n",
    "\n",
    "            guesses = set()\n",
    "            for token_id, prob in zip(top_k_indices, top_k_probs):\n",
    "                # ignore bizarre suggestions\n",
    "                if prob < top_p:\n",
    "                    continue\n",
    "                token = distgpt2_tokenizer.decode([token_id])\n",
    "                guesses.add(token)\n",
    "\n",
    "            # we consider a prediction correct if the actual word was\n",
    "            # one of the (up to) 5 words suggested\n",
    "            if (' ' + actual_next_word) in guesses:\n",
    "                distilgpt2_correct_predictions += 1\n",
    "            distilgpt2_total_predictions += 1\n",
    "\n",
    "            print('prompt:', current_prompt)\n",
    "            print('suggestions:', guesses)\n",
    "            print('answer:', actual_next_word)\n",
    "\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n",
    "        # clear the batch\n",
    "        current_batch_prompt = []\n",
    "        current_batch_answer = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:52:51.245044Z",
     "start_time": "2023-12-11T18:48:48.956702600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:02<00:00,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "        if input_ids.shape[1] == 0:\n",
    "            continue\n",
    "        # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 1024:\n",
    "            input_ids = input_ids[:, -1024:]\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = distgpt2_model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "        top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "        # filter to only words with a probability greater than p%\n",
    "        removal_marked = []\n",
    "        for j in range(len(top_k_probs_list)):\n",
    "            if top_k_probs_list[j] < top_p:\n",
    "                removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "        for to_remove in removal_marked:\n",
    "            top_k_probs_list.remove(to_remove[0])\n",
    "            top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "        guesses = set()\n",
    "        for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "            token = distgpt2_tokenizer.decode([token_id])\n",
    "            guesses.add(token)\n",
    "\n",
    "        # we consider a prediction correct if the actual word was\n",
    "        # one of the (up to) 5 words suggested\n",
    "        if ' ' + actual_next_word in guesses:\n",
    "            distilgpt2_correct_predictions += 1\n",
    "        distilgpt2_total_predictions += 1\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:52:51.245044Z",
     "start_time": "2023-12-11T18:52:51.244074200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3662472919523781\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print(gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:09:04.457717700Z",
     "start_time": "2023-12-11T18:09:04.433184500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.00606661000856735\n",
      "Inference Std: 0.0015065450347984649\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distilgpt2_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distilgpt2_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary of the results from Model Comparison\n",
    "\n",
    "* T5 was the slowest model\n",
    "GPU\n",
    "* 24:39 - T5-Small\n",
    "* 5:10 - DistilBert\n",
    "* 5:32 - DistilGPT2\n",
    "CPU\n",
    "* 3:19:43: T5-Small\n",
    "* 1:41:39 - DistilBert\n",
    "* 2:29:33 - DistilGPT2\n",
    "Accuracy:\n",
    "* 37.27% - T5-Small\n",
    "* 17.73% - DistilBert\n",
    "* 35.86% - DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:21:27.203420Z",
     "start_time": "2023-12-11T14:21:26.794601800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46875\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "for creation, post in random_post_samples:\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    total_words += len(tokenized_words)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### The Problem of Catastrophic Forgetting\n",
    "So far, in the examples we have tested, we have trained our model on the entire dataset at ones (as opposed to training the model on the data sequentially). That is, in the real world, in the type of application we want to develop, our model would be exposed to snippets of data over a long period of time. However, this introduces the problem of \"catastrophic forgetting\" where the model, while optimizing the weights on the new data forgets about the old tasks it learned to do.\n",
    "\n",
    "To overcome this problem, we turn to DeepMind research paper titled [\"Overcoming catastrophic forgetting in neural networks\"](https://arxiv.org/abs/1612.00796). This paper arrives upon 3 key insights to help develop an algorithm for the aforementioned problem:\n",
    "1. Many optimal configurations of the set of weights and biases θ exist.\n",
    "2. It is likely that one those optimal configurations is close to the previous learned tasks.\n",
    "3. It is therefore possible to constraint parameters to stay in a region of low error for previous learned tasks during training.\n",
    "\n",
    "Inspired by biological mechanisms found in real life, the paper refers to this algorithm as elastic weight consolidation (EWC). EWC is essentially a new loss function. This is exactly what we are looking for because:\n",
    "1. Ofcourse now we can learn new tasks with out forgetting the others.\n",
    "2. EWC allows us to define the importance of each tasks relative to each other. In theory, this means we could classify more recent examples as more important! This gives us a lot of power.\n",
    "3. EWC can be trained for an arbitrary number of new tasks.\n",
    "\n",
    "https://github.com/moskomule/ewc.pytorch/blob/master/demo.ipynb\n",
    "https://github.com/kuc2477/pytorch-ewc\n",
    "https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:11:52.575683400Z",
     "start_time": "2023-12-11T18:11:52.567698100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VulturE'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select a user to train with\n",
    "random_user = random.choice(list(author_to_posts_dict.keys()))\n",
    "random_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:11:52.836510600Z",
     "start_time": "2023-12-11T18:11:52.835509300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2676"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = author_to_posts_dict[random_user]\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:20:29.760414Z",
     "start_time": "2023-12-11T18:20:29.757415100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (e.g., 90% train, 10% validation)\n",
    "split_index = int(0.9 * len(training_data))\n",
    "train_data = [x[1] for x in training_data[:split_index]]\n",
    "valid_data = [x[1] for x in training_data[split_index:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:20:30.175719700Z",
     "start_time": "2023-12-11T18:20:30.132961Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm sad that samsung no longer makes its own drives :(\",\n",
       " 'I remember the GLORIOUS DAY when Subway [actually had a letter go out to store managers asking that all employees start tessellating the cheese](http://gawker.com/5551263/subway-finally-agrees-to-tessellate-cheese). I have to remind them to do it still :(',\n",
       " 'Does it need any?',\n",
       " 'disco ball',\n",
       " \"I'd second ES File Explorer. Being able to copy files from my Windows Shares easily is what made this app golden. Others I like:\\n\\n* [Parcels](https://play.google.com/store/apps/details?id=eu.zomtec.android.delivery) for tracking my many packages I send or receive.\\n* [Aix Weather](https://play.google.com/store/apps/details?id=net.veierland.aix) all that you need for a weather widget\\n* [AlarmDroid](https://play.google.com/store/apps/details?id=com.splunchy.android.alarmclock)\\n* [Contact Simple Widget](https://play.google.com/store/apps/details?id=lcc.simplewidgets) simply because it uses nickname values from contacts properly\\n* [GasBuddy](https://play.google.com/store/apps/details?id=gbis.gbandroid) use it weekly....great on vacations\\n* [Google Voice](https://play.google.com/store/apps/details?id=com.google.android.apps.googlevoice)\\n* [Narwhal Notifier](https://play.google.com/store/apps/details?id=com.quicklookbusy.narwhalNotifier) for redditors\\n\\nPaid, but still so useful :(\\n\\n* [Pure calendar widget](https://play.google.com/store/apps/details?id=org.koxx.pure_calendar)\\n* [Scores Widget](https://play.google.com/store/apps/details?id=com.scottkominek.scoreswidget)\\n\\n\",\n",
       " \"After actually using that gas app nonstop, I can't live without it.\",\n",
       " 'I would totally use this if:\\n\\n* It had text size adjustment. Mine is set very small in pure calendar. Smooth has huge text.\\n* it had scrolling support. I currently can view about 30-40 events in my pure calendar.',\n",
       " 'disco ball',\n",
       " 'Indeed, but the delivery is almost the exact same as the Afroman song.',\n",
       " \"Eventually he will, and you will need to have a justification for what you have done. Do not mention the bonuses.\\n\\nAt my place, we are supposed to, by department, generate graphs showing our work order flow (amount still open and who assigned to, completed per week for last 3 weeks, completed this week per person, etc) and I took the time to write a script that grabs all of that data from SharePoint, creates an excel sheet with equations and graphs on it, and then publishes only the graphs to a SharePoint page. With the multiple data sources, this used to take me 15-25 minutes, and now it takes 20 seconds. I have it set as a scheduled task the day they are due. Because I have always had my graphs in on time and they are 100% accurate (it sucked having to make them every Monday morning as the first thing I needed to do), I've received numerous raises.\\n\\nDon't feel morally obligated to turn down/off your ace-in-the-hole. You are dedicated to getting the job done fast, and are helping your team succeed, and should be rewarded.\",\n",
       " 'Also legal issues.',\n",
       " \"I'd rather it not. I'm waiting for the opportunity to help someone quit the internet. The hard way.\",\n",
       " 'We are **NOT** on 4chan, nigger.',\n",
       " 'Next episode of CSI: Redditors kill someone with quinine!',\n",
       " 'What if it was.....over 9000?',\n",
       " \"Isn't there also a general belief that knives of any sort are what not-so-good people have?\",\n",
       " 'IIRC, in PA (which has ass-backwards alcohol laws and nearly everything is state-run) you can buy wine/liquor from a few state-run stores on sunday from 9am to 9pm (but most have kept it closed, or 12-5).',\n",
       " '**THEY ARE SHIT**',\n",
       " 'Just saw *Here Come The Mummies* in Pittsburgh at the Altar Bar and they used TicketFly.',\n",
       " \"The other person could have been worse off than you, and now they're out of money and you're out of morals. Sounds like gaining the money would do nothing to improve that situation at all.\",\n",
       " 'What is the poster in the top right of the 3rd pic? FMP?',\n",
       " 'I remember when it was only 300 or so...wow.',\n",
       " \"Goto shows that use something like TicketFly then. I've come across quite a few in the Pittsburgh area.\",\n",
       " 'graphical representation is in the pro version, but you also get shittons more features than that in the pro version.',\n",
       " 'So basically, Win+D.',\n",
       " 'Okay :(',\n",
       " 'Almost everything I want to see is at TicketFly or at the Benedum :) I almost always buy a center directors circle row for a big show at the benedum in advance and sell half the seats to pay for the ones I use....its like going to the show for free.',\n",
       " \"I talked about it further down, but if you're in the Pittsburgh area, use TicketFly to see some pretty awesome shows instead of TM. And fuck, the shows in all of Pitt's theatres!\",\n",
       " 'I really like the tiny foil spot on my TicketFly ticket. Makes me feel special :)',\n",
       " 'Or make it a non-required question.',\n",
       " \"99% of the time, you don't get anything if you're an intern in the US, paid or not.\",\n",
       " 'Do you know if yours is legit? I\\'ve come across enough fake cartridges lately to make me concerned...typically, the label isn\\'t shiny if its fake, or it will say \"The save file will be loaded. The game can be played.\" at the load game screen.',\n",
       " \"&gt; get a decent pair with an ok warranty.\\n\\nSometimes, depending on where you buy them, this may mean buying that extended warranty that normally makes no sense to buy. With easily breakable items that you're gonna use often, it makes a little more sense.\\n\\nI bought a $100 pair of headphones and got a 3yr extended warranty on them for $20, and sure as hell they stop working well within the 3yr timeframe. Now I get a new pair, and that pair also gets a new 3-year warranty for free since my last pair broke.\\n\\nThis doesn't work with everything, and I wouldn't recommend it for cheaper headphones, but if you're rough on them it can help.\",\n",
       " \"If it's Vista or lower you're on, I'd recommend trying [ToolTipFixer](http://neosmart.net/wiki/display/TTF/ToolTipFixer+Home). Not entirely sure if it's needed on W7.\\n\\nIt also takes care of stuck tooltips, which is what I'm thinking you might have.\",\n",
       " \"Exactly. If you're only buying 20$ headphones and replacing them every year, it makes sense to save up and buy nicer headphones and spend the same money by getting the extended warranty :)\",\n",
       " \"You forgot Rudy's, puffy tacos, and Schlitterbahn.\\n\\nAnd I'm from Pennsylvania. How dare you.\",\n",
       " \"Henry's Puffy Tacos is an enlightening experience. Go prepared to get gassy, and maybe buy a t-shirt.\\n\\nRudy's is so fking good. Look over the menu beforehand, get alot of brisket, some soda, and DEFINITELY GET SOME CREAM CORN. Shit, their cream corn is so popular, they'll sell it by the 5gal bucket if you want that much. I also recommend going to the original one, if you live within feasible driving distance.\",\n",
       " \"What all do you want for it? I'm just lookin for a copy for my younger brother so we can battle against each other.\",\n",
       " 'Na we got a cable to plug my advance into his SP.',\n",
       " 'I could do cash or a steam game whichever you think woild be better.',\n",
       " \"You definitely haven't been here long enough to know how insane they've gone trying to make search even work.\\n\\nThis site is gigantic, and the search functionality is far better than it was only a year ago.\",\n",
       " '&gt; Implying that reddit only upvotes circle jerkers in a thread like this',\n",
       " '&gt; Basically, did everyone in the room see a crippled kid hobble into the room and set up a special computer and tell him \"Fuck You\" at ever turn? Would you, your friends, your teachers, hell the people you hate saw him struggling to keep up with a disability wouldn\\'t you offer to help?\\n\\nWhen I tried getting help when I was temporarily in a wheelchair, I was frequently confronted with the general \"NOT MY JOB\" bullshit and people saying \"yea we can do that\" and then never following through on it. And I\\'ve definitely had experience with schools that are run like businesses instead of educational facilities.',\n",
       " 'Not sure about white vinegar, but I use apple cider vinegar with some bentonite clay to make an awesome mask for my face.',\n",
       " 'Well I do use the mask to try and clean up my face, but I will admit that I stop using it for halloween to get my normal werewolf look rolling.',\n",
       " 'THIS MOVIE IS MAXIMUM!',\n",
       " '**NOBODY-FUCKING-CARES-ABOUT-YOUR-RES-TAG**.com',\n",
       " \"The problem is that everyone has RES, and these posts add nothing to the conversation. If you want others to know what you tag things, go use twitter.\\n\\nI'm utterly tired of seeing it this week.\",\n",
       " \"I personally passed the Server 2008 test with cengage's LabSim software (simulates a real Server 2008 environment and gives you tasks and tutorials to accomplish the EXACT WAY that the test wants you to).\\n\\nLabsim for MCTS Guide to Microsoft Windows Server 2008 Active Directory (Exam #70-640) (ISBN-10: 1423902718 | ISBN-13: 9781423902713)\\n\\nI don't have the CD anymore, nor have I found it on the internet for any reasonable price (or free), but accompanied with either the book it goes with, or free materials, you'd pass easily.\",\n",
       " '&gt; imagines voldemort burning doritos',\n",
       " \"&gt;  it is usually one of the first things a n00b can say because they watch a lot of anime\\n\\nPretty sure I missed the connection there....how does watching alot of anime lead to saying kawaii neko?\\n\\nPretty sure I've never said shit like that.\",\n",
       " 'Never heard it uttered in r/anime, and never heard it on tv-links when I ran that.\\n\\nIs this something new?',\n",
       " 'Ah ok. You mean faggots.',\n",
       " 'I just got that last one yesterday. Decided to go home early.',\n",
       " \"I'll go download one.\",\n",
       " \"cvpcs, huh....\\n\\n&gt; remembers CM7/CM9 Droid X builds\\n\\nNah, I'd rather not rely on that again. Just gonna use MX Player and wait for official builds.\",\n",
       " 'So which does it resemble... the dark red Russian or pink-orangeish Thousand Island? because those dressings are very different, despite being argued upon which properly belongs on a reuben.',\n",
       " \"he's the top submitter in the *porn subreddits, that's why.\",\n",
       " 'It feels like that effect that people use when they want pictures to look like minatures....a few pics of it were on reddit in the past.',\n",
       " 'I happen to have the same mug, but with a handle.\\n\\nI wish I had yours.',\n",
       " 'Sometimes too hot for me :)',\n",
       " 'Last one nearly made me cry.',\n",
       " 'I WILL NOT MAKE A CONTRACT WITH YOU KYUBEY GO AWAY',\n",
       " \"All those poor logs have no idea they're about to die.\",\n",
       " \"&gt; Yet another reason why I have no desire to get my license or drive ever.\\n\\nYou're a real daredevil.\",\n",
       " 'awesome. I had about 13 of them :D',\n",
       " 'now THAT is mildly interesting.',\n",
       " 'Not too far off :(',\n",
       " \"I'm wondering...what does a devil's avocado taste like?\",\n",
       " 'Satire is exempt from all that crap. Where have you been for all these years?',\n",
       " \"It's actually Nintendo's policy that they deny ANY requests at using their copywritten/trademarked stuff.\\n\\nIf they want you making something, they ask YOU to do it, or you're making it illegally.\\n\\nsource: tried to get approval to create a DK tie :(\",\n",
       " \"Atleast it isn't Warren....duck and run from those gunshots!\",\n",
       " 'Like a thing when a bad happens!',\n",
       " 'fun fact: the 3.5 install still fails if you actually disconnect the cable during install after that message appears.',\n",
       " \"Sometimes there are lots of nuts. Sometimes there are lots of upvotes.\\n\\nI'm feeling mildly sorry.\",\n",
       " 'I recently went to a Dell conference in my area, showing off their new products. Here were the questions:\\n\\n* 10% about the new midrange servers (given 1hr during talk)\\n* 5% about the new backup solutions (given 45mins during talk)\\n* 85% about the KACE hardware (left until end and only given 15 minutes)\\n\\nThen they cancelled the local demo everyone wanted and replaced it with a webinar that was truly useless unless you were already using a KACE product.\\n\\nBut none of that is your fault.\\n\\nI actually have never had a problem with Dell support for the 6-7 years I\\'ve been calling them...other than a snag with no fans in my 50 Optiplex 745\\'s over the harddrive. When asked about it, they assured me that \"fans were only put in towers with Core 2 Duo processors, and not older Intel processors....they run cooler\". Which is bullshit. Eventually got our gov\\'t account rep to get someone to send us a box of replacement fans (for the ones we were accused of stealing...lol) and it dropped temps 7-17 degrees across every computer. So yea, just send someone a fan if a 745 is running hot, even if it\\'s out of warranty, to be a GGG.',\n",
       " 'Or you could do the online chat support in IE, and get an emailed log of everything to attach to a work order afterwards.',\n",
       " 'We were offering the entire time to replace the fan, and were instead accused of stealing them',\n",
       " \"Indeed they aren't. We weren't very happy, especially being a large gov't contract in Pittsburgh.\",\n",
       " \"If you ever need just a better AC powered one, I highly recommend the [Apricorn Drivewire](http://www.amazon.com/Apricorn-Drivewire-Universal-Adapter-2-5IN/dp/B000QY9KIS). It has one of the more reliable power bricks I've ever used on these types of tools....have used mine nonstop for 2 years.\\n\\nActually, the rest of their products are pretty nice too.\",\n",
       " 'you might like it [here](http://www.reddit.com/r/mildlyinteresting)',\n",
       " 'We have automated ticket creation from emails sent to the helpdesk email address. It puts their name as requestor, copies any pictures as attachments, and copies the body as text.',\n",
       " 'I have a penis brain.',\n",
       " 'Are you telling me that Nina was in this show?',\n",
       " 'Exactly. These people are idiots, and we are the only two sane humans left.',\n",
       " 'I personally love [galnet miui ics](http://rootzwiki.com/topic/12588-new-galnet-miui-ics-225-1-batt-charge-while-off-updated-bootmenu-scripts/), but I doubt it will ever get updated again :(\\n\\nnever seen my droid x get such a long battery life before.',\n",
       " 'I saw the end coming, but it was still done quite well :)',\n",
       " 'WE NEED THE PORN',\n",
       " 'Just me, or does that look illegal?',\n",
       " 'I also want whatever that 2nd image he posted was.',\n",
       " 'I can eat 50 eggs.',\n",
       " 'Are you merely joking like others in this thread, or are you for real?\\n\\nI\\'ve only got a 110\", and we\\'ve slowly pieced together every component over the years, but it\\'s a pretty good setup.',\n",
       " \"If they're unwilling to use it, they obviously don't respect you, let alone themselves. There are a few other potential reasons (haven't found the right size, so they always complain too tight), but EVERY reason that I can think of is a terrible excuse of why they can't respect their partner.\\n\\nIf there was a pill out there that was as reliable as most female solutions and reversible, I'd get it in a heartbeat.\\n\\nThis post title is infuriating to someone like me.\",\n",
       " 'Always here.\\n\\n*Hide yo kids.....hide yo wife.*',\n",
       " \"&gt; dropped a black bag over his head\\n\\nSo he disappeared into one of Creedy's black bags?\",\n",
       " 'When we finish the tables and beer tap, ill throw some pics up in the home theater subreddit and send you a link',\n",
       " 'I believe V said that artists use lies to tell the truth.',\n",
       " 'Nobody ever eat fifty eggs.',\n",
       " \"It could be done quite easily. Chop off some excess red from top, remove the grey crap from the edges and replace it with black, use the [higher-res copy](http://www.flickr.com/photos/14025511@N03/6872341821/sizes/o/in/photostream/), and add black on the sides.\\n\\nI'm not near a copy of Photoshop or I'd attempt it, even with how terrible I am at Photoshop.\",\n",
       " \"IT does? Well then, I'd make him look like a termite with maracas if he saw my data center!\",\n",
       " 'I heard he likes flipping quarters in the alley, though.',\n",
       " 'I often use [this](http://en.wikipedia.org/wiki/File:S-IC_engines_and_Von_Braun.jpg) as a wallpaper.',\n",
       " 'Huh...never knew it had anything to do with being a redditor.',\n",
       " 'Then utilize the exact formula with advanced calculations we already use to estimate things to account for all of that stuff you mentioned.',\n",
       " 'In 30 minutes, I cleaned up enough of the black artifacts to make it an [acceptable wallpaper image](http://i.imgur.com/vqHnD.jpg). I kept it at that resolution in case someone wants to clean up the red artifacts at full quality, but it will scale down properly to 1920x1080.',\n",
       " \"He means there's still grey in the bottom left corner of where the poster ended before, but I understand what you're saying.\",\n",
       " 'This kills the blood.',\n",
       " 'Now how bout a REAL Midtown Madness 3....',\n",
       " 'It takes plenty of skill. I only wish bozarking were here to explain it to you.',\n",
       " 'I want, on some milestone anniversary of Pokemon, for all google search results to appear as if they were in a pokedex. Read off in a vox voice too.',\n",
       " \"&gt; MS is not EldoS's support team.\\n\\nYou missed the part where an EldoS support person responded I guess, and that answer was marked as the solution by some MS person not involved in the conversation.\",\n",
       " \"Another reference online dated it at '71 when I went to find this pic recently.\",\n",
       " '[Under $90](http://www.amazon.com/Dualit-25375-Touch-2-Slice-Toaster/dp/B00027909I) for a basic one.',\n",
       " \"....or just a cheaper, closer shave than most of that bullshit we have nowadays.\\n\\nI've been meaning to get one too.\",\n",
       " '**SWEEP THE LEG, JONNY**',\n",
       " 'Farnsworth?',\n",
       " 'Remap a key using an application. I did this before to my netbook that had a smaller right ctrl key than the stupid right-click menu button.',\n",
       " \"Indeed it is. But more people will understand what right click menu button would mean, so that's why I used that name instead.\",\n",
       " \"Is there a chance that it just isn't the location in the title?\\n\\n**DAMN RIGHT**\\n\\nI used tineye and checked every link given, and finally found [one with a name](http://es.inmagine.com/similar-images/CD188047). It is indeed a stock image in many downloadable wallpaper packs... [Tianxingqiao waterfall](http://es.inmagine.com/landscapes-of-china-photos/pixtal-pt188). Google images has multiple angles and a few videos of these falls. This is actually real.\\n\\nI hope you know that.\\n\\n**OGGG ZUGGG ZUGGGG ZUGGGGGGGGGGGGGGG**\",\n",
       " 'Chinese physics, dude.',\n",
       " 'No, but surely a quick google search shows you the hundreds of chinese travel agencies that take people to the resort located right next to it.\\n\\nSounds like you need to go make a wiki page to believe it exists. The fuck did you do before the internet?',\n",
       " \"I found a few tours of the area with hotel accommodations for $350 over 4-5 days. As soon as American Airlines merges with USAirways, I'll be headed there.\",\n",
       " 'It\\'s the main reason I got a Dell Mini 1012 instead of another netbook years ago....it had 1366x768 resolution and supports 1080p video processing when most 10\" netbooks were at 1024x600 and couldn\\'t watch a single video without shitting everywhere. /*shudders/*\\n\\n156ppi vs 117ppi is a huge difference.',\n",
       " 'Some of the games translate it as Demon Ray.',\n",
       " 'According to you, what are the vitals you want to power? This is very important and can make the costs thousands or hundreds.',\n",
       " \"Freezer and fridge shouldn't need backup unless you expect power outages to persist for longer than 24 hours. Properly sealed appliances keep their cool that long, and are often designed to deal with such power outages.\",\n",
       " 'Holy shit....there would be riots in my area if it were that long.\\n\\nYea...get a whole house generator.\\n\\n',\n",
       " \"I'd probably try that with [this peanut sauce recipe](http://www.closetcooking.com/2010/07/thai-spicy-peanut-chicken-enchiladas.html), but it does sound good :)\",\n",
       " \"First thing I thought. Now 51 would be an EXCELLENT level if you were heading towards the airport....west carson and the cliffside entry to the busway, boss battle in downtown mckees rocks, and then a survival mode on neville island. Neville island's Poison Park zombies would be  awesome!!!\",\n",
       " '&gt;  Mario and Donkey Kong, and to a lesser extent Zelda are exactly that. Kid games. Nothing less and nothing more. \\n\\nThey are platformers that offer a degree of difficulty that progressively got harder as the game went on. Most of these Family Friendly™ games we have nowadays are not interesting enough to even consider, let alone there is no difficulty level.\\n\\nExample: Elevator Action could be described as a platformer, but since it had very monotone music, no boss level, no difference in levels, and it had sorta clunky controls, it can be played for a little while before switching to a different game. It felt like a high-school experiment. Many of these Family Friendly™ games coming out and announced at E3 feel like fluff developed by interns with zero replayability, and are only sold to attempt to sell more consoles.\\n\\nAt the end of the day, you may be right about some of your points, but there is no denying that these games are utter mediocre crap that rejects the main group of gamers in hopes of bringing new money into the fold.',\n",
       " 'Only complaint is that it still only offers Stretch/Tile/Center. I use a wallpaper changing app to use Worst Fit so that widescreen wallpapers work on my fullscreen work monitors, and visa versa at home.',\n",
       " 'Today I showed a woman how to shutdown her computer with Winkey -&gt; U -&gt; enter and she nearly shit her pants.',\n",
       " \"&gt; However, if you buy a non-subsidized S III from some other source and use it on your Verizon plan, you keep grandfathering.\\n\\nI was under the impression that you were required to buy the unsubsidized phone directly from Verizon or this wouldn't work.\",\n",
       " \"IIRC this is the one I typically use in Hiren's boot cd. Works quite well. Despite the warning, I usually find blanking the password to be more reliable than trying to set it.\\n\\nUsed it to recover a lost local domain admin password once. I love it.\",\n",
       " \"I'm not trying to make sense of it....I'm saying that the wording of their press release when then mentioned unsubsidized made me believe that it had to be purchased unsubsidized though verizon, although i may have read it quite wrong.\",\n",
       " \"I don't have a W7 box infront of me, but I believe it's WinKey -&gt; Arrow Right if you want the default option, and WinKey -&gt; Arrow Right -&gt; Arrow Right if you want the dropdown.\\n\\nI like navigating with keyboard occasionally.\",\n",
       " \"Don't think he's the only one like that either.\",\n",
       " 'They\\'re all \"radio station funny\", which isn\\'t actually funny.',\n",
       " 'Probably /r/techsupport or /r/buildapc .\\n\\nBoth would have people in there with opinions that might help.',\n",
       " \"This AMA is nothing like [Woody Harrelson's AMA trainwreck](http://www.mediaite.com/online/woody-harrelson-ama-ask-me-anything-goes-disastrously-wrong-on-reddit/). I don't even play the game and I'm impressed!\",\n",
       " 'I actually found it to be too buggy and having more non-wallpaper features than I needed. I currently use Automatic Wallpaper Changer, but even this has a few bugs here and there.',\n",
       " \"Could you re-explain for me? I read it 3 times and I know I should be getting it. Maybe it's the food I ate for lunch. I'm confused and sad.\",\n",
       " \"When there's no more sun, it won't really matter then anyways. We'll either be dead or on another planet.\",\n",
       " 'The windows key...between Ctrl and Alt.',\n",
       " '&gt; Vaticunt',\n",
       " \"Ah. I knew it was something like this, but the dots weren't connecting in my head.\\n\\nThank you. I really mean it.\",\n",
       " \"I believe it doesn't, but i would still test out the demo if I were you.\",\n",
       " 'Doesn\\'t always stop some MSI installs, can hose some windows update machines, and sometimes can cause a machine to go into sleep mode instead depending on the settings (or if it lost policy).\\n\\nIt\\'s much better to just throw it in the trash. \"Hey I got an idea! Why don\\'t we buy some ice cream and then throw it out?\"',\n",
       " 'Spoilers ahead.\\n\\nHis parents sacrifice their lives to make sure he lives on, but they lock away an insanely powerful monster inside his body that they can\\'t control but hope that he can. The monster was previously inside his mother, and the seal that keeps it locked away weakens during childbirth (of course).\\n\\nSo some powerful asshole comes along and fucks that up.\\n\\nThe monster eventually gets sealed inside of Naruto, both parents die, the bad guy runs away, and the monster just killed a shit ton of people. Now everyone hates Naruto since his birth, so basically the entire anime is him proving people wrong, being an unteachable idiot who is headstrong, befriending the world one person at a time, and basically creating world peace the slow and hard way.\\n\\nThe full details of that event weren\\'t in the anime until Shippuuden Episode 248-249, and I don\\'t have enough time to skim through the manga to find it again. The anime did it better justice this one time, IMO.\\n\\nAt the end of the day, it is hardly the saddest thing on there, but that is always open to interpretation. If I had to pick an anime character who had it worst for dead parents, it would probably be Nami from One Piece (her mother-figure shot dead in the face right infront of her after she begs for her life and says \"i love you\") or Fullmetal Alchemist and the whole episode about Nina (father basically turns his cute little daughter into a monster for an experiment, with full knowledge of what he was doing).',\n",
       " '\"I\\'d like to give all the phonies of the world a nice swift kick in the ass.\" - The Laughing Man',\n",
       " \"I usually type Win or Windows Key, but lately the book I've been using for a project refers to it as Winkey.\",\n",
       " \"Then you're using a mac keyboard, and I have no idea if that will work the same way, no matter your setup.\",\n",
       " 'If you ever play any half-life based game, here\\'s a bind script that you might find useful. It reloads while you hold the R key, and on release stops a reload and cycles quickly through the weapons in your first 3 slots and back to your primary, as if you have them on your mousewheel (default controls in some games, which would stop a reload). Mind you, I used it for CS:CZ, so you may need to change a few variables:\\n\\n1. userconfig.cfg\\n\\n*     alias w \"wait\"\\n*     alias w3 \"w; w; w\"\\n*     alias +reload2 \"+reload\"\\n*     alias -reload2 \"-reload; slot3; w3; slot2; w3; slot1\"\\n*     bind \"R\" \"+reload2\"\\n\\n2. config.cfg\\n\\n*     bind \"R\" \"+reload2\"\\n*     fps_max \"50\"\\n*     // for old CS 1.6 based games, this is a lifehack\\n*     // don\\'t believe me? then check out:\\n*     // http://www.fortress-forever.com/fpsreport/\\n*     // it makes the auto-shotty sorta useless tho\\n*     exec userconfig.cfg\\n*     // re-executes alias mapping on bind command\\n\\nAnd then mark both files as read-only so a server doesn\\'t change that.\\n',\n",
       " \"I have a very soft spot for:\\n\\n* [The Eagles - Hell Freezes Over](http://www.amazon.com/The-Eagles-Hell-Freezes-Over/dp/B0007Y89ZM/ref=sr_1_1?s=movies-tv&amp;ie=UTF8&amp;qid=1339070955&amp;sr=1-1)....if you have a DTS-capable surround speaker setup, this is quite awesome and the recording was done by some masterful sound engineers at Sony, who basically made the stage iirc. Would love a blu-ray cleanup of the video.\\n* [Carly Simon - A Moonlight Serenade On The Queen Mary 2](http://www.amazon.com/Carly-Simon-Moonlight-Serenade-Queen/dp/B000BNX5AI)...an excellent assortment and while she shows her age at times, she really puts her full heart into it and her voice gets better as the night goes on. It makes you smile.\\n* [Styx - Return to Paradise](http://www.amazon.com/Styx-Return-Paradise/dp/B00001ODGR/ref=sr_1_1?s=movies-tv&amp;ie=UTF8&amp;qid=1339071113&amp;sr=1-1)...ya gotta like some styx songs, but damn, this is quite a concert with some great renditions on classic songs. It starts out *okay*, but the audience gets into it about 3-4 songs in and then it fucking explodes into awesomeness.\\n\\nBut yea, I've gone through about 3 copies of The Last Waltz before I started making backup copies. Shit is amazing.\",\n",
       " 'Power policies since atleast Vista can change that single power press into hibernate or sleep mode.',\n",
       " \"IIRC, there weren't alot of power control policies for XP.\\n\\nSometimes if a W7 laptop were to lose policy temporarily, it would change from shutdown to hibernate mode and really screw up a PC with an app of ours that hates hibernate.\",\n",
       " \"We just got done setting up a nice surround theater in my basement, so I finally get to appreciate some of the surround mixing on my dvd collection. Figured reddit wouldn't downvote me on my honest opinion. Meh.\\n\\n*flips coin*\\n\\nWell, I'm pretty sure the wallpaper I'm gonna submit in a second will work :)\",\n",
       " \"Preview: http://i.imgur.com/aZzxE.jpg\\n\\nI never stitched together an image before, but I made this using Hugin from 13 or 14 of my co-worker's pictures from her visit to Hawaii. It probably isn't the best job, but I couldn't help but to try.\",\n",
       " 'two fags with their butts touching?',\n",
       " 'disco ball',\n",
       " '&gt;roflaarp',\n",
       " 'Angela Lansbury gets me all hot and bothered....',\n",
       " '[disco ball](http://www.reddit.com/r/AskReddit/comments/a8a2v/what_is_the_official_name_for_those_mirror_disco/)\\n\\nPittsburghers know it all.',\n",
       " 'Nope.',\n",
       " \"Preview: http://i.imgur.com/aZzxE.jpg \\n\\nI never stitched together an image before, but I made this using Hugin from 13 or 14 of my co-worker's pictures from her visit to Hawaii. It probably isn't the best job, but I couldn't help but to try.\",\n",
       " \"Let us start off by saying what can be synced with your phone that is built-in to android: Calendar, GMail, Contacts, Drive (docs), Reader, Picasa.\\n\\n1. If I were you, I'd stick with the stock calendar app and just use something like [Pure Calendar Widget](https://play.google.com/store/apps/details?id=org.koxx.pure_calendar&amp;hl=en) to view it in a useful manner.\\n2. Your contacts will be synced in the background. There are 3rd party address book apps, but most aren't necessary. I'd wait to see what the S3 comes with by default for managing them. I used to use [AContact](https://play.google.com/store/apps/details?id=com.movester.quickcontact) and an [addon for it](https://play.google.com/store/apps/details?id=movesti.contact.groupexpand). Nowadays I just use the default for my rom which has similar functionality, and I use [Call Log Calendar](https://play.google.com/store/apps/details?id=jp.joao.android.CallLogCalendar) to track stuff in a way more useful to me.\\n3. Others will give better advice here. I'd say Winamp.\\n4. I personally just use the mobile site and [Narwhal Notifier](https://play.google.com/store/apps/details?id=com.quicklookbusy.narwhalNotifier) to stay light on resources, but to each their own.\",\n",
       " 'thank you for notifying me. first time submitting too.',\n",
       " \"I will...thanks. It was my first time submitting, so the way I understood it putting [OS] made everything ok if it wasn't an approved host. I mean, there really isn't anywhere I can upload a 120mb image like imgur.\",\n",
       " 'I just resubmitted, and hopefully got the correct Picasa link since they seemingly \"upgraded\" it to google+.\\n\\nThanks again :)',\n",
       " \"I'm resubmitting in a sec...screwed up the link.\",\n",
       " 'Full Resolution 120mb image is [here](https://docs.google.com/open?id=0ByYv6HMqT9rtbkZQbURrbXR4d0k).\\n\\nEDIT: [If the Picasa link breaks (meh at G+ upgrade)](http://i.imgur.com/aZzxE.jpg)',\n",
       " 'I love that new people are learning about this that have this. I have it too.\\n\\nAt the same time, I hate even thinking about how much I could bitch about this being a repost. DAE and TIL cover this about every month or so.',\n",
       " 'The fabled half-pound cheezy bean and rice burrito:\\n\\n1. Refried Beans\\n2. Cheese\\n3. Rice\\n4. Diced Tomatoes\\n5. Chives\\n\\nThey used to be $1. They were amazing.',\n",
       " 'Not much of a fix, but if you ever need to uninstall, this works on the older version of Trend AV in my office. Probably requires local admin:\\n\\n    net stop TMBMServer\\n    reg add HKLM\\\\SOFTWARE\\\\TrendMicro\\\\PC-cillinNTCorp\\\\CurrentVersion\\\\Misc. /t REG_DWORD /v \"Allow Uninstall\" /d 1 /f\\n    \"C:\\\\Program Files\\\\Trend Micro\\\\OfficeScan Client\\\\NTRmv.exe\"\\n    del \"C:\\\\Program Files\\\\Trend Micro\\\\\" /Q',\n",
       " 'Our update probably wont happen for weeks unfortunately.',\n",
       " \"I'll record it all for gonewild.\",\n",
       " 'I saw so many nice cars in Texas that looked like they never had a wash since they were purchased. It made me feel sad.',\n",
       " \"Considering it was made at 5pm EST, he's either early for class, or staying late to fuck the professor, depending on the timezone. Unless this is a night class.\",\n",
       " 'It made the filesize smaller and was the default export option out of the stitching program I used. I tried various effects to bring the 13 photos together but it was my first time using hugin.',\n",
       " \"I meant for the full quality export with no compression out of the application I was using. The default export was png down to 120mb and the jpg was around 166 iirc.\\n\\nI'm glad you were able to get it down to 17mb for your use. I simply left it as a preview for those that like water pics, and full original quality for others to do whatever they wanted with, like you did.\",\n",
       " \"I'm talking about a 70k beamer that was 3 years old and had mud on the sides from some rainstorm that probably happened years ago. It made Ohio cars look clean in comparison.\",\n",
       " 'Its ok. I love texas. Just visited a few days ago. Did the original rudys, henrys puffy tacos, mi tierra cafe, the salt lick, schlitterbahn, six flags fiesta texas, and...of course...buc-ees in new braufelds.\\n\\nIt was a great vacation.\\n\\nAlso, being from pittsburgh, texas has loads of steelers sports bars :)',\n",
       " 'Their pork ribs were heaven, but I actually like rudys brisket better.',\n",
       " \"I'm in the US, and have been listening to streaming TripleJ for the past ~15 or so years. It's awesome.\",\n",
       " \"I still can't find my sing-a-long cassette.\\n\\nBut I do have two [ren and stimpy jello molds](http://i.imgur.com/m6zQY.jpg) :)\",\n",
       " 'Alot of people still sell them on ebay. I think they were a promo from Jello....something like \"buy 8 packages of jello and get these for free + $5 shipping\" and me and my dad needed them badly.',\n",
       " 'I kinda did too. It was okay occasionally, but I had other things I would rather watch.',\n",
       " 'Plus?',\n",
       " \"Indeed, its symbol is Fl.\\n\\nI'm sure someone will see that graphic in the article and make that mistake.\",\n",
       " \"Also, the recipe book it comes with has some pretty legit recipes. I think i tried about 20 butternut squash recipes before I found the one in their book.\\n\\nHowever, I still recommend a small black and decker food chopper for small-duty stuff. We've had ours for many, many years.\",\n",
       " 'We are the menorrhagia',\n",
       " \"It is pretty weak and just really hot. Not alot of special there.\\n\\nI actually don't mind Seattle's Best at Subway when they make a fresh pot. Probably the strongest, non-shit coffee out of the fast food joints.\",\n",
       " 'When you mix hills and snow, and you drive anything with all-wheel drive, you can actually get up those hills now.',\n",
       " 'You accidentally a word.',\n",
       " \"My guess is that for some home users on IE6/IE7, windows update got hosed, and that IE8 download can't be applied, and they don't want to pay money for someone to fix it.\",\n",
       " \"Their RSS feed is strangely for *all* valve blogs.\\n\\nFortunately for you, I made a [direct RSS](http://pipes.yahoo.com/pipes/pipe.run?_id=f5205128d6ef4c1915d9a9ca57be69cf&amp;_render=rss) for that blog when I got annoyed.\\n\\nEDIT: See official feed link below. I'll still keep this yahoo pipe operational though, since I use their service alot.\",\n",
       " \"I'm guessing the same thing happened to him that happened to my dad...he started puking up blood, and then he started shitting it too. My dad apparently lost close to half of the blood in his body while on too many medications after his 3rd open heart surgery. He got the leak cauterized, and then got nerve damage in his left arm when the nurse told him to never move his arm from the hospital bed rail (because of all of the needles in it). Ulnar nerve damn near severed. The morphine helped him never realize that he would lose 90% use of his left arm.\",\n",
       " \"Smoking of any kind or any of the other shit they put in their mouth to get nicotine.\\n\\nIf I'm gonna kiss a girl, I don't need to be worrying about the chew she needs to spit first.\\n\\nfuck. that. shit.\",\n",
       " \"I love you. Thanks.\\n\\nI've grown too accustomed to using yahoo pipes or feed43 for making feeds that have what I want....so I completely forgot about that :)\",\n",
       " \"My KFC has a bowl of side salad and hot bread on it's buffet bar.\",\n",
       " 'My body can take it.',\n",
       " \"&gt; Why would they take on an expensive, ambitious and risky project like that when they can just keep churning out the same Pokemon portable games with small tweaks to each generation and know they'll rake in a huge profit?\\n\\nBecause they know it can't last forever, and they could get into the console market and churn out some more games there.\\n\\n&gt; The core audience for Pokemon is little kids.\\n\\nThe **target** audience is little kids. The core audience is anyone that played the original games and enjoyed them, and would be willing to purchase a game like this.\\n\\n&gt; Their parents might be leery of an online component.\\n\\nNo arguments there. I'm sure nintendo could find a way to handle that, though.\\n\\n&gt; Kids might be turned off by the complexity of what you describe.\\n\\nNope. This game would be hyped so much, and would start out with the same simple playstyle that we've grown to love. There would be no chance of this.\\n\\n&gt; Additionally, a lot of more mature gamers who would enjoy this style of game are turned off by Pokemon brand because of its childish nature. They'd be taking a big risk on attracting a relatively niche audience compared to who they're selling to already.\\n\\nTry again. Nostalgia and sheer popularity can win any day. Who they've sold to in the past and who they're selling to currently are very different groups, but all would love this game.\",\n",
       " \"You'd have to trade with someone in a snowy region for a jynx :)\\n\\nAnyways, Ice Punch is a splendid move.\",\n",
       " \"Is B2/W2 going to be for 3DS only?\\n\\nI actually don't know which of the newer games I can play....I just got a used DS Lite for $20 from someone.\",\n",
       " 'LAVA COOKIE',\n",
       " \"I agree, but I'd like to see a tough balance between 2d and 3d. I want both to be used. Fuck, they could cell shade it for all I care. Just as long as the game is made.\",\n",
       " \"they did say something about dropping flash support eventually, so I honestly wouldn't be surprised if it was dead for 5.0. couldn't find sauce tho.\",\n",
       " \"I'm pretty sure that Pittsburgh is at that five point meeting of states.\",\n",
       " 'They had those at the Texas mariott that i just went to.',\n",
       " \"They also have Buc-ees. I can't even begin to explain Buc-ees with how much time I've got right now.\",\n",
       " \"Don't tell Pitt!!\",\n",
       " '[Yup](http://i.imgur.com/qJY65.jpg)',\n",
       " 'What reminds you of Death Note in that picture?',\n",
       " 'Use keyboard remapping software to remap it to something you might like.\\n\\nhttp://www.makeuseof.com/tag/remap-keyboard-free-tools-windows/\\n\\nI personally use KeyTweak for my netbook\\'s \"context menu\" key that is bigger than the right ctrl key.',\n",
       " \"&gt; pittsburgher\\n\\n&gt; currently wearing pens jersey\\n\\nyea.....i'll stay over here for this one ;)\",\n",
       " 'Okay....I can see that :)',\n",
       " \"As long as there is an opportunity for good nicknames, it doesn't matter.\",\n",
       " 'maximum amount of jimmies rustled',\n",
       " \"Oh don't start. I'm still disappointed that [Hungry Hungry Hippos](http://i.imgur.com/qqrwO.jpg) wasn't a real movie.\",\n",
       " 'Seriously, most people have shit, or the same shit as me. This collection really does rock.\\n\\nIs there any chance that you could upload your other wallpaper albums? So far your judgement is awesome when it comes to awesomeness.',\n",
       " 'Get some Walmart-brand pineapple soda (better than faygo) and mix it half-n-half with coconut rum. Call me in the morning.',\n",
       " 'aka In n Out burger',\n",
       " 'I worked selling shoes for 2 years on a weekend-only basis during school at a national chain. Never offered a raise, never offered to open the store, never given any recognition. When I asked for some more responsibilities, I was told I was unimportant, as two new outside managers were coming in. One managed a section of Petsmart, and the other had no prior experience. I put in my two weeks notice. Both people came in on my last day, so I showed them everything I learned and all the small quirks of the inventory that we had. Both quit within a week, and the store closed within 4 months.',\n",
       " 'Eh the new kid never did anything to me. I had another job lined up.',\n",
       " 'He had a manager that only worked Tuesday thursdays. This guy was just a cockhole.',\n",
       " 'iirc, that is the same pattern that the paper wrapper usually gets on the end of the straw.',\n",
       " 'apparently you forgot about this subreddit.',\n",
       " 'Should get a job at Buc-ees.',\n",
       " 'there is no window in the window shot though :(',\n",
       " 'Yea but I like pretty windows since I build things :(',\n",
       " 'Did he have two rhyhorns that know earthquake?',\n",
       " 'It is. Throw some crumbled ice in there and put cherry syrup and a cherry ontop to make it fancy.\\n\\nIts important to use the Walmart kind though. ',\n",
       " 'Rain is his weakness.',\n",
       " 'I may be off by a few, but I counted 1836.\\n\\nCopied source from beginning of list to end of list, ignoring citations. deleted all links to pictures by searching for **editsection** and deleting up until the next &lt;ul&gt; i would find. I then counted the number of **title=\"** in the source and got 1860. Then I manually ran through the list checking for multiple links per manufacturer and counted 25, and 1 that had no links, so that is 1860-24=1836\\n\\n1836 was the year of the Battle of the Alamo, by the way.',\n",
       " \"I hope you're not counting the citation links,as those don't have the title field I was searching for. Otherwise I have no clue what you're referencing \",\n",
       " 'YAY!',\n",
       " 'Office Communicator would have their panties in a bunch.',\n",
       " 'Thank god SOMEONE mentioned Reader:\\n\\n1. Clicking the Reader widget causes it to hang unless I press the home icon, if the widget is supposed to list all feeds.\\n\\n2. The changeover to google+ functionality still left some unselectable \"mark as unread\" options.\\n\\n3. The app still doesn\\'t handle some image content properly.\\n\\n4. The app still doesn\\'t fit some feeds to be completely viewable on the reader screen.\\n\\nIt could use a full overhaul for ICS. It gets ZERO love.',\n",
       " \"I want to say something quite vulgar to you, but can't find the right words.\",\n",
       " 'Both of them are quite awesome. Boston Legal was amazing.',\n",
       " \"Sadly enough, most of the Denny's in my area (Pittsburgh) closed down because of competition from [King's](http://kingsfamily.com/) and [Eat'N Park](http://www.eatnpark.com/).\\n\\nI stopped going after they got rid of the **[Breakfast Dagwood](http://www.youtube.com/watch?v=nFq0XY2WjzU)**.\",\n",
       " 'I used to use Winamp and [Sonique](http://en.wikipedia.org/wiki/Sonique_%28media_player%29). How sad that corporate buyouts can kill software like that.',\n",
       " 'I used to do this:\\n\\n* 5 stars - Songs good for anyone, for a large party\\n* 4 stars - Songs my friends like, but others may not\\n* 3 stars - Songs only I like\\n* 2 stars - Every other song from the album that is meh\\n* 1 star - songs I never want played, but keep just to have the full album stored\\n\\nThen I can just set \"songs 3 and above\" for my car, or \"2 and above\" when I\\'m driving a long distance and want a large mix, or \"1 star only\" when someone is in the car that I want to piss off.',\n",
       " 'Its actually a media file functionality I think is on every OS. ',\n",
       " 'Ah.  Yea not alot support it. ',\n",
       " \"Although this is a [known problem with this Mac app](http://www.reddit.com/r/gaming/comments/vkkso/i_was_playing_solitaire_last_night_and_the/c55cjye), I've noticed windows Hearts not calculating out a proper win percentage when I played 100 games.\",\n",
       " 'rubbernet',\n",
       " 'Thank you for the awesome response.\\n\\nWould simply rounding up fix the issue, or would there potentially be the problem of 101% ever occurring?',\n",
       " 'someone just posted it above:\\n\\nhttp://www.reddit.com/r/Android/comments/vke83/to_all_galaxy_s3_owners_that_do_not_want_to_root/c55a1do',\n",
       " \"If he's saying it works the same way as GoLauncher, then it might actually work. [Someone above posted a solution that lets you remove them with GoLauncher.](http://www.reddit.com/r/Android/comments/vke83/to_all_galaxy_s3_owners_that_do_not_want_to_root/c55a1do)\",\n",
       " \"Thank you kind sir. Now I've gotta find a way to request a patch :D\",\n",
       " 'In our high school, we had a large tub/pump for our ketchup that was approximately 5 gallons, with a lid that just set on top...no screwing required. We proceeded to dump liquid laxative into it and mix it quickly before lunch began. We had 4 sets of bathrooms, two of which were out of order (the set closest to the lunch room, and the set furthest from the lunch room). The other two were the gym locker room bathrooms which were tiny, and a second locker room set that the outside sports teams used.\\n\\nPeople were spewing shit all over the gym floor trying to make it to any of them.',\n",
       " 'Definitely a scammer font.',\n",
       " 'OP will surely deliver!',\n",
       " 'Oh I completely agree. I had nothing to do with it, and simply found it funny because I never get ketchup so I was one of the few people that was unaffected. The kids were arrested and were sued by a few people, but it was the first real senior prank in years, the last being some lame attempt at ripping out trees on school property, and before that the ol\\' \"1,2,4,5,6\" greased pig prank.',\n",
       " 'I talked about Peanut Butter and Jelly sandwiches for hours apparently. Even while the teeth were being taken out.',\n",
       " \"I finally started getting better at [MDT](http://technet.microsoft.com/en-us/solutionaccelerators/dd407791.aspx) and can now say that I'm a pretty damn good amateur expert in that shit. Granted, I'm no expert expert, but I know how to deploy my monkeys properly now..\",\n",
       " 'transbeastophile',\n",
       " \"Is it really hipster to use fixed gear bikes? I wanna say that shitloads of people in Pittsburgh use those and aren't hipsters at all.\",\n",
       " 'I sold my Verizon global Blackberry 99xx on craigslist for $160 only 2 months ago....a mother wanted her daughter to have a verizon phone in Spain because she didn\\'t trust \"those scary phone companies over there\".\\n\\nI tried to give her the international adapters for free with it, and she said \"we already have chargers\". Whatever.',\n",
       " \"In retrospect, she had nothing in her hands, but I just think that she didn't wanna carry anything. She put the phone in her pocket, paid in cash, and left.\",\n",
       " '[this one is 1600x940, but +1 for no people in shot](http://i.imgur.com/hJCoL.jpg)\\n\\npreviously on reddit',\n",
       " 'They will never work on the road?',\n",
       " 'No. Any U or P series has a boxy frame back then, so this is the U2412s.',\n",
       " \"I'll be the dick that nitpicks :)\\n\\n1. Plastic overtop of Dell on back of monitors = anarchy.\\n2. Might as well buy the $140 dock and get nearly the same performance and save $700 by not buying the desktop.\\n3. Spend the extra $3 next time to get the better keyboard with the wrist rest. Your employees will thank you, and can remove the wrist rest if they hate it. Ergonomics are a huge fucking deal nowadays.\\n4. Should probably get a AX510 speaker bar too :)\\n\\nOverall your setup is great though. Ours is similar, although we only get P-series monitors.\",\n",
       " \"It's almost policy in my job that if you deserve dual widescreens for anything, you deserve a laptop/docking station/speaker bar/better keyboard and mouse/gel mousepad since you'll be working your ass off.\",\n",
       " 'well, OP answered it.',\n",
       " 'You must have not been here for [disco ball](http://www.reddit.com/r/AskReddit/comments/a8a2v/what_is_the_official_name_for_those_mirror_disco/).']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:20:32.155596200Z",
     "start_time": "2023-12-11T18:20:32.077437800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm sad that samsung no longer makes its own drives :(\", 'I remember the GLORIOUS DAY when Subway actually had a letter go out to store managers asking that all employees start tessellating the cheese. I have to remind them to do it still :(', 'Does it need any?', 'disco ball', \"I'd second ES File Explorer. Being able to copy files from my Windows Shares easily is what made this app golden. Others I like: Parcels for tracking my many packages I send or receive. Aix Weather all that you need for a weather widget AlarmDroid Contact Simple Widget simply because it uses nickname values from contacts properly GasBuddy use it weekly....great on vacations Google Voice Narwhal Notifier for redditors Paid, but still so useful :( Pure calendar widget * Scores Widget\", \"After actually using that gas app nonstop, I can't live without it.\", 'I would totally use this if: It had text size adjustment. Mine is set very small in pure calendar. Smooth has huge text. it had scrolling support. I currently can view about 30-40 events in my pure calendar.', 'disco ball', 'Indeed, but the delivery is almost the exact same as the Afroman song.', \"Eventually he will, and you will need to have a justification for what you have done. Do not mention the bonuses. At my place, we are supposed to, by department, generate graphs showing our work order flow (amount still open and who assigned to, completed per week for last 3 weeks, completed this week per person, etc) and I took the time to write a script that grabs all of that data from SharePoint, creates an excel sheet with equations and graphs on it, and then publishes only the graphs to a SharePoint page. With the multiple data sources, this used to take me 15-25 minutes, and now it takes 20 seconds. I have it set as a scheduled task the day they are due. Because I have always had my graphs in on time and they are 100% accurate (it sucked having to make them every Monday morning as the first thing I needed to do), I've received numerous raises. Don't feel morally obligated to turn down/off your ace-in-the-hole. You are dedicated to getting the job done fast, and are helping your team succeed, and should be rewarded.\", 'Also legal issues.', \"I'd rather it not. I'm waiting for the opportunity to help someone quit the internet. The hard way.\", 'We are NOT on 4chan, nigger.', 'Next episode of CSI: Redditors kill someone with quinine!', 'What if it was.....over 9000?', \"Isn't there also a general belief that knives of any sort are what not-so-good people have?\", 'IIRC, in PA (which has ass-backwards alcohol laws and nearly everything is state-run) you can buy wine/liquor from a few state-run stores on sunday from 9am to 9pm (but most have kept it closed, or 12-5).', 'THEY ARE SHIT', 'Just saw Here Come The Mummies in Pittsburgh at the Altar Bar and they used TicketFly.', \"The other person could have been worse off than you, and now they're out of money and you're out of morals. Sounds like gaining the money would do nothing to improve that situation at all.\", 'What is the poster in the top right of the 3rd pic? FMP?', 'I remember when it was only 300 or so...wow.', \"Goto shows that use something like TicketFly then. I've come across quite a few in the Pittsburgh area.\", 'graphical representation is in the pro version, but you also get shittons more features than that in the pro version.', 'So basically, Win+D.', 'Okay :(', 'Almost everything I want to see is at TicketFly or at the Benedum :) I almost always buy a center directors circle row for a big show at the benedum in advance and sell half the seats to pay for the ones I use....its like going to the show for free.', \"I talked about it further down, but if you're in the Pittsburgh area, use TicketFly to see some pretty awesome shows instead of TM. And fuck, the shows in all of Pitt's theatres!\", 'I really like the tiny foil spot on my TicketFly ticket. Makes me feel special :)', 'Or make it a non-required question.', \"99% of the time, you don't get anything if you're an intern in the US, paid or not.\", 'Do you know if yours is legit? I\\'ve come across enough fake cartridges lately to make me concerned...typically, the label isn\\'t shiny if its fake, or it will say \"The save file will be loaded. The game can be played.\" at the load game screen.', \"> get a decent pair with an ok warranty. Sometimes, depending on where you buy them, this may mean buying that extended warranty that normally makes no sense to buy. With easily breakable items that you're gonna use often, it makes a little more sense. I bought a $100 pair of headphones and got a 3yr extended warranty on them for $20, and sure as hell they stop working well within the 3yr timeframe. Now I get a new pair, and that pair also gets a new 3-year warranty for free since my last pair broke. This doesn't work with everything, and I wouldn't recommend it for cheaper headphones, but if you're rough on them it can help.\", \"If it's Vista or lower you're on, I'd recommend trying ToolTipFixer. Not entirely sure if it's needed on W7. It also takes care of stuck tooltips, which is what I'm thinking you might have.\", \"Exactly. If you're only buying 20$ headphones and replacing them every year, it makes sense to save up and buy nicer headphones and spend the same money by getting the extended warranty :)\", \"You forgot Rudy's, puffy tacos, and Schlitterbahn. And I'm from Pennsylvania. How dare you.\", \"Henry's Puffy Tacos is an enlightening experience. Go prepared to get gassy, and maybe buy a t-shirt. Rudy's is so fking good. Look over the menu beforehand, get alot of brisket, some soda, and DEFINITELY GET SOME CREAM CORN. Shit, their cream corn is so popular, they'll sell it by the 5gal bucket if you want that much. I also recommend going to the original one, if you live within feasible driving distance.\", \"What all do you want for it? I'm just lookin for a copy for my younger brother so we can battle against each other.\", 'Na we got a cable to plug my advance into his SP.', 'I could do cash or a steam game whichever you think woild be better.', \"You definitely haven't been here long enough to know how insane they've gone trying to make search even work. This site is gigantic, and the search functionality is far better than it was only a year ago.\", '> Implying that reddit only upvotes circle jerkers in a thread like this', '> Basically, did everyone in the room see a crippled kid hobble into the room and set up a special computer and tell him \"Fuck You\" at ever turn? Would you, your friends, your teachers, hell the people you hate saw him struggling to keep up with a disability wouldn\\'t you offer to help? When I tried getting help when I was temporarily in a wheelchair, I was frequently confronted with the general \"NOT MY JOB\" bullshit and people saying \"yea we can do that\" and then never following through on it. And I\\'ve definitely had experience with schools that are run like businesses instead of educational facilities.', 'Not sure about white vinegar, but I use apple cider vinegar with some bentonite clay to make an awesome mask for my face.', 'Well I do use the mask to try and clean up my face, but I will admit that I stop using it for halloween to get my normal werewolf look rolling.', 'THIS MOVIE IS MAXIMUM!', 'NOBODY-FUCKING-CARES-ABOUT-YOUR-RES-TAG.com', \"The problem is that everyone has RES, and these posts add nothing to the conversation. If you want others to know what you tag things, go use twitter. I'm utterly tired of seeing it this week.\", \"I personally passed the Server 2008 test with cengage's LabSim software (simulates a real Server 2008 environment and gives you tasks and tutorials to accomplish the EXACT WAY that the test wants you to). Labsim for MCTS Guide to Microsoft Windows Server 2008 Active Directory (Exam #70-640) (ISBN-10: 1423902718 | ISBN-13: 9781423902713) I don't have the CD anymore, nor have I found it on the internet for any reasonable price (or free), but accompanied with either the book it goes with, or free materials, you'd pass easily.\", '> imagines voldemort burning doritos', \"> it is usually one of the first things a n00b can say because they watch a lot of anime Pretty sure I missed the connection there....how does watching alot of anime lead to saying kawaii neko? Pretty sure I've never said shit like that.\", 'Never heard it uttered in {SUB REDDIT}, and never heard it on tv-links when I ran that. Is this something new?', 'Ah ok. You mean faggots.', 'I just got that last one yesterday. Decided to go home early.', \"I'll go download one.\", \"cvpcs, huh.... > remembers CM7/CM9 Droid X builds Nah, I'd rather not rely on that again. Just gonna use MX Player and wait for official builds.\", 'So which does it resemble... the dark red Russian or pink-orangeish Thousand Island? because those dressings are very different, despite being argued upon which properly belongs on a reuben.', \"he's the top submitter in the *porn subreddits, that's why.\", 'It feels like that effect that people use when they want pictures to look like minatures....a few pics of it were on reddit in the past.', 'I happen to have the same mug, but with a handle. I wish I had yours.', 'Sometimes too hot for me :)', 'Last one nearly made me cry.', 'I WILL NOT MAKE A CONTRACT WITH YOU KYUBEY GO AWAY', \"All those poor logs have no idea they're about to die.\", \"> Yet another reason why I have no desire to get my license or drive ever. You're a real daredevil.\", 'awesome. I had about 13 of them :D', 'now THAT is mildly interesting.', 'Not too far off :(', \"I'm wondering...what does a devil's avocado taste like?\", 'Satire is exempt from all that crap. Where have you been for all these years?', \"It's actually Nintendo's policy that they deny ANY requests at using their copywritten/trademarked stuff. If they want you making something, they ask YOU to do it, or you're making it illegally. source: tried to get approval to create a DK tie :(\", \"Atleast it isn't Warren....duck and run from those gunshots!\", 'Like a thing when a bad happens!', 'fun fact: the 3.5 install still fails if you actually disconnect the cable during install after that message appears.', \"Sometimes there are lots of nuts. Sometimes there are lots of upvotes. I'm feeling mildly sorry.\", 'I recently went to a Dell conference in my area, showing off their new products. Here were the questions: 10% about the new midrange servers (given 1hr during talk) 5% about the new backup solutions (given 45mins during talk) * 85% about the KACE hardware (left until end and only given 15 minutes) Then they cancelled the local demo everyone wanted and replaced it with a webinar that was truly useless unless you were already using a KACE product. But none of that is your fault. I actually have never had a problem with Dell support for the 6-7 years I\\'ve been calling them...other than a snag with no fans in my 50 Optiplex 745\\'s over the harddrive. When asked about it, they assured me that \"fans were only put in towers with Core 2 Duo processors, and not older Intel processors....they run cooler\". Which is bullshit. Eventually got our gov\\'t account rep to get someone to send us a box of replacement fans (for the ones we were accused of stealing...lol) and it dropped temps 7-17 degrees across every computer. So yea, just send someone a fan if a 745 is running hot, even if it\\'s out of warranty, to be a GGG.', 'Or you could do the online chat support in IE, and get an emailed log of everything to attach to a work order afterwards.', 'We were offering the entire time to replace the fan, and were instead accused of stealing them', \"Indeed they aren't. We weren't very happy, especially being a large gov't contract in Pittsburgh.\", \"If you ever need just a better AC powered one, I highly recommend the Apricorn Drivewire. It has one of the more reliable power bricks I've ever used on these types of tools....have used mine nonstop for 2 years. Actually, the rest of their products are pretty nice too.\", 'you might like it here', 'We have automated ticket creation from emails sent to the helpdesk email address. It puts their name as requestor, copies any pictures as attachments, and copies the body as text.', 'I have a penis brain.', 'Are you telling me that Nina was in this show?', 'Exactly. These people are idiots, and we are the only two sane humans left.', 'I personally love galnet miui ics, but I doubt it will ever get updated again :( never seen my droid x get such a long battery life before.', 'I saw the end coming, but it was still done quite well :)', 'WE NEED THE PORN', 'Just me, or does that look illegal?', 'I also want whatever that 2nd image he posted was.', 'I can eat 50 eggs.', 'Are you merely joking like others in this thread, or are you for real? I\\'ve only got a 110\", and we\\'ve slowly pieced together every component over the years, but it\\'s a pretty good setup.', \"If they're unwilling to use it, they obviously don't respect you, let alone themselves. There are a few other potential reasons (haven't found the right size, so they always complain too tight), but EVERY reason that I can think of is a terrible excuse of why they can't respect their partner. If there was a pill out there that was as reliable as most female solutions and reversible, I'd get it in a heartbeat. This post title is infuriating to someone like me.\", 'Always here. Hide yo kids.....hide yo wife.', \"> dropped a black bag over his head So he disappeared into one of Creedy's black bags?\", 'When we finish the tables and beer tap, ill throw some pics up in the home theater subreddit and send you a link', 'I believe V said that artists use lies to tell the truth.', 'Nobody ever eat fifty eggs.', \"It could be done quite easily. Chop off some excess red from top, remove the grey crap from the edges and replace it with black, use the higher-res copy, and add black on the sides. I'm not near a copy of Photoshop or I'd attempt it, even with how terrible I am at Photoshop.\", \"IT does? Well then, I'd make him look like a termite with maracas if he saw my data center!\", 'I heard he likes flipping quarters in the alley, though.', 'I often use this as a wallpaper.', 'Huh...never knew it had anything to do with being a redditor.', 'Then utilize the exact formula with advanced calculations we already use to estimate things to account for all of that stuff you mentioned.', 'In 30 minutes, I cleaned up enough of the black artifacts to make it an acceptable wallpaper image. I kept it at that resolution in case someone wants to clean up the red artifacts at full quality, but it will scale down properly to 1920x1080.', \"He means there's still grey in the bottom left corner of where the poster ended before, but I understand what you're saying.\", 'This kills the blood.', 'Now how bout a REAL Midtown Madness 3....', 'It takes plenty of skill. I only wish bozarking were here to explain it to you.', 'I want, on some milestone anniversary of Pokemon, for all google search results to appear as if they were in a pokedex. Read off in a vox voice too.', \"> MS is not EldoS's support team. You missed the part where an EldoS support person responded I guess, and that answer was marked as the solution by some MS person not involved in the conversation.\", \"Another reference online dated it at '71 when I went to find this pic recently.\", 'Under $90 for a basic one.', \"....or just a cheaper, closer shave than most of that bullshit we have nowadays. I've been meaning to get one too.\", 'SWEEP THE LEG, JONNY', 'Farnsworth?', 'Remap a key using an application. I did this before to my netbook that had a smaller right ctrl key than the stupid right-click menu button.', \"Indeed it is. But more people will understand what right click menu button would mean, so that's why I used that name instead.\", \"Is there a chance that it just isn't the location in the title? DAMN RIGHT I used tineye and checked every link given, and finally found one with a name. It is indeed a stock image in many downloadable wallpaper packs... Tianxingqiao waterfall. Google images has multiple angles and a few videos of these falls. This is actually real. I hope you know that. OGGG ZUGGG ZUGGGG ZUGGGGGGGGGGGGGGG\", 'Chinese physics, dude.', 'No, but surely a quick google search shows you the hundreds of chinese travel agencies that take people to the resort located right next to it. Sounds like you need to go make a wiki page to believe it exists. The fuck did you do before the internet?', \"I found a few tours of the area with hotel accommodations for $350 over 4-5 days. As soon as American Airlines merges with USAirways, I'll be headed there.\", 'It\\'s the main reason I got a Dell Mini 1012 instead of another netbook years ago....it had 1366x768 resolution and supports 1080p video processing when most 10\" netbooks were at 1024x600 and couldn\\'t watch a single video without shitting everywhere. {SUB REDDIT}/ 156ppi vs 117ppi is a huge difference.', 'Some of the games translate it as Demon Ray.', 'According to you, what are the vitals you want to power? This is very important and can make the costs thousands or hundreds.', \"Freezer and fridge shouldn't need backup unless you expect power outages to persist for longer than 24 hours. Properly sealed appliances keep their cool that long, and are often designed to deal with such power outages.\", 'Holy shit....there would be riots in my area if it were that long. Yea...get a whole house generator.', \"I'd probably try that with this peanut sauce recipe, but it does sound good :)\", \"First thing I thought. Now 51 would be an EXCELLENT level if you were heading towards the airport....west carson and the cliffside entry to the busway, boss battle in downtown mckees rocks, and then a survival mode on neville island. Neville island's Poison Park zombies would be awesome!!!\", '> Mario and Donkey Kong, and to a lesser extent Zelda are exactly that. Kid games. Nothing less and nothing more. They are platformers that offer a degree of difficulty that progressively got harder as the game went on. Most of these Family Friendly™ games we have nowadays are not interesting enough to even consider, let alone there is no difficulty level. Example: Elevator Action could be described as a platformer, but since it had very monotone music, no boss level, no difference in levels, and it had sorta clunky controls, it can be played for a little while before switching to a different game. It felt like a high-school experiment. Many of these Family Friendly™ games coming out and announced at E3 feel like fluff developed by interns with zero replayability, and are only sold to attempt to sell more consoles. At the end of the day, you may be right about some of your points, but there is no denying that these games are utter mediocre crap that rejects the main group of gamers in hopes of bringing new money into the fold.', 'Only complaint is that it still only offers Stretch/Tile/Center. I use a wallpaper changing app to use Worst Fit so that widescreen wallpapers work on my fullscreen work monitors, and visa versa at home.', 'Today I showed a woman how to shutdown her computer with Winkey -> U -> enter and she nearly shit her pants.', \"> However, if you buy a non-subsidized S III from some other source and use it on your Verizon plan, you keep grandfathering. I was under the impression that you were required to buy the unsubsidized phone directly from Verizon or this wouldn't work.\", \"IIRC this is the one I typically use in Hiren's boot cd. Works quite well. Despite the warning, I usually find blanking the password to be more reliable than trying to set it. Used it to recover a lost local domain admin password once. I love it.\", \"I'm not trying to make sense of it....I'm saying that the wording of their press release when then mentioned unsubsidized made me believe that it had to be purchased unsubsidized though verizon, although i may have read it quite wrong.\", \"I don't have a W7 box infront of me, but I believe it's WinKey -> Arrow Right if you want the default option, and WinKey -> Arrow Right -> Arrow Right if you want the dropdown. I like navigating with keyboard occasionally.\", \"Don't think he's the only one like that either.\", 'They\\'re all \"radio station funny\", which isn\\'t actually funny.', 'Probably /r/techsupport or /{SUB REDDIT} . Both would have people in there with opinions that might help.', \"This AMA is nothing like Woody Harrelson's AMA trainwreck. I don't even play the game and I'm impressed!\", 'I actually found it to be too buggy and having more non-wallpaper features than I needed. I currently use Automatic Wallpaper Changer, but even this has a few bugs here and there.', \"Could you re-explain for me? I read it 3 times and I know I should be getting it. Maybe it's the food I ate for lunch. I'm confused and sad.\", \"When there's no more sun, it won't really matter then anyways. We'll either be dead or on another planet.\", 'The windows key...between Ctrl and Alt.', '> Vaticunt', \"Ah. I knew it was something like this, but the dots weren't connecting in my head. Thank you. I really mean it.\", \"I believe it doesn't, but i would still test out the demo if I were you.\", 'Doesn\\'t always stop some MSI installs, can hose some windows update machines, and sometimes can cause a machine to go into sleep mode instead depending on the settings (or if it lost policy). It\\'s much better to just throw it in the trash. \"Hey I got an idea! Why don\\'t we buy some ice cream and then throw it out?\"', 'Spoilers ahead. His parents sacrifice their lives to make sure he lives on, but they lock away an insanely powerful monster inside his body that they can\\'t control but hope that he can. The monster was previously inside his mother, and the seal that keeps it locked away weakens during childbirth (of course). So some powerful asshole comes along and fucks that up. The monster eventually gets sealed inside of Naruto, both parents die, the bad guy runs away, and the monster just killed a shit ton of people. Now everyone hates Naruto since his birth, so basically the entire anime is him proving people wrong, being an unteachable idiot who is headstrong, befriending the world one person at a time, and basically creating world peace the slow and hard way. The full details of that event weren\\'t in the anime until Shippuuden Episode 248-249, and I don\\'t have enough time to skim through the manga to find it again. The anime did it better justice this one time, IMO. At the end of the day, it is hardly the saddest thing on there, but that is always open to interpretation. If I had to pick an anime character who had it worst for dead parents, it would probably be Nami from One Piece (her mother-figure shot dead in the face right infront of her after she begs for her life and says \"i love you\") or Fullmetal Alchemist and the whole episode about Nina (father basically turns his cute little daughter into a monster for an experiment, with full knowledge of what he was doing).', '\"I\\'d like to give all the phonies of the world a nice swift kick in the ass.\" - The Laughing Man', \"I usually type Win or Windows Key, but lately the book I've been using for a project refers to it as Winkey.\", \"Then you're using a mac keyboard, and I have no idea if that will work the same way, no matter your setup.\", 'If you ever play any half-life based game, here\\'s a bind script that you might find useful. It reloads while you hold the R key, and on release stops a reload and cycles quickly through the weapons in your first 3 slots and back to your primary, as if you have them on your mousewheel (default controls in some games, which would stop a reload). Mind you, I used it for CS:CZ, so you may need to change a few variables: 1. userconfig.cfg alias w \"wait\" alias w3 \"w; w; w\" alias +reload2 \"+reload\" alias -reload2 \"-reload; slot3; w3; slot2; w3; slot1\" bind \"R\" \"+reload2\" 2. config.cfg bind \"R\" \"+reload2\" fps_max \"50\" // for old CS 1.6 based games, this is a lifehack // don\\'t believe me? then check out: // {URL} // it makes the auto-shotty sorta useless tho exec userconfig.cfg * // re-executes alias mapping on bind command And then mark both files as read-only so a server doesn\\'t change that.', \"I have a very soft spot for: The Eagles - Hell Freezes Over....if you have a DTS-capable surround speaker setup, this is quite awesome and the recording was done by some masterful sound engineers at Sony, who basically made the stage iirc. Would love a blu-ray cleanup of the video. Carly Simon - A Moonlight Serenade On The Queen Mary 2...an excellent assortment and while she shows her age at times, she really puts her full heart into it and her voice gets better as the night goes on. It makes you smile. Styx - Return to Paradise...ya gotta like some styx songs, but damn, this is quite a concert with some great renditions on classic songs. It starts out okay*, but the audience gets into it about 3-4 songs in and then it fucking explodes into awesomeness. But yea, I've gone through about 3 copies of The Last Waltz before I started making backup copies. Shit is amazing.\", 'Power policies since atleast Vista can change that single power press into hibernate or sleep mode.', \"IIRC, there weren't alot of power control policies for XP. Sometimes if a W7 laptop were to lose policy temporarily, it would change from shutdown to hibernate mode and really screw up a PC with an app of ours that hates hibernate.\", \"We just got done setting up a nice surround theater in my basement, so I finally get to appreciate some of the surround mixing on my dvd collection. Figured reddit wouldn't downvote me on my honest opinion. Meh. flips coin Well, I'm pretty sure the wallpaper I'm gonna submit in a second will work :)\", \"Preview: {URL} I never stitched together an image before, but I made this using Hugin from 13 or 14 of my co-worker's pictures from her visit to Hawaii. It probably isn't the best job, but I couldn't help but to try.\", 'two fags with their butts touching?', 'disco ball', '>roflaarp', 'Angela Lansbury gets me all hot and bothered....', 'disco ball Pittsburghers know it all.', 'Nope.', \"Preview: {URL} I never stitched together an image before, but I made this using Hugin from 13 or 14 of my co-worker's pictures from her visit to Hawaii. It probably isn't the best job, but I couldn't help but to try.\", \"Let us start off by saying what can be synced with your phone that is built-in to android: Calendar, GMail, Contacts, Drive (docs), Reader, Picasa. 1. If I were you, I'd stick with the stock calendar app and just use something like Pure Calendar Widget to view it in a useful manner. 2. Your contacts will be synced in the background. There are 3rd party address book apps, but most aren't necessary. I'd wait to see what the S3 comes with by default for managing them. I used to use AContact and an addon for it. Nowadays I just use the default for my rom which has similar functionality, and I use Call Log Calendar to track stuff in a way more useful to me. 3. Others will give better advice here. I'd say Winamp. 4. I personally just use the mobile site and Narwhal Notifier to stay light on resources, but to each their own.\", 'thank you for notifying me. first time submitting too.', \"I will...thanks. It was my first time submitting, so the way I understood it putting [OS] made everything ok if it wasn't an approved host. I mean, there really isn't anywhere I can upload a 120mb image like imgur.\", 'I just resubmitted, and hopefully got the correct Picasa link since they seemingly \"upgraded\" it to google+. Thanks again :)', \"I'm resubmitting in a sec...screwed up the link.\", 'Full Resolution 120mb image is here. EDIT: If the Picasa link breaks (meh at G+ upgrade)', 'I love that new people are learning about this that have this. I have it too. At the same time, I hate even thinking about how much I could bitch about this being a repost. DAE and TIL cover this about every month or so.', 'The fabled half-pound cheezy bean and rice burrito: 1. Refried Beans 2. Cheese 3. Rice 4. Diced Tomatoes 5. Chives They used to be $1. They were amazing.', 'Not much of a fix, but if you ever need to uninstall, this works on the older version of Trend AV in my office. Probably requires local admin: net stop TMBMServer reg add HKLM\\\\SOFTWARE\\\\TrendMicro\\\\PC-cillinNTCorp\\\\CurrentVersion\\\\Misc. /t REG_DWORD /v \"Allow Uninstall\" /d 1 /f \"C:\\\\Program Files\\\\Trend Micro\\\\OfficeScan Client\\\\NTRmv.exe\" del \"C:\\\\Program Files\\\\Trend Micro\\\\\" /Q', 'Our update probably wont happen for weeks unfortunately.', \"I'll record it all for gonewild.\", 'I saw so many nice cars in Texas that looked like they never had a wash since they were purchased. It made me feel sad.', \"Considering it was made at 5pm EST, he's either early for class, or staying late to fuck the professor, depending on the timezone. Unless this is a night class.\", 'It made the filesize smaller and was the default export option out of the stitching program I used. I tried various effects to bring the 13 photos together but it was my first time using hugin.', \"I meant for the full quality export with no compression out of the application I was using. The default export was png down to 120mb and the jpg was around 166 iirc. I'm glad you were able to get it down to 17mb for your use. I simply left it as a preview for those that like water pics, and full original quality for others to do whatever they wanted with, like you did.\", \"I'm talking about a 70k beamer that was 3 years old and had mud on the sides from some rainstorm that probably happened years ago. It made Ohio cars look clean in comparison.\", 'Its ok. I love texas. Just visited a few days ago. Did the original rudys, henrys puffy tacos, mi tierra cafe, the salt lick, schlitterbahn, six flags fiesta texas, and...of course...buc-ees in new braufelds. It was a great vacation. Also, being from pittsburgh, texas has loads of steelers sports bars :)', 'Their pork ribs were heaven, but I actually like rudys brisket better.', \"I'm in the US, and have been listening to streaming TripleJ for the past ~15 or so years. It's awesome.\", \"I still can't find my sing-a-long cassette. But I do have two ren and stimpy jello molds :)\", 'Alot of people still sell them on ebay. I think they were a promo from Jello....something like \"buy 8 packages of jello and get these for free + $5 shipping\" and me and my dad needed them badly.', 'I kinda did too. It was okay occasionally, but I had other things I would rather watch.', 'Plus?', \"Indeed, its symbol is Fl. I'm sure someone will see that graphic in the article and make that mistake.\", \"Also, the recipe book it comes with has some pretty legit recipes. I think i tried about 20 butternut squash recipes before I found the one in their book. However, I still recommend a small black and decker food chopper for small-duty stuff. We've had ours for many, many years.\", 'We are the menorrhagia', \"It is pretty weak and just really hot. Not alot of special there. I actually don't mind Seattle's Best at Subway when they make a fresh pot. Probably the strongest, non-shit coffee out of the fast food joints.\", 'When you mix hills and snow, and you drive anything with all-wheel drive, you can actually get up those hills now.', 'You accidentally a word.', \"My guess is that for some home users on IE6/IE7, windows update got hosed, and that IE8 download can't be applied, and they don't want to pay money for someone to fix it.\", \"Their RSS feed is strangely for all valve blogs. Fortunately for you, I made a direct RSS for that blog when I got annoyed. EDIT: See official feed link below. I'll still keep this yahoo pipe operational though, since I use their service alot.\", \"I'm guessing the same thing happened to him that happened to my dad...he started puking up blood, and then he started shitting it too. My dad apparently lost close to half of the blood in his body while on too many medications after his 3rd open heart surgery. He got the leak cauterized, and then got nerve damage in his left arm when the nurse told him to never move his arm from the hospital bed rail (because of all of the needles in it). Ulnar nerve damn near severed. The morphine helped him never realize that he would lose 90% use of his left arm.\", \"Smoking of any kind or any of the other shit they put in their mouth to get nicotine. If I'm gonna kiss a girl, I don't need to be worrying about the chew she needs to spit first. fuck. that. shit.\", \"I love you. Thanks. I've grown too accustomed to using yahoo pipes or feed43 for making feeds that have what I want....so I completely forgot about that :)\", \"My KFC has a bowl of side salad and hot bread on it's buffet bar.\", 'My body can take it.', \"> Why would they take on an expensive, ambitious and risky project like that when they can just keep churning out the same Pokemon portable games with small tweaks to each generation and know they'll rake in a huge profit? Because they know it can't last forever, and they could get into the console market and churn out some more games there. > The core audience for Pokemon is little kids. The target audience is little kids. The core audience is anyone that played the original games and enjoyed them, and would be willing to purchase a game like this. > Their parents might be leery of an online component. No arguments there. I'm sure nintendo could find a way to handle that, though. > Kids might be turned off by the complexity of what you describe. Nope. This game would be hyped so much, and would start out with the same simple playstyle that we've grown to love. There would be no chance of this. > Additionally, a lot of more mature gamers who would enjoy this style of game are turned off by Pokemon brand because of its childish nature. They'd be taking a big risk on attracting a relatively niche audience compared to who they're selling to already. Try again. Nostalgia and sheer popularity can win any day. Who they've sold to in the past and who they're selling to currently are very different groups, but all would love this game.\", \"You'd have to trade with someone in a snowy region for a jynx :) Anyways, Ice Punch is a splendid move.\", \"Is B2/W2 going to be for 3DS only? I actually don't know which of the newer games I can play....I just got a used DS Lite for $20 from someone.\", 'LAVA COOKIE', \"I agree, but I'd like to see a tough balance between 2d and 3d. I want both to be used. Fuck, they could cell shade it for all I care. Just as long as the game is made.\", \"they did say something about dropping flash support eventually, so I honestly wouldn't be surprised if it was dead for 5.0. couldn't find sauce tho.\", \"I'm pretty sure that Pittsburgh is at that five point meeting of states.\", 'They had those at the Texas mariott that i just went to.', \"They also have Buc-ees. I can't even begin to explain Buc-ees with how much time I've got right now.\", \"Don't tell Pitt!!\", 'Yup', 'What reminds you of Death Note in that picture?', 'Use keyboard remapping software to remap it to something you might like. {URL} I personally use KeyTweak for my netbook\\'s \"context menu\" key that is bigger than the right ctrl key.', \"> pittsburgher > currently wearing pens jersey yea.....i'll stay over here for this one ;)\", 'Okay....I can see that :)', \"As long as there is an opportunity for good nicknames, it doesn't matter.\", 'maximum amount of jimmies rustled', \"Oh don't start. I'm still disappointed that Hungry Hungry Hippos wasn't a real movie.\", 'Seriously, most people have shit, or the same shit as me. This collection really does rock. Is there any chance that you could upload your other wallpaper albums? So far your judgement is awesome when it comes to awesomeness.', 'Get some Walmart-brand pineapple soda (better than faygo) and mix it half-n-half with coconut rum. Call me in the morning.', 'aka In n Out burger', 'I worked selling shoes for 2 years on a weekend-only basis during school at a national chain. Never offered a raise, never offered to open the store, never given any recognition. When I asked for some more responsibilities, I was told I was unimportant, as two new outside managers were coming in. One managed a section of Petsmart, and the other had no prior experience. I put in my two weeks notice. Both people came in on my last day, so I showed them everything I learned and all the small quirks of the inventory that we had. Both quit within a week, and the store closed within 4 months.', 'Eh the new kid never did anything to me. I had another job lined up.', 'He had a manager that only worked Tuesday thursdays. This guy was just a cockhole.', 'iirc, that is the same pattern that the paper wrapper usually gets on the end of the straw.', 'apparently you forgot about this subreddit.', 'Should get a job at Buc-ees.', 'there is no window in the window shot though :(', 'Yea but I like pretty windows since I build things :(', 'Did he have two rhyhorns that know earthquake?', 'It is. Throw some crumbled ice in there and put cherry syrup and a cherry ontop to make it fancy. Its important to use the Walmart kind though.', 'Rain is his weakness.', 'I may be off by a few, but I counted 1836. Copied source from beginning of list to end of list, ignoring citations. deleted all links to pictures by searching for editsection and deleting up until the next <ul> i would find. I then counted the number of title=\" in the source and got 1860. Then I manually ran through the list checking for multiple links per manufacturer and counted 25, and 1 that had no links, so that is 1860-24=1836 1836 was the year of the Battle of the Alamo, by the way.', \"I hope you're not counting the citation links,as those don't have the title field I was searching for. Otherwise I have no clue what you're referencing\", 'YAY!', 'Office Communicator would have their panties in a bunch.', 'Thank god SOMEONE mentioned Reader: 1. Clicking the Reader widget causes it to hang unless I press the home icon, if the widget is supposed to list all feeds. 2. The changeover to google+ functionality still left some unselectable \"mark as unread\" options. 3. The app still doesn\\'t handle some image content properly. 4. The app still doesn\\'t fit some feeds to be completely viewable on the reader screen. It could use a full overhaul for ICS. It gets ZERO love.', \"I want to say something quite vulgar to you, but can't find the right words.\", 'Both of them are quite awesome. Boston Legal was amazing.', \"Sadly enough, most of the Denny's in my area (Pittsburgh) closed down because of competition from King's and Eat'N Park. I stopped going after they got rid of the Breakfast Dagwood.\", 'I used to use Winamp and Sonique. How sad that corporate buyouts can kill software like that.', 'I used to do this: 5 stars - Songs good for anyone, for a large party 4 stars - Songs my friends like, but others may not 3 stars - Songs only I like 2 stars - Every other song from the album that is meh * 1 star - songs I never want played, but keep just to have the full album stored Then I can just set \"songs 3 and above\" for my car, or \"2 and above\" when I\\'m driving a long distance and want a large mix, or \"1 star only\" when someone is in the car that I want to piss off.', 'Its actually a media file functionality I think is on every OS.', 'Ah. Yea not alot support it.', \"Although this is a known problem with this Mac app, I've noticed windows Hearts not calculating out a proper win percentage when I played 100 games.\", 'rubbernet', 'Thank you for the awesome response. Would simply rounding up fix the issue, or would there potentially be the problem of 101% ever occurring?', 'someone just posted it above: {URL}', \"If he's saying it works the same way as GoLauncher, then it might actually work. Someone above posted a solution that lets you remove them with GoLauncher.\", \"Thank you kind sir. Now I've gotta find a way to request a patch :D\", 'In our high school, we had a large tub/pump for our ketchup that was approximately 5 gallons, with a lid that just set on top...no screwing required. We proceeded to dump liquid laxative into it and mix it quickly before lunch began. We had 4 sets of bathrooms, two of which were out of order (the set closest to the lunch room, and the set furthest from the lunch room). The other two were the gym locker room bathrooms which were tiny, and a second locker room set that the outside sports teams used. People were spewing shit all over the gym floor trying to make it to any of them.', 'Definitely a scammer font.', 'OP will surely deliver!', 'Oh I completely agree. I had nothing to do with it, and simply found it funny because I never get ketchup so I was one of the few people that was unaffected. The kids were arrested and were sued by a few people, but it was the first real senior prank in years, the last being some lame attempt at ripping out trees on school property, and before that the ol\\' \"1,2,4,5,6\" greased pig prank.', 'I talked about Peanut Butter and Jelly sandwiches for hours apparently. Even while the teeth were being taken out.', \"I finally started getting better at MDT and can now say that I'm a pretty damn good amateur expert in that shit. Granted, I'm no expert expert, but I know how to deploy my monkeys properly now..\", 'transbeastophile', \"Is it really hipster to use fixed gear bikes? I wanna say that shitloads of people in Pittsburgh use those and aren't hipsters at all.\", 'I sold my Verizon global Blackberry 99xx on craigslist for $160 only 2 months ago....a mother wanted her daughter to have a verizon phone in Spain because she didn\\'t trust \"those scary phone companies over there\". I tried to give her the international adapters for free with it, and she said \"we already have chargers\". Whatever.', \"In retrospect, she had nothing in her hands, but I just think that she didn't wanna carry anything. She put the phone in her pocket, paid in cash, and left.\", 'this one is 1600x940, but +1 for no people in shot previously on reddit', 'They will never work on the road?', 'No. Any U or P series has a boxy frame back then, so this is the U2412s.', \"I'll be the dick that nitpicks :) 1. Plastic overtop of Dell on back of monitors = anarchy. 2. Might as well buy the $140 dock and get nearly the same performance and save $700 by not buying the desktop. 3. Spend the extra $3 next time to get the better keyboard with the wrist rest. Your employees will thank you, and can remove the wrist rest if they hate it. Ergonomics are a huge fucking deal nowadays. 4. Should probably get a AX510 speaker bar too :) Overall your setup is great though. Ours is similar, although we only get P-series monitors.\", \"It's almost policy in my job that if you deserve dual widescreens for anything, you deserve a laptop/docking station/speaker bar/better keyboard and mouse/gel mousepad since you'll be working your ass off.\", 'well, OP answered it.', 'You must have not been here for disco ball.']\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of strings to a single string\n",
    "train_text_data = []\n",
    "for train in train_data:\n",
    "    train_text_data += [normalize_text(train)]\n",
    "\n",
    "valid_text_data = []\n",
    "for valid in valid_data:\n",
    "    valid_text_data += [normalize_text(valid)]\n",
    "\n",
    "# Save the text data to a file\n",
    "with open(f\"{data_dir}/training.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(train_text_data)\n",
    "\n",
    "# Save the text data to a file\n",
    "with open(f\"{data_dir}/validation.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(valid_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:27:28.372475900Z",
     "start_time": "2023-12-11T18:26:31.452424500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/2025 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2025, training_loss=3.973758179464458, metrics={'train_runtime': 54.6245, 'train_samples_per_second': 74.142, 'train_steps_per_second': 37.071, 'total_flos': 132281479987200.0, 'train_loss': 3.973758179464458, 'epoch': 5.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model == distgpt2_model)\n",
    "\n",
    "# Load and preprocess the training data\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{data_dir}/training.txt\",\n",
    "    block_size=128  # adjust block_size as needed\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation data\n",
    "valid_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{data_dir}/validation.txt\",\n",
    "    block_size=128  # adjust block_size as needed\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # we're not using masked language modeling here\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{data_dir}/gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # adjust as needed\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,  # adjust as needed\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the GPT-2 model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:48:29.287124100Z",
     "start_time": "2023-12-11T18:44:11.849369400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:17<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "        if input_ids.shape[1] == 0:\n",
    "            continue\n",
    "        # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 1024:\n",
    "            input_ids = input_ids[:, -1024:]\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "        top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "        # filter to only words with a probability greater than p%\n",
    "        removal_marked = []\n",
    "        for j in range(len(top_k_probs_list)):\n",
    "            if top_k_probs_list[j] < top_p:\n",
    "                removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "        for to_remove in removal_marked:\n",
    "            top_k_probs_list.remove(to_remove[0])\n",
    "            top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "        guesses = set()\n",
    "        for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "            token = distgpt2_tokenizer.decode([token_id])\n",
    "            guesses.add(token)\n",
    "\n",
    "        # we consider a prediction correct if the actual word was\n",
    "        # one of the (up to) 5 words suggested\n",
    "        if ' ' + actual_next_word in guesses:\n",
    "            distilgpt2_correct_predictions += 1\n",
    "        distilgpt2_total_predictions += 1\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:48:29.288125500Z",
     "start_time": "2023-12-11T18:48:29.271124800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3338699739629917\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print(gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vocab updated??\n",
    "https://github.com/huggingface/tokenizers/issues/1160"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
