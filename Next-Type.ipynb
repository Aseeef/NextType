{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Next Type - A Mobile Typing Assistant\n",
    "[CS 505 - NLP] [Final Project]\n",
    "Completed by Muhammad Aseef Imran\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Compared to typing on a keyboard, typing on our phones considerably slower. Luckily most phones come built in with a feature to predict your next word to make up for this. However, after much experimentation, it seems most of these prediction algorithms seem to use simple N-grams with a window of around 3-4 words of left context. Many times, predicting the next word based on simple N-gram based probabilities work \"fine\" but often, this strategy produces poor results due to ignoring the context of the sentence. As NLP technology leaps forward exponentially we can do much better than simplistic N-grams. For this reason **Next Type** aims to bring the next generation of typing experiance to its users in order to raise productivity and typing speed for users leaving more time for the important things in life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"assets/bad-phone-example-1.jpg\" alt=\"N-gram example 1\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-2.jpg\" alt=\"N-gram example 2\" style=\"width: 33%; padding-right: 10px;\">\n",
    "    <img src=\"assets/bad-phone-example-3.jpg\" alt=\"N-gram example 3\" style=\"width: 33%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Presented above are three examples where unfortunately, the standard N-gram model employed on \"modern\" devices fails miserably.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Goals\n",
    "\n",
    "N-gram models perform poorly with shorter context window, and any large context windows require an immense amount of data to produce reasonable results. Moreover, at least in this case, our N-gram models do not consider the right context when suggesting words. However, despite having their weaknesses, the N-gram approach also has some nice benefits. Particularly, the nature of the N-gram model allows it to be easily updated with new data and adapt to the typing habits of its users with little processing power.\n",
    "\n",
    "Keeping these things in mind, we can when we set out on this project our (rather ambitious) goals for the model were:\n",
    "1. The model should consider the left (and ideally also the right context) before suggesting a \"natural\" word that fits the context.\n",
    "2. The model should be able to adapt to or learn from the user's word choices in various contexts.\n",
    "3. The model should be able to learn the user's word choice as—outlined above—quick enough to be useful.\n",
    "4. The model should be reasonably sized allowing it to be run and be updated on most modern-phone hardware with in reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definition a \"Successful\" Prediction\n",
    "\n",
    "We define a successful prediction as a prediction that meets the following criteria:\n",
    "```The predicted next word is one of the top 5 predictions```\n",
    "\n",
    "Also considered was to define a successful prediction if the predicted word as a similar meaning to the actual word. However, given the constraints this turned out to be a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Questions & Challenges\n",
    "\n",
    "As we set out on this prototype application, I outlined a few guiding key questions for myself:\n",
    "\n",
    "1. People evolve and change. So can I get a better accuracy by consider ALL known history? Or only \"recent\" history? Should the more recent history be weighted more\"? If so, should history be judged more by time or the volume typed? If I'm wrong about this, then more data = better. OR the time people change in is just longer.\n",
    "\n",
    "2. My problem is my data is ever evolving. So how do I prevent the model from overfitting? Overfitting will make newer data harder to generalize. (i.e. How do I prevent it from forgetting the stuff it knew in the base model?). Also suppose if I don't want my model to consider the \"old\" history, how can I make the model \"forgot\" the old stuff (some kind of vanishing gradient is my best choice)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Obtaining the Fine-Tuning Data\n",
    "\n",
    "It is important for my task that the fine-tunning data only come from a single person since my goal is to train the model to adapt to the typing behavior of a specific user. In order to achieve this we can use scraped messages from reddit for specific users and examine how the model adapts to their specific word-choice and typing habits. Particularly, we will be using data from a pre-scrapped dataset [Reddit comments/submissions 2005-06](https://academictorrents.com/details/89d24ff9d5fbc1efcdaf9d7689d72b7548f699fc). Further, we also want to make sure that sample data from any user we use:\n",
    "* Provides a reasonably large enough dataset to train on\n",
    "* Posts regularly (as opposed to posting a lot occasionally)\n",
    "\n",
    "With that defined, our focus will be between and including messages posted between [1/2011 - 6/2012]. Why this particular time range? No particular reason besides that data with in this time range was reasonably enough sized to be processed quickly yet still leave us with enough data to work with.\n",
    "\n",
    "We can then process the reddit dump creating a dictionary consisting of reddit users and the messages they sent. We further filter this data as follows:\n",
    "* \"Deleted\" users aren't included\n",
    "* First, we remove all authors with less than 546 (i.e. they must average more than 1 post a day) - although future filteration steps would've already ensured this requirement, we start of with this since this is a quick and dirty elimination step allowing for quicker processing in the next steps (which are a bit more complicated implementation wise)\n",
    "* Second, we filter to only authors that made at least 18 post per month in the time range without missing any month.\n",
    "* Third, we filter to only authors that made at least 2 post every week without missing a week.\n",
    "* Fourth, we filter down the remaining users to those that posted on at least 80% of the days.\n",
    "\n",
    "Additional post-processing:\n",
    "* In our final step, we post process the post messages by running it through a sequence-to-sequence model already developed by someone correcting silly grammatical errors. Otherwise, we may end up having messages in our data set that contain non-existent vocab. In many cases, this post-processing fundamentally changed the sentence. However, I considered this a necessary evil because otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Establishing a Baseline\n",
    "\n",
    "As previously mentioned, we want to develop a model the beats the naive n-gram models (with a window of 3) used by most keyboard apps. Therefore, we will define our baseline as 3-gram model trained on the entire reddit corpus. A larger window size will not be feasible do to the massive amounts of memory required to hold such a model. This model uses simple probabilities based on the frequency of the n-gram in the corpus to make predictions.\n",
    "\n",
    "However, compared to n-gram models used by most keyboard apps, we will be making several simplification assumptions for our baseline:\n",
    "1. Our baseline model will not \"update\" n-gram model by using data from its user. On real keyboard apps, the model updates based on the users behavior (much like how we will update our models too later in this project). However, just for the purpose of having a working \"baseline\" and in the interest of time, this is a detail we neglect.\n",
    "2. Our model will also predict punctuations. Most keyboard apps only predict words however to get a fairer comparison to the models we will me employing (which do predict) punctuations, our version predicts punctuations (or tries to anyways).\n",
    "3. As previously implied, our n-gram model will be constructed solely of reddit. Specifically, the n-gram model is \"trained\" on all messages typed on reddit between [1/2011 - 6/2012].\n",
    "\n",
    "#### Baseline Training Details\n",
    "\n",
    "We will be training a 3-gram model and a 2-gram model. Our model will first attempt to use the 3-gram to make a prediction. If the 3-gram model does not have a machine n-gram, we will then fall back to 2-grams.\n",
    "\n",
    "The 3-gram model is trained as follows:\n",
    "1. For each month in the range [1/2011 - 6/2012], gather all posts and for each posts construct a 3-gram.\n",
    "    1. Add the 3-grams to a counter.\n",
    "    2. If by the time we finish processing the current month, if n-gram has not occurred at least 2 times delete it.\n",
    "2. Now for the entire n-gram dictionary, delete all n-gram that occurred less than 16 times in the entire corpus.\n",
    "\n",
    "Similarly, for the 2-gram model:\n",
    "1. For each month in the range [1/2011 - 6/2012], gather all posts and for each posts construct a 2-gram.\n",
    "    1. Add the 2-grams to a counter.\n",
    "    2. If by the time we finish processing the current month, if n-gram has not occurred at least 5 times delete it.\n",
    "2. Now for the entire n-gram dictionary, delete all n-gram that occurred less than 25 times in the entire corpus.\n",
    "\n",
    "Exact training script used can be found in the [create_raw_reddit_ngrams.py](./create_raw_reddit_ngrams.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation Stategy\n",
    "\n",
    "In light of our above outlined goals, we have two major evaluation goals:\n",
    "\n",
    "\n",
    "1. How well does the model predict the user's next token after having seen x tokens of examples from the user. In other words, not only how well the model predicts the user's next token but also how fast the model improves its prediction as a function of the data it has already seen?\n",
    "\n",
    "> We can evaluate \"how well\" the model predicts the user's next token by measuring the loss between what the user actually types vs what the model suggests. Then, we can further measure this loss as function of the number of tokens of examples the model has seen during its Fine-Tuning. For example, how does the loss change after the model has seen 1000 tokens of examples from the user?\n",
    "\n",
    "2. How much computation is needed is to both run the model and update the model on new data?\n",
    "\n",
    "> Measuring how long various parts of the model such prediction and training takes is trivial. (We can simply calculate the time between the target area of code). We may then analyze the run-time in context of the hardware the code is run on and comparing this information with current state of computational power of modern mobile devices. This information can be used to make an informed decision on the sequence lengths to input to the model to ensure our model can suggest new words to users in real-time on standard mobile hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project Plan and Exploring Potential Solutions\n",
    "\n",
    "Once again, our goal is to accuractely and effectively predict the user's next token in real-time while adapting to the user's behavior and writing style over time.\n",
    "\n",
    "In order to reasonably meet these goals, we will fine-tune one or more combination of existing models such as T5, Bert, and GPT2, and/or their \"Distilled\" counterparts. We will use the Hugging Face transformers library simply due to its vast popularity and easy of use. I intend to use the SCC for rapid prototyping and experimentation as I already have significant experiance using the SCC at this stage.\n",
    "\n",
    "Finally I should note that in my initial research, I have identified potential pitfalls with each of these models and their strengths and weaknesses for my task. However, further experimentation will be needed to make a final decision on which model (or model combinations) to use. Detailed experimentation with each model will be required to evaluate its pros/cons.\n",
    "\n",
    "#### Bert\n",
    "Having been trained on a mask-fill task, Bert naturally lends itself to the kind of project I am trying to do. Being a relatively small model, and still quite versatile for the task, Bert may be a great choice. However, one downside to Bert is that Bert seems to perform poorly when attempting to Mask-Fill multiple words in the middle of a sentence. (See the bottom of this notebook for a demonstration).\n",
    "\n",
    "#### GPT2\n",
    "GPT2 was essentially trained on predicting the next word. Indeed, this is the task we want to achieve ourselves. However, in some cases, we may need to predict the middle word (if someone is editing the middle of a sentence they wrote). This is not a task GPT2 was designed for although this may still be possible due to the surprising generality of the model. Further research and experimentation will be needed.\n",
    "\n",
    "#### T5\n",
    "T5 is an extremely general purpose model than can adapt to many NLP tasks. Unlike bert Being a substantially larger model than both GPT2 and Bert, T5 is slower to retrain. Yet at the same time, T5 seems to do a much better job mask-filling between sentences. Yet, in a realistic scenario how often does one write in the middle of the sentence? Is the increased computing cost really worth it? These are the questions I hope to answer with the first stages of my research.\n",
    "\n",
    "#### The \"Distilled Version\"\n",
    "Models like Distilled-Bert and Distilled-GPT2 are indeed smaller. However, one *major* pitfall to this may be that the model may struggle to generalize to new tasks and during retraining. Retraining forms an essential component to this project and depending on the severity of this effected, the \"Distilled\" models may prove ineffective. Further experimentation is needed to make any conclusions, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Limitations\n",
    "* Compared to the N-gram approach, this new model cannot easily learn new words?\n",
    "* You may talk different with friends vs family vs boss. This training and results was done specifically for reddit. It may be the case that the model will not generalize as well to a broader domain in an actual key board app. (Still probability at least better than the ngram stuff right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparison and Exploratory Between Various Models\n",
    "\n",
    "We begin with an exploratory between different promising transformer models that have been historically very successful with a variety of tasks. Namely, we will do a comparisons between Bert, GPT2, and T5 on various tasks.\n",
    "\n",
    "1. We compare between the following 5 sentences to see how well the models do. These results are just to get a basic idea of each model's capabilities. We will \"eyeball this result\". Note that the \"|\" token represents the current \"cursor\" location.\n",
    "\n",
    "    a. `After forcefully breaking into the bank, they|`\n",
    "\n",
    "    b. `Every |, my family and I visit Hawaii.`\n",
    "\n",
    "    c. `In my family, there is my |`\n",
    "\n",
    "    d. `My favorite | is apple.`\n",
    "\n",
    "    e. `It has been 2 months since I graduated. However, unfortunately I still haven't found a |. At this rate I won't be able to pay rent!`\n",
    "\n",
    "2. After that, we will test each model to see how well the model does in predicting this \"next\" word token. This task is the most important task for our proposed application to do well.\n",
    "\n",
    "3. Next, we will see how well each model does at predicting a randomly removed \"middle\" token.\n",
    "\n",
    "4. Then, we do a similar challenge by now comparing how each model does at predicting when multiple \"middle\" tokens have been deleted.\n",
    "\n",
    "5. Finally, we will choose the most promising model to develop a fine-tuning method for this continuous learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:26:50.902307400Z",
     "start_time": "2023-12-23T00:26:43.632208200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs505ws/aseef/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports here\n",
    "from typing import Union\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List, Set, Any, Union\n",
    "import statistics\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, PreTrainedTokenizerBase, PreTrainedModel, GPT2Config\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, DistilBertConfig, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from datasets import Dataset\n",
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:26:50.902307400Z",
     "start_time": "2023-12-23T00:26:50.870768Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"After forcefully breaking into the bank, they |\", # predict the next word using context\n",
    "    \"In my family, there is my |\", # predict the next word using context\n",
    "    \"Every |, my family and I visit Hawaii.\",  # simple mask fill with one missing word\n",
    "    \"My favorite | is apple.\", # simple mask fill with one missing word\n",
    "    \"It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\", # need multiple words here before sentence makes sense\n",
    "    \"Following the American Civil War, | assassinated.\",  # need multiple words here before sentence makes sense\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:26:50.902307400Z",
     "start_time": "2023-12-23T00:26:50.870768Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"/projectnb/cs505ws/projects/NextType/data\"\n",
    "\n",
    "def does_var_exists_gz(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'{data_dir}/{var_name}.pkl.gz')\n",
    "\n",
    "def dump_var_gz(var_name: str, obj) -> None:\n",
    "    os.makedirs(f\"{data_dir}\", exist_ok=True)\n",
    "    with gzip.open(F'{data_dir}/{var_name}.pkl.gz', 'wb', compresslevel=1) as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_var_gz(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists_gz(var_name):\n",
    "        return None\n",
    "\n",
    "    file_path = F'{data_dir}/{var_name}.pkl.gz'  # Updated file extension\n",
    "    with gzip.open(file_path, 'rb', compresslevel=1) as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:26:58.309854Z",
     "start_time": "2023-12-23T00:26:50.876767700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load n_gram models\n",
    "reddit_2_grams_pmf: Dict[Tuple[str, str], Dict[str, float]] = load_var_gz(\"reddit_2_grams_pmf\")\n",
    "reddit_3_grams_pmf: Dict[Tuple[str, str], Dict[str, float]] = load_var_gz(\"reddit_3_grams_pmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.481125100Z",
     "start_time": "2023-12-23T00:26:58.312878800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load author to lines data\n",
    "author_to_posts_dict: Dict[str, Tuple[int, str]] = load_var_gz(\"author_to_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.486160600Z",
     "start_time": "2023-12-23T00:27:26.482125Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kiaha',\n",
       " 'mtheory007',\n",
       " 'chroncile',\n",
       " 'chileangod',\n",
       " 'ghostchamber',\n",
       " 'leif777',\n",
       " 'JudoTrip',\n",
       " 'japaneseknotweed',\n",
       " 'amorpheus',\n",
       " 'JamesLiptonIcedTea',\n",
       " 'ProjektTHOR',\n",
       " 'universl',\n",
       " 'AyeAyeCaptain',\n",
       " 'shinnen',\n",
       " 'genron1111',\n",
       " 'neodiogenes',\n",
       " 'Travis-Touchdown',\n",
       " 'Sykotik',\n",
       " 'Patrick5555',\n",
       " 'Skitrel',\n",
       " 'akatherder',\n",
       " 'Lizardizzle',\n",
       " 'InfinitePower',\n",
       " 'hjqusai',\n",
       " 'Jaketh']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_user_sample = random.sample(list(author_to_posts_dict.keys()), 25)\n",
    "random_user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.493129700Z",
     "start_time": "2023-12-23T00:27:26.491133Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample up to 50 posts from each user\n",
    "random_post_samples = []\n",
    "for rand_user in random_user_sample:\n",
    "    all_posts = author_to_posts_dict[rand_user]\n",
    "    sampled_posts = random.sample(list(all_posts), 50)\n",
    "    random_post_samples += sampled_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.560645Z",
     "start_time": "2023-12-23T00:27:26.554641900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device={device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.618729100Z",
     "start_time": "2023-12-23T00:27:26.560645Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import html\n",
    "def normalize_text(post_text: str):\n",
    "    # get rid of new lines\n",
    "    post_text = re.sub(\"\\n\", \" \", post_text)\n",
    "    # remove html characters\n",
    "    post_text = html.unescape(post_text)\n",
    "    # Remove bold and italic formatting\n",
    "    post_text = post_text.replace('*', \"\")\n",
    "    # Remove headers\n",
    "    post_text = re.sub(r'^#{1,6}\\s', '', post_text)\n",
    "    # Remove hyperlinks\n",
    "    post_text = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'\\1', post_text)\n",
    "    # Remove inline code\n",
    "    post_text = re.sub(r'`([^`]+)`', r'\\1', post_text)\n",
    "    # Remove block code\n",
    "    post_text = re.sub(r'```[^`]*```', '', post_text)\n",
    "    # Remove lists (unordered and ordered)\n",
    "    post_text = re.sub(r'^\\s*([\\*\\-\\+]\\s|(\\d+\\.)\\s)', '', post_text)\n",
    "    post_text = re.sub(\"(\\*\\*|__)(.*?)\\1|(\\*|_)(.*?)\\3\", \"\", post_text)\n",
    "    # remove double spaces\n",
    "    post_text = re.sub(\" {2,}\", \" \", post_text)\n",
    "    # replace urls\n",
    "    post_text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"{URL}\", post_text)\n",
    "    # replace references to a specific subreddit but just the token \"{SUB_REDDIT}\"\n",
    "    post_text = re.sub(r\"(\\W)(r/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB_REDDIT}\\3\", post_text)\n",
    "    post_text = re.sub(r\"(\\W)(/[a-z0-9A-Z_]{2,10})(\\W)\", r\"\\1{SUB_REDDIT}\\3\", post_text)\n",
    "    post_text = post_text.strip()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:35:56.123109700Z",
     "start_time": "2023-12-21T17:35:56.062574400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ngram_prediction(current_sent) -> Union[None, str]:\n",
    "    def tokenize_post(pre_tokenized_post_data):\n",
    "        return [word for sent in nltk.sent_tokenize(pre_tokenized_post_data) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    current_sent = normalize_text(current_sent)\n",
    "    current_sent = current_sent.lower()\n",
    "    ngram_input = tokenize_post(current_sent)\n",
    "\n",
    "    if tuple(ngram_input[-2:]) in reddit_3_grams_pmf:\n",
    "        prediction = reddit_3_grams_pmf[tuple(ngram_input[-2:])]\n",
    "    elif tuple(ngram_input[-1:]) in reddit_2_grams_pmf:\n",
    "        prediction = reddit_2_grams_pmf[tuple(ngram_input[-1:])]\n",
    "    else:\n",
    "        prediction = None\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Baseline n-grams On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:32:44.615245400Z",
     "start_time": "2023-12-21T17:32:44.605229500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: {'are': 0.3346073933449349, \"'re\": 0.30994814463253, 'do': 0.12743342298159574, 'have': 0.12373499270509238, 'will': 0.104276046335847}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: {'favorite': 0.5853830605651704, 'first': 0.1888119327439376, 'favourite': 0.0983645108370054, 'new': 0.06445652599929452, 'opinion': 0.06298396985459208}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: {'time': 0.3896827529049508, 'day': 0.23419787074369317, 'single': 0.18737812391879125, 'other': 0.1147109820572633, 'year': 0.0740302703753015}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: {'.': 0.33868374032162, 'part': 0.25645473496128646, 'is': 0.17503722453841572, ',': 0.12518612269207863, 'thing': 0.10463817748659916}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: {'way': 0.4054198728671797, 'new': 0.1742165718746515, 'few': 0.1631091780974685, 'good': 0.14767480762796922, 'lot': 0.10957956953273112}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: {'and': 0.39923997633459246, 'but': 0.24832749283211214, 'the': 0.16813816957174715, 'i': 0.09372866700040959, 'it': 0.09056569426113867}\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    # out ngram model can only consider left context\n",
    "    input_text = sentence[:sentence.index(\"|\")].strip()\n",
    "    suggestions = get_ngram_prediction(input_text)\n",
    "\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Baseline n-grams on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:36:51.974502800Z",
     "start_time": "2023-12-21T17:36:22.130391200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:29<00:00, 41.95it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_correct_guesses = 0\n",
    "baseline_total_guesses = 0\n",
    "baseline_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "\n",
    "        predictions = get_ngram_prediction(current_prompt)\n",
    "        if predictions is not None and actual_next_word in predictions:\n",
    "            baseline_correct_guesses += 1\n",
    "        baseline_total_guesses += 1\n",
    "\n",
    "        baseline_inference_times += [time.time() - start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:37:05.216969400Z",
     "start_time": "2023-12-21T17:37:05.183749800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38662746585735963\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = baseline_correct_guesses / baseline_total_guesses\n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.022976395781948235\n",
      "Inference Std: 0.00535418975672332\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(baseline_inference_times))\n",
    "print('Inference Std:', statistics.stdev(baseline_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T18:40:47.422233Z",
     "start_time": "2023-12-21T18:40:46.030432200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the t5 model\n",
    "T5_path = 't5-small'\n",
    "t5_config = T5Config.from_pretrained(T5_path)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_path, legacy=False)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_path, config=t5_config).to(device)\n",
    "t5_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T18:45:53.425747900Z",
     "start_time": "2023-12-21T18:45:53.267704800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8238, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(1733, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(33, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(130, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(56, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('<pad> <extra_id_0> broke', 0.22146426141262054), ('<pad> <extra_id_0> break', 0.21711385250091553), ('<pad> <extra_id_0> are', 0.21013765037059784), ('<pad> <extra_id_0> were', 0.17779791355133057), ('<pad> <extra_id_0> will', 0.1734863519668579)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "tensor(384, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(293, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(3062, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(2039, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(234, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('<pad> <extra_id_0> family', 0.27247941493988037), ('<pad> <extra_id_0> own', 0.25976160168647766), ('<pad> <extra_id_0> daughter', 0.15796756744384766), ('<pad> <extra_id_0> mother', 0.1555204540491104), ('<pad> <extra_id_0> home', 0.1542709767818451)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "tensor(215, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(239, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(847, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(471, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(97, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('<pad> <extra_id_0> year', 0.3100126087665558), ('<pad> <extra_id_0> day', 0.2699778378009796), ('<pad> <extra_id_0> month', 0.1532614529132843), ('<pad> <extra_id_0> week', 0.14059576392173767), ('<pad> <extra_id_0> time', 0.12615235149860382)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "tensor(8947, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(2728, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(589, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(6253, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(2696, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('<pad> <extra_id_0> apple', 0.469388872385025), ('<pad> <extra_id_0> fruit', 0.16375479102134705), ('<pad> <extra_id_0> thing', 0.15048746764659882), ('<pad> <extra_id_0> pie', 0.11327831447124481), ('<pad> <extra_id_0> recipe', 0.10309050232172012)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "tensor(3170, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(1080, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(3599, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(3571, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(2667, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('<pad> <extra_id_0> rent', 0.24719730019569397), ('<pad> <extra_id_0> rate', 0.22261090576648712), ('<pad> <extra_id_0> fixed', 0.21800415217876434), ('<pad> <extra_id_0> rental', 0.16750206053256989), ('<pad> <extra_id_0> flat', 0.1446855515241623)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "tensor(8, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(3, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(797, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(186, device='cuda:0')\n",
      "torch.Size([512])\n",
      "tensor(46, device='cuda:0')\n",
      "torch.Size([512])\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('<pad> <extra_id_0> the', 0.314527302980423), ('<pad> <extra_id_0> ', 0.2124253213405609), ('<pad> <extra_id_0> American', 0.2062595784664154), ('<pad> <extra_id_0> many', 0.13480444252490997), ('<pad> <extra_id_0> an', 0.13198336958885193)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"<extra_id_0>\")\n",
    "    input_ids = t5_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    num_samples = 5\n",
    "    with torch.no_grad():\n",
    "        output = t5_model.generate(\n",
    "            input_ids,\n",
    "            num_beams=num_samples,\n",
    "            min_new_tokens=2,\n",
    "            max_new_tokens=2,\n",
    "            num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(output.sequences, probabilities):\n",
    "        decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "        suggestions += [(decoded_output, prob.item())]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:04:55.703081Z",
     "start_time": "2023-12-21T19:04:55.612548200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16497]\n",
      "[21353]\n",
      "tensor(730.3172, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: DELETE ME\n",
    "\n",
    "w1 = 'lol'\n",
    "w2 = 'quantum'\n",
    "\n",
    "def remove_special_tokens(token_list: List):\n",
    "    to_remove = []\n",
    "    for elem in token_list:\n",
    "        if elem in {0, 1, 2, 3, 32099}:\n",
    "            to_remove += [elem]\n",
    "    for elem in to_remove:\n",
    "        token_list.remove(elem)\n",
    "    return to_remove\n",
    "\n",
    "#print(t5_model.shared(sample_output[2]))\n",
    "w1_e = t5_tokenizer.encode(w1, return_tensors=\"pt\").to(device)[0].cpu().numpy().tolist()\n",
    "w2_e = t5_tokenizer.encode(w2, return_tensors=\"pt\").to(device)[0].cpu().numpy().tolist()\n",
    "remove_special_tokens(w1_e)\n",
    "remove_special_tokens(w2_e)\n",
    "print(w1_e)\n",
    "print(w2_e)\n",
    "\n",
    "w1_vec = t5_model.shared(torch.tensor(w1_e, device=device))\n",
    "w2_vec = t5_model.shared(torch.tensor(w2_e, device=device))\n",
    "print(torch.norm(w1_vec - w2_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:07:25.738568700Z",
     "start_time": "2023-12-21T19:06:49.107531700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▏                                                                                                                                                                    | 24/1250 [00:10<09:21,  2.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "  2%|████                                                                                                                                                                    | 30/1250 [00:36<24:31,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# generate one word at a time\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m---> 24\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mt5_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Generate multiple suggestions\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output\u001b[38;5;241m.\u001b[39msequences_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Decode and print the predicted token\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/generation/utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3089\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3091\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3099\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    704\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    592\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    599\u001b[0m ):\n\u001b[1;32m    600\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 601\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    611\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:543\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    541\u001b[0m         position_bias\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# if key and values are already calculated\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# we want only the last query position bias\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:443\u001b[0m, in \u001b[0;36mT5Attention.compute_bias\u001b[0;34m(self, query_length, key_length, device)\u001b[0m\n\u001b[1;32m    441\u001b[0m memory_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(key_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    442\u001b[0m relative_position \u001b[38;5;241m=\u001b[39m memory_position \u001b[38;5;241m-\u001b[39m context_position  \u001b[38;5;66;03m# shape (query_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m relative_position_bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_relative_position_bucket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape (query_length, key_length)\u001b[39;49;00m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_decoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_num_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_max_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention_bias(relative_position_bucket)  \u001b[38;5;66;03m# shape (query_length, key_length, num_heads)\u001b[39;00m\n\u001b[1;32m    450\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (1, num_heads, query_length, key_length)\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:416\u001b[0m, in \u001b[0;36mT5Attention._relative_position_bucket\u001b[0;34m(relative_position, bidirectional, num_buckets, max_distance)\u001b[0m\n\u001b[1;32m    414\u001b[0m     relative_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(relative_position)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     relative_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmin(relative_position, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# now relative_position is in the range [0, inf)\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# half of the buckets are for exact increments in positions\u001b[39;00m\n\u001b[1;32m    420\u001b[0m max_exact \u001b[38;5;241m=\u001b[39m num_buckets \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t5_correct_guesses = 0\n",
    "t5_total_guesses = 0\n",
    "t5_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" <extra_id_0>\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids = t5_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # t5 can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            num_samples = 7\n",
    "            output = t5_model.generate(\n",
    "                input_ids,\n",
    "                num_beams=num_samples,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=2,\n",
    "                num_return_sequences=num_samples,  # Generate multiple suggestions\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output.sequences_scores, dim=-1)\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for sample_output, prob in zip(output.sequences, probabilities):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            decoded_output = t5_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            t5_correct_guesses += 1\n",
    "        t5_total_guesses += 1\n",
    "        t5_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:59:26.006531800Z",
     "start_time": "2023-12-21T17:59:26.001529100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37467754172989376\n"
     ]
    }
   ],
   "source": [
    "t5_accuracy = t5_correct_guesses / t5_total_guesses\n",
    "print(t5_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T17:59:26.198139Z",
     "start_time": "2023-12-21T17:59:26.007530800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.02229035642783812\n",
      "Inference Std: 0.0007565512458901443\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(t5_inference_times))\n",
    "print('Inference Std:', statistics.stdev(t5_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### T5-Small on \"Predict the Middle Token\"\n",
    "OK, predicting the next token is fun and all. But what if a user wants to add something to the middle of their sentence? Wouldn't it be nice to be able to both use left and right context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:15.422994300Z",
     "start_time": "2023-12-11T16:15:15.419996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "#### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T18:04:34.651566200Z",
     "start_time": "2023-12-11T18:04:33.756145900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distbert_path = 'distilbert-base-cased'\n",
    "distbert_model = DistilBertForMaskedLM.from_pretrained(distbert_path).to(device=device)\n",
    "distbert_config = DistilBertConfig.from_pretrained(distbert_path)\n",
    "distbert_tokenizer = DistilBertTokenizer.from_pretrained(distbert_path, config=distbert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert On the Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T16:15:16.485638800Z",
     "start_time": "2023-12-11T16:15:16.418844600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [('!', 7.7542314529418945), ('.', 7.455650806427002), ('escape', 6.470279693603516), (':', 6.367412567138672), ('find', 6.3458967208862305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [('heart', 6.815902233123779), ('destiny', 6.189598560333252), ('.', 6.164964199066162), ('love', 6.162736415863037), ('family', 6.145949363708496)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [('##day', 6.442167282104492), ('day', 5.875978946685791), ('morning', 5.826064586639404), ('##night', 5.271539688110352), ('night', 5.13131046295166)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [('fruit', 11.054349899291992), ('apple', 10.834712028503418), ('tree', 10.448899269104004), ('grape', 8.978232383728027), ('vegetable', 8.545177459716797)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [('decent', 9.64558219909668), ('reasonable', 8.736651420593262), ('fixed', 7.847301006317139), ('satisfactory', 7.8360819816589355), ('flat', 7.648664474487305)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [('many', 5.271701812744141), ('he', 4.914363384246826), ('soldiers', 3.7219643592834473), ('rebels', 3.6212308406829834), ('others', 3.600987672805786)]\n",
      "-=+=--=+=--=+=--=+=--=+=-\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "\n",
    "    input_text = sentence.replace(\"|\", \"[MASK]\")\n",
    "    input_ids = distbert_tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "    # Generate predictions for the next token\n",
    "    with torch.no_grad():\n",
    "        output = distbert_model(input_ids)\n",
    "        predictions = output.logits\n",
    "\n",
    "    # Get the top-k predicted tokens and their probabilities\n",
    "    top_k = 5  # Adjust as needed\n",
    "    probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "        suggestions += [(sample_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)\n",
    "    print(\"-=+=--=+=--=+=--=+=--=+=-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### DistilBert on \"Predict the Next Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:14.923844Z",
     "start_time": "2023-12-11T16:55:59.910583900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:14<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "distbert_correct_guesses = 0\n",
    "distbert_total_guesses = 0\n",
    "distbert_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]) + \" [MASK]\"\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        input_ids: torch.Tensor = distbert_tokenizer(current_prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "        # bert can only accept up to 512 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "        # Get the position of the masked token\n",
    "        mask_token_index = torch.where(input_ids == distbert_tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate one word at a time\n",
    "            output = distbert_model(input_ids)\n",
    "            predictions = output.logits\n",
    "\n",
    "        # Get the top-k predicted tokens and their probabilities\n",
    "        top_k = 8  # Adjust as needed\n",
    "        probs, indices = torch.topk(predictions[0, mask_token_index], k=top_k, dim=-1)\n",
    "\n",
    "        # Convert indices back to tokens\n",
    "        predicted_tokens = distbert_tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "        # Decode and print the predicted token\n",
    "        suggestions = set()\n",
    "        for decoded_output, prob in zip(predicted_tokens, probs.tolist()):\n",
    "            if len(suggestions) >= 5:\n",
    "                break\n",
    "            if prob < 0.04:\n",
    "                # avoid bizarre suggestions by simply filtering out low prob\n",
    "                # terms. We don't HAVE TO suggest exactly 5 words\n",
    "                break\n",
    "            # bert also suggests \"sub-words\". Ehh... we'll just ignore those.\n",
    "            # otherwise stuff will get too complicated\n",
    "            if decoded_output.startswith(\"##\"):\n",
    "                continue\n",
    "            if decoded_output.strip() == '':\n",
    "                continue\n",
    "            suggestions.add(decoded_output)\n",
    "\n",
    "        if actual_next_word in suggestions:\n",
    "            distbert_correct_guesses += 1\n",
    "        # since the way the bert tokenizer works, it can suggest \"sub-words\" - example: characteristically = characteristic + ##ally,\n",
    "        # so we will give bert half a point for suggest the same start of the word\n",
    "        for s in suggestions:\n",
    "            if actual_next_word.startswith(s):\n",
    "                distbert_correct_guesses += 0.5\n",
    "                break\n",
    "        distbert_total_guesses += 1\n",
    "        distbert_inference_times += [time.time() - start_time]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:20.001862900Z",
     "start_time": "2023-12-11T17:00:19.985839900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18298021889747967\n"
     ]
    }
   ],
   "source": [
    "bert_accuracy = distbert_correct_guesses / distbert_total_guesses\n",
    "print(bert_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T17:00:22.286787800Z",
     "start_time": "2023-12-11T17:00:22.234767400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.005107757104114663\n",
      "Inference Std: 0.001992808943326104\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distbert_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distbert_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Distil-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:17:59.963504100Z",
     "start_time": "2023-12-23T02:17:58.367356500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the GPT-2 model variant\n",
    "distgpt2_model_name = \"distilgpt2\"\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "distgpt2_model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(distgpt2_model_name)\n",
    "distgpt2_tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(distgpt2_model_name)\n",
    "# Move the model to the GPU (if available)\n",
    "distgpt2_model = distgpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:09:22.420843400Z",
     "start_time": "2023-12-21T19:09:21.863161400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: After forcefully breaking into the bank, they |\n",
      "Suggestions: [(' were', 0.6859785914421082), (' found', 0.10938050597906113), (' had', 0.08820205926895142), (' took', 0.059164125472307205), (' began', 0.05727475509047508)]\n",
      "Sentence: In my family, there is my |\n",
      "Suggestions: [(' family', 0.27839019894599915), (' brother', 0.2245674729347229), (' mother', 0.18426108360290527), (' sister', 0.1591966301202774), (' wife', 0.1535845696926117)]\n",
      "Sentence: Every |, my family and I visit Hawaii.\n",
      "Suggestions: [(' The', 0.40960025787353516), ('.', 0.16524524986743927), (' A', 0.15191836655139923), ('\\n', 0.13858996331691742), ('The', 0.13464611768722534)]\n",
      "Sentence: My favorite | is apple.\n",
      "Suggestions: [(' favorite', 0.44314318895339966), (' part', 0.18813173472881317), (' thing', 0.16900213062763214), (' of', 0.11418430507183075), ('.', 0.08553868532180786)]\n",
      "Sentence: It has been 2 months since I graduated. However, unfortunately I still haven't found a | rate I won't be able to pay rent!\n",
      "Suggestions: [(' way', 0.38864630460739136), (' job', 0.33211749792099), (' place', 0.1317850798368454), (' new', 0.09153558313846588), (' good', 0.055915530771017075)]\n",
      "Sentence: Following the American Civil War, | assassinated.\n",
      "Suggestions: [(' the', 0.7203230857849121), (' a', 0.11123217642307281), (' it', 0.07635104656219482), (' and', 0.05197402089834213), (' in', 0.04011968895792961)]\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    # delete everything including and after |\n",
    "    # GPT2 is physically unable to consider right context\n",
    "    input_text = sentence[:sentence.index(\"|\")].strip()\n",
    "    # Tokenize the prompt\n",
    "    input_ids = distgpt2_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    # Generate probabilities for the next words\n",
    "    with torch.no_grad():\n",
    "        outputs = distgpt2_model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "    # after all, we don't want to suggest too many words!\n",
    "    top_k = 5\n",
    "    # Get the probability distribution for the next word\n",
    "    next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "    top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "    # Normalize probabilities\n",
    "    top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "    # Convert the probabilities to a list\n",
    "    top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "    top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "    # Decode and print the predicted token\n",
    "    suggestions = []\n",
    "    for sample_output, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "        decoded_output = distgpt2_tokenizer.decode(sample_output)\n",
    "        suggestions += [(decoded_output, prob)]\n",
    "    print('Sentence:', sentence)\n",
    "    print('Suggestions:', suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:07:41.365317200Z",
     "start_time": "2023-12-21T19:07:40.619814600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/1250 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [84,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [141,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "  0%|                                                                                                                                                                                 | 0/1250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Generate probabilities for the next words\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdistgpt2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# we are only interested in either the top 5 words or words with a probability of > 1%\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# after all, we don't want to suggest too many words!\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/projectnb/cs505ws/projects/NextType/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "#distgpt2_tokenizer.pad_token = distgpt2_tokenizer.eos_token\n",
    "distgpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "\n",
    "    current_batch_prompt = []\n",
    "    current_batch_answer = []\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i]).strip()\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        current_batch_prompt += [current_prompt]\n",
    "        current_batch_answer += [actual_next_word]\n",
    "\n",
    "        assert len(current_batch_prompt) == len(current_batch_answer)\n",
    "        if len(current_batch_prompt) < 8:\n",
    "            continue\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.batch_encode_plus(current_batch_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024, truncation_strategy=\"longest_first\").input_ids.to(device)\n",
    "\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = distgpt2_model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()\n",
    "        top_k_indices_list: List = top_k_indices.tolist()\n",
    "\n",
    "        # for each element in the batch\n",
    "        for current_prompt, top_k_probs, top_k_indices, actual_next_word in zip(current_batch_prompt, top_k_probs_list, top_k_indices_list, current_batch_answer):\n",
    "\n",
    "            guesses = set()\n",
    "            for token_id, prob in zip(top_k_indices, top_k_probs):\n",
    "                # ignore bizarre suggestions\n",
    "                if prob < top_p:\n",
    "                    continue\n",
    "                token = distgpt2_tokenizer.decode([token_id])\n",
    "                guesses.add(token)\n",
    "\n",
    "            # we consider a prediction correct if the actual word was\n",
    "            # one of the (up to) 5 words suggested\n",
    "            if (' ' + actual_next_word) in guesses:\n",
    "                distilgpt2_correct_predictions += 1\n",
    "            distilgpt2_total_predictions += 1\n",
    "\n",
    "            print('prompt:', current_prompt)\n",
    "            print('suggestions:', guesses)\n",
    "            print('answer:', actual_next_word)\n",
    "\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n",
    "        # clear the batch\n",
    "        current_batch_prompt = []\n",
    "        current_batch_answer = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.059494800Z",
     "start_time": "2023-12-21T19:09:27.834935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████████████████████████████████▌                                                                                           | 566/1250 [01:41<02:03,  5.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [04:22<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions = 0\n",
    "distilgpt2_total_predictions = 0\n",
    "distilgpt2_inference_times = []\n",
    "\n",
    "for creation, post in tqdm(random_post_samples, smoothing=0):\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    for i in range(1, len(tokenized_words) - 1):\n",
    "        start_time = time.time()\n",
    "        current_prompt = ' '.join(tokenized_words[:i])\n",
    "        current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "        current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "        actual_next_word = tokenized_words[i]\n",
    "        # Tokenize the prompt\n",
    "        input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "        if input_ids.shape[1] == 0:\n",
    "            continue\n",
    "        # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "        # we trim it before passing into the model\n",
    "        if input_ids[0].shape[0] > 1024:\n",
    "            input_ids = input_ids[:, -1024:]\n",
    "        # Generate probabilities for the next words\n",
    "        with torch.no_grad():\n",
    "            outputs = distgpt2_model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "        # after all, we don't want to suggest too many words!\n",
    "        top_k = 5\n",
    "        top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "        # Normalize probabilities\n",
    "        top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "        # Convert the probabilities to a list\n",
    "        top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "        top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "        # filter to only words with a probability greater than p%\n",
    "        removal_marked = []\n",
    "        for j in range(len(top_k_probs_list)):\n",
    "            if top_k_probs_list[j] < top_p:\n",
    "                removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "        for to_remove in removal_marked:\n",
    "            top_k_probs_list.remove(to_remove[0])\n",
    "            top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "        guesses = set()\n",
    "        for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "            token = distgpt2_tokenizer.decode([token_id])\n",
    "            guesses.add(token)\n",
    "\n",
    "        # we consider a prediction correct if the actual word was\n",
    "        # one of the (up to) 5 words suggested\n",
    "        if ' ' + actual_next_word in guesses:\n",
    "            distilgpt2_correct_predictions += 1\n",
    "        distilgpt2_total_predictions += 1\n",
    "        distilgpt2_inference_times += [time.time() - start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.064494100Z",
     "start_time": "2023-12-21T19:13:50.062513200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36095087922396757\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print(gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T19:13:50.153521100Z",
     "start_time": "2023-12-21T19:13:50.064494100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Inference Time: 0.0052210369432941625\n",
      "Inference Std: 0.002328083751368582\n"
     ]
    }
   ],
   "source": [
    "print('Avg Inference Time:', statistics.mean(distilgpt2_inference_times))\n",
    "print('Inference Std:', statistics.stdev(distilgpt2_inference_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary of the results from Model Comparison\n",
    "\n",
    "With a 38.6% accuracy, on top of having the fastest inference speeds, none of the models beat the baseline n-gram model. I was surprised by this result. All 3 transformer models had much larger context windows to work of from and understand language much better than n-grams, yet none beat n-grams on the random sample used.\n",
    "\n",
    "In 2nd place was T5-small with an accuracy of 37.5%.\n",
    "In 3rd place was DistilGPT2 with an accuracy of 36.1%.\n",
    "And in 4th place was DistilBert with an accuracy of 18.2%.\n",
    "\n",
    "The fact T5 beat DistilGPT2 was another shock. Despite being trained on the \"predict the next word\" task and being a larger model in raw size than T5-small, DistilGPT2 still lost.\n",
    "\n",
    "Let's now look at the runtimes of all 3 models on both GPU and CPU\n",
    "\n",
    "* T5 was the slowest model\n",
    "GPU\n",
    "* 24:39 - T5-Small\n",
    "* 5:10 - DistilBert\n",
    "* 5:32 - DistilGPT2\n",
    "CPU\n",
    "* 3:19:43: T5-Small\n",
    "* 1:41:39 - DistilBert\n",
    "* 2:29:33 - DistilGPT2\n",
    "\n",
    "Based on these results, I have decided to **use GPT2 for further fine-tuning** to see if we can beat the baseline for the following reasons:\n",
    "* GPT2 is much faster than T5 on a GPU and considerably faster on a CPU. On an actual mobile device, using GPT2 allows me to reach a user-base with lower end CPUs as well. In other words, T5 is simply to slow to test and run!\n",
    "* Due to T5's dexterity, initial research suggests fine-tuning T5 will be a much more complicated task than fine-tuning GPT2 for a \"predict the next word task\". After all, \"predict the next word\" is not the primary thing T5 was trained for. As a result, not as many resources exist either on achieving this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Experimenting with Fine-Tuning GPT2\n",
    "In this section, we Fine-tune GPT2 using data from a randomly select reddit user and then examining how GPT2 performs on an evaluation set from that user. This section is simply an experiment to see how well GPT2 might be able to adapt to the writing styles from a specific user.\n",
    "\n",
    "This section differs from a real-word use case because in the real world, you will not have all the data from the user in once place to train on. Rather, in the real-word, we would continuously be updating an existing model as new data comes in. Continuous learning is instead something we explore in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:01:56.669436300Z",
     "start_time": "2023-12-23T02:01:56.589934100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The_Jackal'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select a user to train with\n",
    "#random_user = random.choice(list(author_to_posts_dict.keys()))\n",
    "random_user = 'The_Jackal'\n",
    "random_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T23:32:51.924636800Z",
     "start_time": "2023-12-22T23:32:50.665044500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this was the only small-ish grammar correction model I found.\n",
    "# had I more time, I would create my own model. But I don't so I will focus\n",
    "# on my primary task.\n",
    "# the downsides of this models is that besides correcting spellings, it often alters the\n",
    "# structure of the sentence which could fundamentally undermine our purpose.\n",
    "# so question: does benefits of correcting grammar using this outweighs the harms?\n",
    "# after all, if the model doesnt recgonize a word, it'll just ignore it and wont learn from it!\n",
    "grammar_corrector = pipeline(\n",
    "               'text2text-generation',\n",
    "               'pszemraj/grammar-synthesis-small',\n",
    "                 device=device\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/3794 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(user_posts_corrected))):\n\u001b[1;32m      3\u001b[0m     creation, message \u001b[38;5;241m=\u001b[39m user_posts_corrected[i]\n\u001b[0;32m----> 4\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m(message)\n\u001b[1;32m      5\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(message)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_text' is not defined"
     ]
    }
   ],
   "source": [
    "# user_posts_corrected = author_to_posts_dict[random_user]\n",
    "# for i in tqdm(range(len(user_posts_corrected))):\n",
    "#     creation, message = user_posts_corrected[i]\n",
    "#     message = normalize_text(message)\n",
    "#     sentences = sent_tokenize(message)\n",
    "#     for j in range(len(sentences)):\n",
    "#         sent = sentences[j]\n",
    "#         updated_message = grammar_corrector(sent)[0]['generated_text']\n",
    "#         sentences[j] = updated_message\n",
    "#     updated_message = ' '.join(sentences)\n",
    "#     user_posts_corrected[i] = (creation, updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dump_var_gz(f\"{random_user}-corrected-posts\", user_posts_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:27:26.634164400Z",
     "start_time": "2023-12-23T00:27:26.602642600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_posts_corrected = load_var_gz('The_Jackal-corrected-posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:20.453945300Z",
     "start_time": "2023-12-23T02:10:20.412616300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3787"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_dataset_list = user_posts_corrected\n",
    "#entire_dataset_list = author_to_posts_dict[random_user]\n",
    "# filter out posts with just a single word\n",
    "entire_dataset_list = [data for data in entire_dataset_list if len(data[1].split(\" \")) > 1]\n",
    "\n",
    "entire_dataset_list = [{\"post\": x[1], \"time\": x[0]} for x in entire_dataset_list]\n",
    "len(entire_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:22.333295600Z",
     "start_time": "2023-12-23T02:10:21.101527500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 89805\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "for data_entry in entire_dataset_list:\n",
    "    post = data_entry[\"post\"]\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    total_words += len(tokenized_words)\n",
    "print('Total words:', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:22.334294800Z",
     "start_time": "2023-12-23T02:10:22.243741200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the list based on the \"time\" key\n",
    "entire_dataset_list = sorted(entire_dataset_list, key=lambda x: x[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:22.489602600Z",
     "start_time": "2023-12-23T02:10:22.334294800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entire_dataset = Dataset.from_list(entire_dataset_list)\n",
    "split_sets = entire_dataset.train_test_split(train_size=0.85, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:22.882239600Z",
     "start_time": "2023-12-23T02:10:22.842709800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['post', 'time'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['post', 'time'],\n",
       "        num_rows: 569\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:23.484031500Z",
     "start_time": "2023-12-23T02:10:23.378176200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b12f63961224378a1e5aac339b143fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc37935cd1540ec843b1e44f87d23e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_sets.save_to_disk(f\"{data_dir}/dataset-{random_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:06:40.532891400Z",
     "start_time": "2023-12-23T03:06:37.771009900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 3218\n",
      "Validation dataset size: 569\n",
      "Example from training dataset: {'post': 'Or a right-wing multi-millionaire?', 'time': '1315765171'}\n",
      "Example from validation dataset: {'post': \"They didn't? . .\", 'time': '1310158819'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model_original = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "distgpt2_tokenizer: PreTrainedTokenizerBase = GPT2Tokenizer.from_pretrained(model_name)\n",
    "distgpt2_tokenizer.pad_token = distgpt2_tokenizer.eos_token\n",
    "\n",
    "trained_dataset = load_from_disk(f\"{data_dir}/dataset-{random_user}\")['train']\n",
    "validation_dataset = load_from_disk(f\"{data_dir}/dataset-{random_user}\")['test']\n",
    "\n",
    "print(\"Training dataset size:\", len(trained_dataset))\n",
    "print(\"Validation dataset size:\", len(validation_dataset))\n",
    "print(\"Example from training dataset:\", trained_dataset[0])\n",
    "print(\"Example from validation dataset:\", validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:28.498418Z",
     "start_time": "2023-12-23T02:10:28.427327500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(batch):\n",
    "    return distgpt2_tokenizer([x.strip('\\n\\r') for x in batch['post']], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:32.186403400Z",
     "start_time": "2023-12-23T02:10:29.030029400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359f1708ed7d4f7093afb1a558f738fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff848c81fa194a39a2f855100a905ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/569 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_dataset_processed = trained_dataset.map(encode, batched=True, batch_size=len(trained_dataset))\n",
    "trained_dataset_processed.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "validation_dataset_processed = validation_dataset.map(encode, batched=True, batch_size=len(validation_dataset))\n",
    "validation_dataset_processed.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T02:10:32.266405800Z",
     "start_time": "2023-12-23T02:10:32.204405500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['post', 'time', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3218\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:24:34.532585400Z",
     "start_time": "2023-12-23T03:19:23.489064Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1074 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1074, training_loss=3.063382762770413, metrics={'train_runtime': 309.1785, 'train_samples_per_second': 20.816, 'train_steps_per_second': 3.474, 'total_flos': 1326971051311104.0, 'train_loss': 3.063382762770413, 'epoch': 2.0})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=distgpt2_tokenizer,\n",
    "    mlm=False  # we're not using masked language modeling here\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{data_dir}/gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,  # adjust as needed\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=12,\n",
    "    logging_steps=1,\n",
    "    save_steps=1_000,  # adjust as needed\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=distgpt2_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=trained_dataset_processed,\n",
    "    #eval_dataset=validation_dataset_processed,  #TODO\n",
    ")\n",
    "\n",
    "# Fine-tune the GPT-2 model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:11:58.039018800Z",
     "start_time": "2023-12-23T03:11:57.985666300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(test_model, data: List[str]):\n",
    "    distilgpt2_correct_predictions = 0\n",
    "    distilgpt2_total_predictions = 0\n",
    "    distilgpt2_inference_times = []\n",
    "\n",
    "    test_model.eval()\n",
    "\n",
    "    for post in tqdm(data, smoothing=0):\n",
    "        sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "        tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "        tokenized_words = [word for s in tokenized_words for word in s]\n",
    "        for i in range(1, len(tokenized_words) - 1):\n",
    "            start_time = time.time()\n",
    "            current_prompt = ' '.join(tokenized_words[:i])\n",
    "            current_prompt = re.sub(r\" ([!.?,;:\\\"')\\]}]{1,9})\", r\"\\1\", current_prompt)\n",
    "            current_prompt = re.sub(r\"([\\[({]{1,9}) \", r\"\\1\", current_prompt)\n",
    "            actual_next_word = tokenized_words[i]\n",
    "            # Tokenize the prompt\n",
    "            input_ids = distgpt2_tokenizer.encode(current_prompt, return_tensors=\"pt\").to(device)\n",
    "            if input_ids.shape[1] == 0:\n",
    "                continue\n",
    "            # gpt2 can only accept up to 1024 tokens so if our tensor is bigger than this\n",
    "            # we trim it before passing into the model\n",
    "            if input_ids[0].shape[0] > 1024:\n",
    "                input_ids = input_ids[:, -1024:]\n",
    "            # Generate probabilities for the next words\n",
    "            with torch.no_grad():\n",
    "                outputs = test_model(input_ids)\n",
    "                logits = outputs.logits\n",
    "            # we are only interested in either the top 5 words or words with a probability of > 1%\n",
    "            # after all, we don't want to suggest too many words!\n",
    "            top_k = 5\n",
    "            top_p = 0.04  # don't suggest 5 words just for the sake of suggesting 5 words\n",
    "            # Get the probability distribution for the next word\n",
    "            next_word_probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "            top_k_values, top_k_indices = torch.topk(next_word_probs, k=top_k, dim=-1)\n",
    "            # Normalize probabilities\n",
    "            top_k_probs_normalized = top_k_values / top_k_values.sum()\n",
    "\n",
    "            # Convert the probabilities to a list\n",
    "            top_k_probs_list: List = top_k_probs_normalized.tolist()[0]\n",
    "            top_k_indices_list: List = top_k_indices.tolist()[0]\n",
    "\n",
    "            # filter to only words with a probability greater than p%\n",
    "            removal_marked = []\n",
    "            for j in range(len(top_k_probs_list)):\n",
    "                if top_k_probs_list[j] < top_p:\n",
    "                    removal_marked += [(top_k_probs_list[j], top_k_indices_list[j])]\n",
    "\n",
    "            for to_remove in removal_marked:\n",
    "                top_k_probs_list.remove(to_remove[0])\n",
    "                top_k_indices_list.remove(to_remove[1])\n",
    "\n",
    "            guesses = set()\n",
    "            for token_id, prob in zip(top_k_indices_list, top_k_probs_list):\n",
    "                token = distgpt2_tokenizer.decode([token_id])\n",
    "                guesses.add(token)\n",
    "\n",
    "            # we consider a prediction correct if the actual word was\n",
    "            # one of the (up to) 5 words suggested\n",
    "            if ' ' + actual_next_word in guesses:\n",
    "                distilgpt2_correct_predictions += 1\n",
    "            distilgpt2_total_predictions += 1\n",
    "            distilgpt2_inference_times += [time.time() - start_time]\n",
    "\n",
    "    return (distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:31:52.317402100Z",
     "start_time": "2023-12-23T03:30:47.787432300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 569/569 [01:04<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(distgpt2_model, validation_dataset_processed['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:31:52.317402100Z",
     "start_time": "2023-12-23T03:31:52.307403600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla accuracy: 0.320960024695169\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print('vanilla accuracy:', gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:32:56.593069200Z",
     "start_time": "2023-12-23T03:31:52.319483800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 569/569 [01:04<00:00,  8.85it/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(trainer.model, validation_dataset_processed['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:36:00.831782900Z",
     "start_time": "2023-12-23T03:36:00.820787100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned accuracy: 0.3979009106343572\n"
     ]
    }
   ],
   "source": [
    "gpt2_accuracy = distilgpt2_correct_predictions / distilgpt2_total_predictions\n",
    "print('finetuned accuracy:', gpt2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### GPT2 Fine-tuning Experiment Results\n",
    "I conducted some experiments to see how well the model could adapt to the posts of a randomly selected user. On the raw posts the answer is not well. I found that Reddit contains a lot of domain specific vocabulary. Typos in posts also complicate things.\n",
    "\n",
    "To try to circumnavigate this issues, I used a T5-based grammar correction model. Due to the aforementioned issues, if this model saw unfamiliar words, it would make a guess and replace those works with something it knew. Moreover, the model also sometimes changed the sentence structures if it recognized unfamiliar grammar.\n",
    "\n",
    "So even though in the above tests, we see the model achieve a 39.7% accuracy after fine-tuning (finally beating the baseline), before this prototype can become a real app, we would need to solve the issues of domain specific vocabulary.\n",
    "\n",
    "##### Challenges with Domain Specific\n",
    "\n",
    "I conducted significant research on adapting models to domain specific vocabulary. Unfortunately, generally speaking, there is no good way to do it without retraining the entire model. Approaches involving unfreezing weights of the embedding layers and simply updating existing vocabulary exist however, according to some sources, this only works in the amount of new vocabulary is small. And even then, we simply may not have enough data to update the embeddings for those models.\n",
    "\n",
    "One solution I see would be to collect massive amounts of data on what people most often type on their phones, retrain the entire GPT2 model (or something else) from scratch on that data and then allow small fine-tuning to be done on user-devices. However, I neither have the skill-set or the resources to pull something like that off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Continuous Learning and The Problem of Catastrophic Forgetting\n",
    "So far, in the examples we have tested, we have trained our model on the entire dataset at ones (as opposed to training the model on the data sequentially). That is, in the real world, in the type of application we want to develop, our model would be exposed to snippets of data over a long period of time. However, this introduces the problem of \"catastrophic forgetting\" where the model, while optimizing the weights on the new data forgets about the old tasks it learned to do.\n",
    "\n",
    "To overcome this problem, we turn to DeepMind research paper titled [\"Overcoming catastrophic forgetting in neural networks\"](https://arxiv.org/abs/1612.00796). This paper arrives upon 3 key insights to help develop an algorithm for the aforementioned problem:\n",
    "1. Many optimal configurations of the set of weights and biases θ exist.\n",
    "2. It is likely that one those optimal configurations is close to the previous learned tasks.\n",
    "3. It is therefore possible to constraint parameters to stay in a region of low error for previous learned tasks during training.\n",
    "\n",
    "Inspired by biological mechanisms found in real life, the paper refers to this algorithm as elastic weight consolidation (EWC). EWC is essentially a new loss function. This is exactly what we are looking for because:\n",
    "1. Ofcourse now we can learn new tasks without forgetting the others.\n",
    "2. EWC allows us to define the importance of each tasks relative to each other. In theory, this means we could classify more recent examples as more important! This gives us a lot of power.\n",
    "3. EWC can be trained for an arbitrary number of new tasks.\n",
    "\n",
    "Despite discouraging results in previous experiments we conducted in this notebook, another advantage of this approach is the model in many cases improves performances compared to standard fine-tuning approaches. Therefore, its worth experimenting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### EWC - Elastic Weight Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:42:05.138932900Z",
     "start_time": "2023-12-23T03:42:05.091413700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words_required = 800\n",
    "num_epochs = 2\n",
    "ewc_lambda = 0.8\n",
    "adam_learning_rate = 5e-5\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T03:42:05.138932900Z",
     "start_time": "2023-12-23T03:42:05.107414400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [01:11<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 87 different task sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████████████████████████████████████████████████████████                                                                 | 50/108 [00:42<00:49,  1.18it/s]"
     ]
    }
   ],
   "source": [
    "# split the dataset into different \"tasks\" where each task is composed of at least 1200 words\n",
    "# every 1200 words we update the model using ewc\n",
    "# at the end of each task set, we see how well the model is able to predict the words in the next task set\n",
    "# we consider a single post the smallest divisible unit\n",
    "task_sets = []\n",
    "current_set_len = 0\n",
    "current_task_set = []\n",
    "for data_entry in entire_dataset_list:\n",
    "    post = data_entry[\"post\"]\n",
    "    sent_tokenized = sent_tokenize(normalize_text(post))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sent_tokenized]\n",
    "    tokenized_words = [word for s in tokenized_words for word in s]\n",
    "    current_set_len += len(tokenized_words)\n",
    "    current_task_set += [data_entry]\n",
    "    if current_set_len >= num_words_required:\n",
    "        task_sets += [current_task_set]\n",
    "        current_task_set = []\n",
    "        current_set_len = 0\n",
    "task_sets += [current_task_set]  # last task set may have less than 1000 words\n",
    "\n",
    "task_datasets = []\n",
    "disable_progress_bar()\n",
    "for task_set in tqdm(task_sets):\n",
    "    # no need to split this data into training and eval\n",
    "    # because we next task is the evaluation\n",
    "    task_set = Dataset.from_list(task_set)\n",
    "    task_set_processed = task_set.map(encode, batched=True, batch_size=len(task_set))\n",
    "    task_set_processed.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "    task_datasets += [task_set_processed]\n",
    "enable_progress_bar()\n",
    "\n",
    "print(f'Created {len(task_datasets)} different task sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T23:11:56.544015400Z",
     "start_time": "2023-12-22T23:11:56.537014700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data_loader = DataLoader(\n",
    "    dataset=trained_dataset_processed,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "eval_data_loader = DataLoader(\n",
    "    dataset=validation_dataset_processed,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T22:22:47.949991600Z",
     "start_time": "2023-12-22T22:22:46.760103800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=adam_learning_rate, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_data_loader) * 2)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(training_data_loader):\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(training_data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "total_eval_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data_loader:\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "average_eval_loss = total_eval_loss / len(eval_data_loader)\n",
    "print(f\"Average Evaluation Loss: {average_eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model_accuracy(model, user_posts_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:30:47.074300100Z",
     "start_time": "2023-12-23T00:30:45.838912200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:30:47.663946800Z",
     "start_time": "2023-12-23T00:30:47.619360800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "optimizer = AdamW(model.parameters(), lr=adam_learning_rate, weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:30:51.733091Z",
     "start_time": "2023-12-23T00:30:51.726087800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def on_task_update(model, device, task_id, optimizer, training_data_loader):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # accumulating gradients\n",
    "    for batch in training_data_loader:\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "    fisher_dict[task_id] = {}\n",
    "    optpar_dict[task_id] = {}\n",
    "\n",
    "    # gradients accumulated can be used to calculate fisher\n",
    "    for name, param in model.named_parameters():\n",
    "        # this fills up GPU memory real fast - must calc this on CPU :(\n",
    "        optpar_dict[task_id][name] = param.data.clone().cpu()\n",
    "        fisher_dict[task_id][name] = param.grad.data.clone().pow(2).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:30:52.051154500Z",
     "start_time": "2023-12-23T00:30:52.044152600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, device, task_id, optimizer, training_data_loader):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_data_loader))\n",
    "\n",
    "    for batch in training_data_loader:\n",
    "        inputs = batch[\"input_ids\"].to(device=device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels=inputs, attention_mask=attention_mask, token_type_ids=None)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        ### magic here! :-)\n",
    "        for task in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fisher_dict[task][name]\n",
    "                optpar = optpar_dict[task][name]\n",
    "                param = param.cpu()\n",
    "                loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(training_data_loader)\n",
    "    print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T00:31:32.347695400Z",
     "start_time": "2023-12-23T00:31:23.950777600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93/93 [00:08<00:00, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.29802414928649834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# original model accuracies\n",
    "print(\"Testing on next dataset...\")\n",
    "target_data_set = task_datasets[0]['post']\n",
    "distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(model, target_data_set)\n",
    "print(\"Accuracy: \", distilgpt2_correct_predictions / distilgpt2_total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T01:42:23.466610100Z",
     "start_time": "2023-12-23T00:31:38.704336100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.8221683823145354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:02<00:02,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8038067267491267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:08<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2955174322080797\n",
      "Training on task:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7853942559315608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:09<00:09,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6326841345200171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93/93 [00:08<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3243688254665203\n",
      "Training on task:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7550806726018587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:20<00:20, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6155611748496691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:41<00:00, 20.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:09<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3129610115911486\n",
      "Training on task:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8210489418771532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:18<00:18, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6489332881238725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:08<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.40138963121325494\n",
      "Training on task:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0144194900989532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:29<00:29, 29.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8098071575164795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:00<00:00, 30.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93/93 [00:08<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3327991452991453\n",
      "Training on task:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7560772101084391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:49<00:49, 49.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5882190826038519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:21<00:00, 40.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.36204954954954954\n",
      "Training on task:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8796777299472264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:37<00:37, 37.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6425533209528241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:18<00:00, 39.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:09<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3457645178476292\n",
      "Training on task:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6305383443832397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:37<00:37, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.515290054678917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:21<00:00, 40.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [00:08<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33297701763762694\n",
      "Training on task:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4389977869060304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:47<00:47, 47.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.37931856678591835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:33<00:00, 46.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 86/86 [00:08<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3735725938009788\n",
      "Training on task:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.685645813291723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [01:04<01:04, 64.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5554615773937919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:10<00:00, 65.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:09<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3805970149253731\n",
      "Training on task:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5782792134718462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [01:16<01:16, 76.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.421897980299863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:32<00:00, 76.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37280701754385964\n",
      "Training on task:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8383682606120905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [01:29<01:29, 89.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6348338599006335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:00<00:00, 90.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [00:08<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3374439461883408\n",
      "Training on task:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.814585014184316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:01<02:01, 121.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5889935771624247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [04:05<00:00, 122.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85/85 [00:08<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.370112479914301\n",
      "Training on task:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8381003019484606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [01:41<01:41, 101.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6416747597130862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:24<00:00, 102.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:09<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3528497409326425\n",
      "Training on task:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4984439522027969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [01:41<01:41, 101.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4346691817045212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:23<00:00, 102.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.38946224877783814\n",
      "Training on task:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6155359993378321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:07<02:07, 127.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.48834773153066635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [04:18<00:00, 129.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:08<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3808769149498151\n",
      "Training on task:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.9225851088762284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [01:08<01:08, 68.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7474996984004975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:55<00:00, 87.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81/81 [00:09<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33574754267977236\n",
      "Training on task:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0412134257229892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:17<02:17, 137.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.746917028318752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [04:32<00:00, 136.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:08<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4037097654118931\n",
      "Training on task:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6032216869867765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:50<02:50, 170.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.49076265612473857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [05:39<00:00, 169.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:08<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3721185510428101\n",
      "Training on task:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6745993449137762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:59<02:59, 179.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.51234170794487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [05:56<00:00, 178.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:08<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33095499451152577\n",
      "Training on task:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8088575924436251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [02:56<02:56, 176.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5761137629548708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [05:52<00:00, 176.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:23<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.09427464485578993\n",
      "Training on task:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0502725094556808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:40<00:40, 40.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8128634579479694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:18<00:00, 39.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:08<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3543661971830986\n",
      "Training on task:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0250905930995942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 1/2 [03:53<03:53, 233.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6756918013095856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [07:46<00:00, 233.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on next dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:08<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3369256948383437\n",
      "Training on task:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2 [01:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on task: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrain_ewc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     on_task_update(model, device, \u001b[38;5;28mid\u001b[39m, optimizer, training_data_loader)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting on next dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m, in \u001b[0;36mtrain_ewc\u001b[0;34m(model, device, task_id, optimizer, training_data_loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m         fisher \u001b[38;5;241m=\u001b[39m fisher_dict[task][name]\n\u001b[1;32m     22\u001b[0m         optpar \u001b[38;5;241m=\u001b[39m optpar_dict[task][name]\n\u001b[0;32m---> 23\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (fisher \u001b[38;5;241m*\u001b[39m (optpar \u001b[38;5;241m-\u001b[39m param)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m ewc_lambda\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ewc_accs = []\n",
    "ewc_accs += [distilgpt2_correct_predictions / distilgpt2_total_predictions]\n",
    "\n",
    "for id, task_dataset in enumerate(task_datasets):\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        dataset=task_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    print(\"Training on task: \", id)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        train_ewc(model, device, id, optimizer, training_data_loader)\n",
    "        on_task_update(model, device, id, optimizer, training_data_loader)\n",
    "\n",
    "    print(\"Testing on next dataset...\")\n",
    "    target_data_set = task_datasets[id+1]['post']\n",
    "    distilgpt2_correct_predictions, distilgpt2_total_predictions, distilgpt2_inference_times = test_model_accuracy(model, target_data_set)\n",
    "    print(\"Accuracy: \", distilgpt2_correct_predictions / distilgpt2_total_predictions)\n",
    "    ewc_accs += [distilgpt2_correct_predictions / distilgpt2_total_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T01:43:08.040741700Z",
     "start_time": "2023-12-23T01:43:07.435627700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14cfd61bbcd0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ewc_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Further exploration stuff:\n",
    "\n",
    "* Vocab updated??\n",
    "https://github.com/huggingface/tokenizers/issues/1160\n",
    "* More messing with hyper params\n",
    "* Using T5 instead\n",
    "soruces:\n",
    "* https://github.com/moskomule/ewc.pytorch/blob/master/demo.ipynb\n",
    "https://github.com/kuc2477/pytorch-ewc\n",
    "https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With grammar correction.\n",
    "https://share.aseef.dev/mOLEW0j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resources and Publications\n",
    "\n",
    "For this project, I am considering/planning to use the following resources for research:\n",
    "\n",
    "1. Tunstall, L., Werra, L., Wolf, T., &amp; Géron, A. (2022). Natural language processing with transformers: Building language applications with hugging face. O’Reilly.\n",
    "\n",
    "> This book has been repeatedly recommended by both Professor Snyder and other students in CS505. Upon a coarse inspection, I expect to particular find the sections on \"Fine Tuning\" various models helpful (since this is very much a Fine-Tuning project). The book also contains extensive details about various transformer architectures which will inevitably prove useful.\n",
    "\n",
    "2. \"Fine-Tune a Pretrained Model.\" Hugging Face, https://huggingface.co/docs/transformers/training. Accessed 1 Dec. 2023.\n",
    "\n",
    "> This blog post contains examples Fine-Tuning bert using three different methods along with model evaluation. Although unlike source (#1), it goes into less details on the theory, the sample code is more rich, and easier to work with.\n",
    "\n",
    "3. Raffel, Colin, et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Arxiv, 23 Oct. 2019, Accessed 1 Dec. 2023.\n",
    "\n",
    "> This is the original paper that introduced T5 to the world. As covered in lecture, similar to Bert, T5 was (partially) trained on Mask-Fill task where random spans of texts were removed which naturally make it a great candidate for what we want to achieve here. However, this Mask-Fill process was only part of the process. T5 is a very versitile (and large) model, and has huge applications. There is no better place to learn what it is, how it works, and how to use it than the original paper!\n",
    "\n",
    "4. https://212digital.medium.com/fine-tuning-the-gpt-2-large-language-model-unlocking-its-full-potential-66e3a082ab9c\n",
    "\n",
    "> This great blog post contains all the coded needed to get started on fine-tuning GPT2 and the warnings about the various pitfalls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
